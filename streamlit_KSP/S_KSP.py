# S_KSP_clickpro_v4_plotly_patch_FIXED.py
# ===============================================
# KSP Explorer â€” Leaflet + Plotly (Pro v4 â€¢ Plotly patch, FIXED)
# - ì§€ë„: â‘  êµ­ê°€ë³„ ì´ê³„(í´ë¦­) â‘¡ ICT ìœ í˜• ë‹¨ì¼í´ë˜ìŠ¤(í´ë¦­)
# - ìƒì„¸: ì›Œë“œí´ë¼ìš°ë“œ(í•­ìƒ: í•´ì‹œíƒœê·¸+ìš”ì•½/ë‚´ìš©) + ìƒìœ„ í‚¤ì›Œë“œ ê°€ë¡œë§‰ëŒ€(ë¼ë²¨ ì˜ë¦¼ ë°©ì§€)
# - ì „ì—­ ëŒ€ì‹œë³´ë“œ: ë„ë„› 2ê°œ + ì£¼ì œÃ—WB 100% ëˆ„ì  ë§‰ëŒ€
# - ì—°ë„ ì‹œê°í™”: ìˆœìœ„ Bump / 100% ëˆ„ì  ë§‰ëŒ€ (í† ê¸€)
# - ì¶”ê°€ ì‹œê°í™”: ëŒ€í‘œ í‚¤ì›Œë“œ ìƒëŒ€ íŠ¸ë Œë“œ(ìƒ/í•˜, Plotly) + ëŒ€í‘œ 'ì£¼ì œ(í‚¤ì›Œë“œ)' ìƒëŒ€ íŠ¸ë Œë“œ(ìƒ/í•˜, Plotly)
# - ë¶ˆí•„ìš”í•œ ìŠ¬ë¼ì´ë”/ì˜µì…˜ ì œê±°: ì›Œë“œí´ë¼ìš°ë“œ ì†ŒìŠ¤ ê³ ì •, Top-K ì¡°ì ˆ/Jeffreys+ë¡¤ë§ ìœˆë„ ì¡°ì ˆ ì œê±°
# - FIX: with/else ë“¤ì—¬ì“°ê¸° ì •ë¦¬, ë¸”ë¡ ì‚¬ì´ì— ì½”ë“œ ì‚½ì…ìœ¼ë¡œ ì¸í•œ SyntaxError í•´ê²°
# ===============================================
import os, io, re, json, urllib.request, hashlib, pathlib, copy, colorsys
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
os.environ.setdefault("HF_HUB_DISABLE_TELEMETRY", "1")
os.environ.setdefault("KMP_DUPLICATE_LIB_OK", "True")
from typing import List, Dict, Tuple, Optional
from collections import Counter, defaultdict, OrderedDict
import urllib.request
from pathlib import Path
import traceback, platform, numpy as np
import pandas as pd
from PIL import Image
import itertools
from PIL import ImageFont
import streamlit as st
import folium
from streamlit_folium import st_folium
from wordcloud import WordCloud
import plotly.express as px
import plotly.graph_objects as go
from matplotlib import font_manager, rcParams
import math
from sklearn.feature_extraction.text import TfidfVectorizer
from functools import lru_cache     


# --------------------- í˜ì´ì§€/í…Œë§ˆ ---------------------
st.set_page_config(page_title="KSP Explorer (Pro v4)", layout="wide", page_icon="ğŸŒ")

@st.cache_resource
def resolve_korean_font() -> str | None:
    # 1) ë¦¬í¬ì— ë™ë´‰ëœ í°íŠ¸ ìš°ì„ 
    candidates = [
        Path(__file__).parent / "assets/fonts/NanumGothic.ttf",
        Path(__file__).parent / "assets/fonts/NotoSansKR-Regular.otf",
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf",
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
        r"C:\Windows\Fonts\malgun.ttf",
        "/System/Library/Fonts/AppleGothic.ttf",
    ]
    for p in candidates:
        p = str(p)
        if os.path.exists(p):
            try:
                ImageFont.truetype(p, 20)
                return p
            except Exception:
                pass

    # 2) ìµœí›„: ì‹œìŠ¤í…œ í°íŠ¸ ë””ë ‰í† ë¦¬ ì „ì²´ ìŠ¤ìº”(ìºì‹œë¨)
    roots = ["/usr/share/fonts", "/Library/Fonts", "/System/Library/Fonts", r"C:\Windows\Fonts"]
    keys  = ["nanum","noto","apple","malgun","gulim","batang","sourcehan","cjk"]
    for root in roots:
        if not os.path.isdir(root):
            continue
        for dirpath, _, filenames in os.walk(root):
            for fn in filenames:
                if fn.lower().endswith((".ttf",".otf",".ttc")) and any(k in fn.lower() for k in keys):
                    cand = os.path.join(dirpath, fn)
                    try:
                        ImageFont.truetype(cand, 20)
                        return cand
                    except Exception:
                        continue
    return None


WC_FONT_PATH = resolve_korean_font()
#GLOBAL_FONT_FAMILY = "Noto Sans KR, NanumGothic, Malgun Gothic, AppleGothic, Arial Unicode MS, sans-serif"

st.sidebar.header("í™˜ê²½ ì„¤ì •")
theme_name = st.sidebar.selectbox(
    "í…Œë§ˆ", ["Nord", "Emerald", "Sandstone", "Slate"], index=0
)

THEME_PRESETS = {
    "Obsidian":  {"bg":"#0f1115","text":"#e6e8eb","panel":"#151923","card":"#0f141c","border":"#202634","accent":"#4f9cf0","plotly_template":"plotly_dark"},
    "Midnight":  {"bg":"#0b1220","text":"#e8eefc","panel":"#101a2c","card":"#0d1526","border":"#1c2a45","accent":"#8ac6ff","plotly_template":"plotly_dark"},
    "Nord":      {"bg":"#ECEFF4","text":"#2E3440","panel":"#E5E9F0","card":"#FFFFFF","border":"#D8DEE9","accent":"#5E81AC","plotly_template":"plotly_white"},
    "Emerald":   {"bg":"#f3fbf7","text":"#123026","panel":"#e8f6ee","card":"#ffffff","border":"#cfe9dc","accent":"#2bb673","plotly_template":"plotly_white"},
    "Sandstone": {"bg":"#faf7f2","text":"#2c251b","panel":"#f1ece3","card":"#ffffff","border":"#e3d9c6","accent":"#d49a6a","plotly_template":"plotly_white"},
    "Slate":     {"bg":"#f6f7fb","text":"#111827","panel":"#eef1f6","card":"#ffffff","border":"#e5e7eb","accent":"#3b82f6","plotly_template":"plotly_white"},
}
ui = THEME_PRESETS[theme_name]



st.markdown(f"""
<style>
:root {{
  --bg:{ui['bg']}; --text:{ui['text']};
  --panel:{ui['panel']}; --card:{ui['card']}; --border:{ui['border']}; --accent:{ui['accent']};
}}
html, body, .block-container {{ background:var(--bg) !important; color:var(--text) !important; }}
section[data-testid="stSidebar"] {{ background:var(--panel) !important; }}
div[data-testid="stHeader"] {{ background:var(--bg) !important; }}
.stMarkdown, p, h1,h2,h3,h4,h5,h6 {{ color:var(--text) !important; }}
.ksp-card {{ background:var(--card); border:1px solid var(--border); border-radius:14px; padding:14px; }}
.ksp-chip {{ display:inline-block; background:var(--panel); border:1px solid var(--border);
            border-radius:14px; padding:4px 10px; margin:4px; }}
a {{ color: var(--accent); }}
</style>
""", unsafe_allow_html=True)

# ---- Force Korean-capable fonts in the browser (Plotly/HTML) ----
#st.markdown(f"""
#<style>
#* {{ font-family: {GLOBAL_FONT_FAMILY} !important; }}
#</style>
#""", unsafe_allow_html=True)

# ---- Plotly font stack (safe getter) ----
FONT_STACK_DEFAULT = "Noto Sans KR, NanumGothic, Malgun Gothic, AppleGothic, Arial Unicode MS, Arial, sans-serif"

def _plotly_font_family():
    # ì„¸ì…˜/ê¸€ë¡œë²Œ/ë””í´íŠ¸ ìˆœìœ¼ë¡œ ê°€ì ¸ì˜´
    return (
        st.session_state.get("plotly_font_family")
        or globals().get("GLOBAL_FONT_FAMILY")
        or FONT_STACK_DEFAULT
    )



st.title("KSP Explorer ğŸŒ â€” Pro v4")



# --------------------- ë¶ˆìš©ì–´ ---------------------
STOP = {
    "ë°","ë“±","ê´€ë ¨","ìˆ˜ë¦½","ë°©ì•ˆ","ê°œì„ ","ì „ëµ","ì§€ì›","ì •ì±…","ì‚¬ì—…","í”„ë¡œì íŠ¸","ì œë„", "í•œêµ­ê³¼", "ë©•ì‹œì½”ì˜", "í–¥ìƒì„", "í”„ë¡œì íŠ¸ëŠ”", "í˜„ëŒ€í™”ë¥¼", "í—¬í”„ë°ìŠ¤í¬ì™€", "ê¸°ëŠ¥", "ìƒìŠ¹", "íƒ‘ì¬", "ê³¼ì œ", "100ìœ„ì—ì„œ", "ëª¨ë‹ˆí„°ë§ì„", "ê³µìœ ", "ì§€ì ", "ë†’ì€", "28ì¤‘", "ë¯¸í¡", "9ëŒ€í•­ëª©",
    "êµ¬ì¶•","ë„ì…","ê°œìš”","í˜„í™©","ìœ„í•œ","í™œìš©","ë¶„ì„","ì œê³µ","ê°œë°œ","ê¸°ë°˜","ë””ì§€í„¸","data", "ê·¼ê±°", "ê²½í—˜", "67ì†Œ", "9ëŒ€", "3ê·¸ë£¹", "ì œë„ì ", "ë³€í™”", "ê¸°ê´€", "ì¡°ì‚¬", "ë¶€ë¬¸", "í™•ëŒ€", "ê¸°ì—…", "í˜ì‹ ì„", "í™œìš©í•œ", "ë“±ì´ë‹¤", "íš¨ê³¼ëŠ”", "ê²½í—˜ì„", "ë°©ì•ˆì„",
    "ë°ì´í„°","system","ì •ë¶€","ksp","koica","kdi","idb","ebrd","wb","adb","êµ­ê°€","í•œêµ­", "ì œë„ì™€", "ì‹œìŠ¤í…œì„", "ê²€ìƒ‰", "ì „ë¬¸ê°€", "ì—…ì²´", "ì‚¬ì—…ì€", "ì œë„ê°œì„ ì„", "ë¡œë“œë§µê³¼", "ì„¤ë¬¸ì„", "ë””ì§€í„¸ì •ë¶€ì˜", "ì²´ê³„ë¥¼", "ìˆœìœ„", "ìˆœìœ„ê°€", "80ìœ„ë¡œ", "í•„ìš”ì„±", "í‘œì¤€", "ê¸°ê´€ì˜", "28ì¤‘í•­ëª©",
    "ì—°êµ¬","ë³´ê³ ","ìµœì¢…","ì¤‘ê°„","ì„±ê³¼","í–¥ìƒ","ì œê³ ","ë„ì›€","ì°¨ì„¸ëŒ€","ë¡œë“œë§µ","ìš´ì˜","ì„œë¹„ìŠ¤", "ë°”íƒ•ìœ¼ë¡œ", "ë§ë ˆì´ì‹œì•„ì˜", "ì§„ë‹¨í•˜ê³ ", "ì •ë¶€ì™€", "KSPëŠ”", "ì§€ì›í–ˆë‹¤", "í•œêµ­ì˜", "ì‹œìŠ¤í…œ", "íšŒì˜", "ë…¼ì˜", "ì°¸ì—¬", "ì‹œìŠ¤í…œê³¼", "KSPì—ì„œ", "ì°¸ê³ í•˜ì—¬", "êµ¬ì¶•ì„", "ë°©ë¬¸", "í’€í…ìŠ¤íŠ¸",
    "ë¼ì˜¤ìŠ¤","2030","ntca","to be","í—ê°€ë¦¬","be","ì‚¬ë¡€","ëª¨ë¸","ì‚°ì—…", "ë¹„êµ", "ê°•ì¡°", "ìµœì¢…ë³´ê³ ", "ìœ„í•´", "ì—°ìˆ˜", "ë¹„ì „", "ê°œìµœ", "í˜‘ì˜", "ì œ2", "êµ¬ì¶•", "ê¶Œê³ ", "ë¬¸ì œ", "ì˜¨ë‚˜ë¼", "ì¤‘ì•™", "ë„ì…ê³¼", "í•˜ë¶€", "ì„±ì¥", "ë“±ì„", 'í’ˆì§ˆ', "ì—°êµ¬ê°œë°œê³¼", "ê±°ë²„ë„ŒìŠ¤ë¥¼", "ì„¤ë¦½",
    "ê²©ì°¨","í•´ì†Œ","ì—­ëŸ‰","ê°•í™”","ì‹¤í–‰ê³„íš","ì—°ê¸ˆ","vision","ì‹¤ìš©ì‹ ì•ˆ", "í‰ê°€", "ì œì‹œ", "í†µí•´", "ì„¤ì¹˜", "ì œì‹œí–ˆë‹¤", "ê¶Œê³ í•˜ë©°", "ë¶„ì„í•˜ê³ ", "ì‹œìŠ¤í…œì˜", "ê°œì„ ì•ˆì„", "í–ˆë‹¤", "í†µí•´", "ëª©í‘œë¡œ", "í–¥ìƒê³¼", "ì œì•ˆ", "ê´€ë¦¬", "í†µí•©", "í˜‘ë ¥", "ì œì•ˆí•˜ì˜€ë‹¤", "ì²´ê³„", "ë¹„êµí•˜ì—¬", "ì„¤ëª…ê°€ëŠ¥",
    "ê°€ì§€", "ì¥ê¸°", "íˆ¬ëª…ì„±ê³¼", "ê¸°ëŒ€í–ˆë‹¤", "ì§„í–‰ë˜ì—ˆìœ¼ë©°", "ì „í™˜ê³¼", "ë§ˆë ¨í–ˆë‹¤", "ëª¨ë¸ë¡œ", "í˜‘ì˜ì™€", "ë¹„êµì™€", "ê°œë°œì„", "ì´í›„", "ë§ˆë ¨", "ë¶€ì¡±ì„", "ìˆ˜ë¦½ì„", "ì‚¬ë¡€ë¥¼", "ì •ë¶€ì™€", "í”„ë ˆì„ì›Œí¬", "í†µí•œ", "ê¸°ë°˜ìœ¼ë¡œ", "ì¹´í•˜", "ë°œì „", "ì œì•ˆí–ˆë‹¤", "ì œì‹œí•˜ê³ ", "ì •ì±…ì„", "í™”ìƒíšŒì˜ë¥¼",
    "ì ìš©", "í¬í„¸", "ìˆ˜ë¦½í•˜ì˜€ë‹¤", "ê²ƒì´ë‹¤", "ë„ì…ì„", "ì •ë¶€ì˜", "ì„±ìˆ™ë„", "ê³¼ì •", "ê³„íšì„", "í¬ìˆ˜", "ë”°ë¥¸", "ê³„íšê³¼", "TV", "ìƒˆë¡œìš´", "ì¤‘ì†Œê¸°ì—…ì˜", "ìë£Œ", "ì¶”ì •", "ì•ˆì •", "ì„ ì •", "ê²½ìŸë ¥", "ì „ëµì„", "í˜„í™©ì„", "ê°œì„ ì„", "í™œì„±í™”ë¥¼", "To", "ê²€í† ", "ì‹¤í–‰ê³„íšê³¼", "ë‹¨ê³„ë³„",
    "ê°•í™”ì™€", "í˜„í–‰", "ìƒíƒœê³„ë¥¼", "í˜„í™©ê³¼", "ê°€ì¹˜í‰ê°€", "ì „í™˜ì—", "í™œë™", "êµ­ë¯¼", "í¬í•¨", "ê¸°ë³¸ê³„íš", "ì ‘ê·¼ì„±", "ì „í™˜", "í• ë‹¹", "ì–‘êµ­", "íš¨ê³¼", "ì¶”ì§„", "í˜‘ë ¥ê³¼", "ì´ìš©ê³„íš", "ë‹´ì€", "ë¡œë“œë§µì„", "ì œì‹œí•˜ì˜€ë‹¤", "WASH", "ê¸°ìˆ ", "ê¸°ëŒ€íš¨ê³¼", "ê°€ë‚˜ì˜", "ì†”ë£¨ì…˜ì„", "ìˆ˜í–‰í•˜ì˜€ë‹¤", "ë¶„ì•¼",
    "67ì†Œí•­ëª©ìœ¼ë¡œ", "ë¶„ì•¼", "ì§„ë‹¨", "ì¤‘ë³µì¡°ì‚¬ì™€", "ì‚¬ìš©", "í•­ëª©", "í¬ìš©ì ", "ê·œì œ", "ê¸°ì—…ë“¤ì˜", "í˜ì‹ ", "ë¼ì˜¤ìŠ¤ì˜", "ë©”ì½©ê°•", "ë² íŠ¸ë‚¨ì˜", "í†µê³„", "ì²˜ë¦¬", "ì œë„ì˜", "ì œì•ˆí•©ë‹ˆë‹¤", "ìœ ì—­ì˜", "ì´ë¥¼", "ê°•í™”ë¥¼", "ìˆìŠµë‹ˆë‹¤", "íê¸°ë¬¼", "í•„ë¦¬í•€ì˜", "tuneps", "í‘œì¤€í™”", "ì¬ì •ê°ì‚¬ê°ë…ì²­", "ì´ì§‘íŠ¸ì˜",
    "ê°œí˜ì„", "ì²´ê³„ì ìœ¼ë¡œ", "dur", "íŒŒë¼ê³¼ì´ì˜", "ê³¼í…Œë§ë¼ì˜", "ì œì•ˆí•œë‹¤", "ë³´ê³ ì„œëŠ”", "ê°•í™”í•˜ê³ ", "êµ­ê°€ë“¤ì˜", "íšŒì›êµ­", "ë„ì¶œí–ˆìŠµë‹ˆë‹¤", "ë¶„ì„í•˜ì—¬", "íŒ€ì€", "ì œì•ˆí•œë‹¤", "ê²ƒì„", "ì œì‹œí•œë‹¤", "êµ­ê°€ë“¤ì˜", "ìš°ì¦ˆë² í‚¤ìŠ¤íƒ„ì˜", "ê²€í† í•˜ì˜€ìŠµë‹ˆë‹¤", "ë‚®ì€", "ê·¸ë£¹", "ê²½ìŸë ¥ì„", "ê°•ì¡°í•œë‹¤", "ì¤‘ìš”ì„±ì„",
    "í•µì‹¬", "ìˆ˜ë™", "ì§€ì—°", "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ë³´ê³ ì„œì…ë‹ˆë‹¤", "ê²ªê³ ", "ì¸í•´", "í˜„ì¬", "ë‹¤ë‹ˆëŠ”", "ë‹¤ë£¬ë‹¤", "ì¤‘ì‹¬ìœ¼ë¡œ", "ê°€ëŠ¥í•œ", "í•œë‹¤", "ìœ„ì¹˜", "ë¶€ë¬¸ì˜", "ê°€ì¥", "ì˜¨ë‘ë¼ìŠ¤ì˜", "ìš´ì˜ì„", "ì„¼í„°", "íŠ¹íˆ", "ì°¸ì—¬ë¥¼", "ë“±ë¡", "ì´ˆì ì„", "ì§€ì›ì„", "ì œì‹œí–ˆìŠµë‹ˆë‹¤", "í–‰ì •ì˜",
    "ì ‘ê·¼ì„±ì„", "ë°œìƒí•˜ëŠ”", "ìˆ˜ë¦½í•©ë‹ˆë‹¤", "ì œì‹œí•©ë‹ˆë‹¤", "ì„±ê³µì ì¸", "íš¨ìœ¨ì ì¸", "ì „í™˜ì„", "ëŒ€ì‘", "ìë¬¸ì„", "í•©ë‹ˆë‹¤", "ê¸°ìˆ ì„", "ì„œë¹„ìŠ¤ì—", "ë“±ì˜", "ì£¼ìš”", "ë¶„ì ˆëœ", "ì‹œìŠ¤í…œì€", "ê¸°ëŠ¥ì´", "ì„¸ë¥´ë¹„ì•„ì˜", "ë°©ê¸€ë¼ë°ì‹œì˜", "ê°•í™”í•˜ê¸°", "ì²´ê³„ì ì¸", "ë¬¸ì œë¥¼", "ì§€ì›í•©ë‹ˆë‹¤", "ë†’ì´ëŠ”", "ê¸°ë³¸", "ë‹¨ì§€ì˜", "ì‚°ì—…ì˜",
    "ë¯¸í¡í•œ", "ì‹œìŠ¤í…œì´", "ë¹„ë¡¯í•œ", "ë‹¤ê°í™”ì™€", "íƒ€ì§€í‚¤ìŠ¤íƒ„ì€", "íƒ€ì§€í‚¤ìŠ¤íƒ„ì˜", "ì •ë³´", "ì´ì—", "ë”°ë¼", "ì‹¤ì •ì…ë‹ˆë‹¤", "ë°ì´í„°ì˜", "ë°ì´í„°ë¥¼", "ê³µìœ í•˜ê³ ", "ê²ƒì…ë‹ˆë‹¤", "ê¶ê·¹ì ìœ¼ë¡œ", "ê¸°ì—¬í• ", "ì •í™•ì„±ì„", "ìë™í™”í•˜ì—¬", "ìˆ˜ë¦½ì˜", "ê³µìœ ë¥¼", "ìœµí•©í•˜ì—¬", "ë¶€ë¬¸ì„", "ì§€ì†", "ë‹¬ì„±í•˜ë„ë¡", "ì„±ì¥ì„", "ë•ëŠ”", "ì‚°ì—…ê³¼",
    "ê²½ì œì—ì„œ", "ê²½ì œë¡œ", "ì „í™˜í•˜ê³ ì", "ê·¸ëŸ¬ë‚˜" ,"ë¶€ë¬¸ì€", "ë¶€ì¡±ìœ¼ë¡œ", "ì ì¬ë ¥ì„", "ì¶©ë¶„íˆ", "í™œìš©í•˜ì§€", "ëª»í•˜ê³ ", "í˜¸ì£¼ëŠ”", "í˜¸ì£¼ì˜", "ë¶„ì•¼ì—ì„œ", "ì¸ë„ë„¤ì‹œì•„ëŠ”", "ë¬¸ì œì ì„", "íš¨ìœ¨ì„±ì„", "ê²ƒìœ¼ë¡œ", "ì§€ì—­ì˜", "ë²¤ì¹˜ë§ˆí‚¹í•˜ì—¬", "ê¸°ëŒ€ë©ë‹ˆë‹¤"
}
STOP_LOW = {w.lower() for w in STOP}

# ---- Trend config (safe defaults; can be overridden later) ----
# ì½”ë“œ ì–´ë””ì„œë“  ì°¸ì¡°í•´ë„ NameErrorê°€ ì•ˆ ë‚˜ë„ë¡ ì•ˆì „ ê¸°ë³¸ê°’ì„ ë¨¼ì € ê¹”ì•„ë‘”ë‹¤.
YEAR_SOURCE = globals().get("YEAR_SOURCE", None)   # ì˜ˆ: "ì—°ë„"ë¡œ ë°”ê¾¸ë©´ í•´ë‹¹ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
STOP_CUSTOM = globals().get("STOP_CUSTOM", set())  # ì½”ë“œì—ì„œ ì§ì ‘ ì¶”ê°€í•  ë¶ˆìš©ì–´
BASE_STOP   = globals().get("BASE_STOP", set())

STOP_LOW_ALL = (
    {w.lower() for w in STOP} |
    {w.lower() for w in STOP_CUSTOM} |
    {w.lower() for w in BASE_STOP}
)

# --------------------- ë°ì´í„° ì…ë ¥ ---------------------
st.sidebar.header("ë°ì´í„° ì…ë ¥")

# ê¸°ë³¸ê°’(ìˆì–´ë„ ë˜ê³  ì—†ì–´ë„ ë¨ â€” ìë™ íƒì§€ ì‹œ ë¬´ì‹œë¨)
DEFAULT_DATA_PATH = r"df1_20250901_145328.xlsx"

# ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€ ë””ë ‰í† ë¦¬(ë…¸íŠ¸ë¶/REPL ëŒ€ë¹„ fallback)
DATA_DIR = Path(__file__).resolve().parent if "__file__" in globals() else Path.cwd()
SEARCH_DIRS = [DATA_DIR, DATA_DIR / "data", DATA_DIR / "assets"]

# --- Safe column selector (êµì§‘í•© + ë™ì˜ì–´ + í´ë°±) ---
ALT_NAMES = {
    "ì‚¬ì—… ê¸°ê°„": ["ì‚¬ì—…ê¸°ê°„", "ê¸°ê°„", "Project Period", "Years", "Year", "year"],
    "ìš”ì•½": ["ìš”ì•½ë¬¸", "ë‚´ìš© ìš”ì•½"],
    "ì£¼ìš” ë‚´ìš©": ["ë³¸ë¬¸", "ë‚´ìš©"],
    "Hashtag_str": ["Hashtag", "í•´ì‹œíƒœê·¸", "í•´ì‹œíƒœê·¸_ë¬¸ìì—´"],
    "ëŒ€ìƒê¸°ê´€": ["ê¸°ê´€", "ê¸°ê´€ëª…"],
    "ì§€ì›ê¸°ê´€": ["ì§€ì› ê¸°ê´€"],
    "ICT ìœ í˜•": ["ICTìœ í˜•"],
    "ì£¼ì œë¶„ë¥˜(ëŒ€)": ["ì£¼ì œ(ëŒ€)", "ì£¼ì œ ëŒ€ë¶„ë¥˜"],
    "íŒŒì¼ëª…": ["Filename", "íŒŒì¼ ì´ë¦„"],
    "ëŒ€ìƒêµ­": ["Country"],
}

def pick_existing_columns(df: pd.DataFrame, preferred: list[str], fallback_max: int = 8) -> list[str]:
    """preferred ìš°ì„  â†’ ALT_NAMESë¡œ ëŒ€ì²´ â†’ ê·¸ë˜ë„ ë¶€ì¡±í•˜ë©´ ì• nê°œ ì„ì˜ í´ë°±"""
    existing = [c for c in preferred if c in df.columns]
    wanted = set(preferred)

    # í•„ìš”í•œë° ë¹ ì§„ ê²ƒë“¤ì— ëŒ€í•´ ë™ì˜ì–´ ì‹œë„
    for col in preferred:
        if col in existing:
            continue
        for alt in ALT_NAMES.get(col, []):
            if alt in df.columns and alt not in existing and alt not in wanted:
                existing.append(alt)
                break

    # ê·¸ë˜ë„ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì•ì—ì„œ ëª‡ ê°œ í´ë°±
    if not existing:
        existing = list(df.columns[:fallback_max])

    return existing


def ensure_unique_columns(df: pd.DataFrame) -> pd.DataFrame:
    cols = pd.Series(df.columns)
    if cols.duplicated().any():
        # ë™ì¼ ì´ë¦„ ì—´ì´ ì—¬ëŸ¬ ê°œë©´ ì²« ë²ˆì§¸ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ë²„ë¦¼
        df = df.loc[:, ~cols.duplicated()].copy()
    return df

# === NEW: ì»¬ëŸ¼ ì •ê·œí™”(ìœ ì‚¬ëª… â†’ í‘œì¤€ëª…) ===
def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    - ê³µë°±/ê°œí–‰ ì œê±°, ëŒ€ì†Œë¬¸ì í‹€ì–´ì§ ë³´ì •
    - ìì£¼ ì“°ì´ëŠ” ë³€í˜•ëª…ì„ í‘œì¤€ ì»¬ëŸ¼ìœ¼ë¡œ í†µì¼
    """
    if df is None or df.empty:
        return df

    # 1) íŠ¸ë¦¬ë°
    new_cols = []
    for c in df.columns:
        c2 = str(c).replace("\n", " ").strip()
        c2 = re.sub(r"\s+", " ", c2)
        new_cols.append(c2)
    df = df.copy()
    df.columns = new_cols

    # 2) ìœ ì‚¬ëª… ë§¤í•‘
    #   í‘œì¤€: "ì‚¬ì—… ê¸°ê°„", "ì—°ë„", "ìš”ì•½", "ì£¼ìš” ë‚´ìš©", "Hashtag", "Hashtag_str", "ICT ìœ í˜•", "ì£¼ì œë¶„ë¥˜(ëŒ€)", "ëŒ€ìƒêµ­", "ëŒ€ìƒê¸°ê´€", "ì§€ì›ê¸°ê´€", "íŒŒì¼ëª…"
    rename_map = {
        "ì‚¬ì—…ê¸°ê°„": "ì‚¬ì—… ê¸°ê°„",
        "í”„ë¡œì íŠ¸ ê¸°ê°„": "ì‚¬ì—… ê¸°ê°„",
        "ê¸°ê°„": "ì‚¬ì—… ê¸°ê°„",
        "Project Period": "ì‚¬ì—… ê¸°ê°„",
        "Years": "ì—°ë„",
        "Year": "ì—°ë„",
        "year": "ì—°ë„",
        "Hashtags": "Hashtag",
        "í•´ì‹œíƒœê·¸": "Hashtag",
        "í•´ì‹œíƒœê·¸_ë¬¸ìì—´": "Hashtag_str",
        "ICTìœ í˜•": "ICT ìœ í˜•",
        "ì£¼ì œ(ëŒ€)": "ì£¼ì œë¶„ë¥˜(ëŒ€)",
        "ì£¼ì œ ëŒ€ë¶„ë¥˜": "ì£¼ì œë¶„ë¥˜(ëŒ€)",
        "Country": "ëŒ€ìƒêµ­",
        "ê¸°ê´€": "ëŒ€ìƒê¸°ê´€",
        "ê¸°ê´€ëª…": "ëŒ€ìƒê¸°ê´€",
        "ì§€ì› ê¸°ê´€": "ì§€ì›ê¸°ê´€",
        "Filename": "íŒŒì¼ëª…",
        "íŒŒì¼ ì´ë¦„": "íŒŒì¼ëª…",
        "ìš”ì•½ë¬¸": "ìš”ì•½",
        "ë‚´ìš© ìš”ì•½": "ìš”ì•½",
        "ë³¸ë¬¸": "ì£¼ìš” ë‚´ìš©",
    }
    for k, v in list(rename_map.items()):
        if k in df.columns and v not in df.columns:
            df = df.rename(columns={k: v})

    # 3) ìµœì†Œ í•„ìš”í•œ í•µì‹¬ ì»¬ëŸ¼ì´ ì—†ì„ ë•Œë„ í›„ì† ë¡œì§ì´ ì£½ì§€ ì•Šë„ë¡ ë³´ì •
    for must in ["íŒŒì¼ëª…", "ëŒ€ìƒêµ­", "ICT ìœ í˜•", "ì£¼ì œë¶„ë¥˜(ëŒ€)"]:
        if must not in df.columns:
            # ì—†ëŠ” ê²½ìš°ë¼ë„ ì°¨íŠ¸ ì „ì²´ê°€ ì£½ì§€ ì•Šê²Œ placeholder ìƒì„±
            df[must] = df.get(must, pd.Series(["-"] * len(df)))

    return df


@st.cache_data(show_spinner=False)
def load_from_path(path: str) -> pd.DataFrame:
    ext = os.path.splitext(path)[1].lower()
    if ext in [".xlsx", ".xls"]: return pd.read_excel(path)
    if ext == ".csv": return pd.read_csv(path, encoding_errors="ignore")
    return pd.read_excel(path)

def load_from_uploader(f) -> pd.DataFrame:
    name = f.name.lower()
    if name.endswith((".xlsx", ".xls")): return pd.read_excel(f)
    if name.endswith(".csv"): return pd.read_csv(f, encoding_errors="ignore")
    return pd.read_excel(f)

def load_from_csv_text(txt: str) -> pd.DataFrame:
    return pd.read_csv(io.StringIO(txt), encoding_errors="ignore")

@st.cache_data(show_spinner=False)
def discover_data_files(dirs: list[Path]) -> list[Path]:
    """ê°™ì€ í´ë”(+ ê´€ìš© ì„œë¸Œí´ë”)ì—ì„œ ì—‘ì…€/CSV í›„ë³´ íƒìƒ‰ & ìŠ¤ì½”ì–´ë§"""
    cands: list[Path] = []
    for base in dirs:
        if not base.exists(): continue
        for pat in ("*.xlsx", "*.xls", "*.csv"):
            cands.extend(sorted(base.glob(pat)))
    # ìŠ¤ì½”ì–´: íŒŒì¼ëª… íŒíŠ¸(ê°€ì¤‘) + ìµœì‹  ìˆ˜ì •ì‹œê°„
    def score(p: Path) -> tuple:
        name = p.name.lower()
        s = 0
        # í”„ë¡œì íŠ¸ì—ì„œ ìì£¼ ì“°ëŠ” íŒ¨í„´ ê°€ì¤‘ì¹˜
        if "df1" in name: s += 8
        if "ksp" in name: s += 6
        if "state_of_the_table" in name: s += 5
        if "export" in name or "table" in name: s += 2
        if name.startswith("~$") or name.endswith(".tmp"): s -= 100
        # ìµœì‹  íŒŒì¼ ìš°ì„ 
        return (-s, -p.stat().st_mtime)
    # ê°€ì¥ ë†’ì€ ì ìˆ˜(ìŒìˆ˜ ì •ë ¬ ë³´ì • ìœ„í•´ -s)ë¶€í„° ì˜¤ë¦„ì°¨ìˆœ â†’ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê±´ ì ìˆ˜ í° ìˆœì´ë¯€ë¡œ ë‹¤ì‹œ ì •ë ¬ ê¸°ì¤€ ì£¼ì˜
    # ìœ„ scoreì—ì„œ -s, -mtimeì„ ì¤¬ìœ¼ë‹ˆ "ì˜¤ë¦„ì°¨ìˆœ"ìœ¼ë¡œ ì •ë ¬í•˜ë©´ ì‹¤ì§ˆì ìœ¼ë¡œ ì ìˆ˜â†“ â†’ ì›í•˜ëŠ” ê±´ ë°˜ëŒ€.
    # ê°„ë‹¨íˆ ë³„ë„ keyë¡œ ë‹¤ì‹œ ì •ë ¬:
    def _rank_value(path: Path) -> int:
        name = path.name.lower()
        score = 0
        score += 8 if "df1" in name else 0
        score += 6 if "ksp" in name else 0
        score += 5 if "state_of_the_table" in name else 0
        score += 2 if ("export" in name or "table" in name) else 0
        return score
    
    cands = sorted(
        cands,
        key=lambda p: (-_rank_value(p), -p.stat().st_mtime)  # ì ìˆ˜â†“, ìµœê·¼íŒŒì¼â†‘
    )

    # ì¤‘ë³µ ì œê±°(ë™ì¼ ê²½ë¡œ ëŒ€ë¹„ ì•ˆì „)
    seen = set(); out = []
    for p in cands:
        if p.resolve() not in seen:
            out.append(p); seen.add(p.resolve())
    return out

# â”€â”€ UI: ì†ŒìŠ¤ ì„ íƒ(ìë™ì´ ê¸°ë³¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
src_mode = st.sidebar.radio(
    "ì†ŒìŠ¤ ì„ íƒ",
    ["ìë™(ê°™ì€ í´ë”)", "íŒŒì¼ ì—…ë¡œë“œ", "CSV ë¶™ì—¬ë„£ê¸°", "íŒŒì¼ ê²½ë¡œ"],
    index=0
)

# ìºì‹œ ë¦¬ë¡œë“œ
if st.sidebar.button("ë¡œë“œ/ìƒˆë¡œê³ ì¹¨", use_container_width=True):
    st.cache_data.clear()

df = None
auto_files = discover_data_files(SEARCH_DIRS)

if src_mode == "ìë™(ê°™ì€ í´ë”)":
    if auto_files:
        # í›„ë³´ê°€ ì—¬ëŸ¬ ê°œë©´ ì„ íƒ ë°•ìŠ¤ ì œê³µ(ê¸°ë³¸: ìµœìš°ì„  í›„ë³´)
        labels = [f"{p.name}  â€”  {p.parent.name}/  (ìˆ˜ì •: {pd.to_datetime(p.stat().st_mtime, unit='s'):%Y-%m-%d %H:%M})"
                  for p in auto_files]
        sel_idx = 0
        if len(auto_files) > 1:
            sel_idx = st.sidebar.selectbox("ìë™ íƒì§€ëœ íŒŒì¼", list(range(len(auto_files))),
                                           index=0, format_func=lambda i: labels[i])
        st.sidebar.caption(f"ê²½ë¡œ: `{auto_files[sel_idx]}`")
        df = load_from_path(str(auto_files[sel_idx]))
        df = normalize_columns(df)
        df = ensure_unique_columns(df)

    else:
        st.sidebar.info("ê°™ì€ í´ë”(ë˜ëŠ” ./data, ./assets)ì—ì„œ ì í•©í•œ ë°ì´í„° íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì†ŒìŠ¤ ë°©ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")

elif src_mode == "íŒŒì¼ ì—…ë¡œë“œ":
    up = st.sidebar.file_uploader("ì—‘ì…€(.xlsx/.xls) ë˜ëŠ” CSV ì—…ë¡œë“œ", type=["xlsx", "xls", "csv"])
    if up is not None:
        df = load_from_uploader(up)
        df = normalize_columns(df)
        df = ensure_unique_columns(df)


elif src_mode == "CSV ë¶™ì—¬ë„£ê¸°":
    pasted = st.sidebar.text_area("CSV ì›ë¬¸ ë¶™ì—¬ë„£ê¸°(í—¤ë” í¬í•¨)", height=160)
    if pasted.strip():
        df = load_from_csv_text(pasted)
        df = normalize_columns(df)
        df = ensure_unique_columns(df)


else:  # íŒŒì¼ ê²½ë¡œ
    # ìë™ í›„ë³´ê°€ ìˆìœ¼ë©´ ê¸°ë³¸ê°’ì„ ê·¸ ì¤‘ ì²« ë²ˆì§¸ë¡œ ë…¸ì¶œ(ì—†ìœ¼ë©´ DEFAULT ì‚¬ìš©)
    default_path = str(auto_files[0]) if auto_files else DEFAULT_DATA_PATH
    data_path = st.sidebar.text_input("ì—‘ì…€/CSV ê²½ë¡œ", default_path)
    if os.path.exists(data_path):
        df = load_from_path(data_path)
        df = normalize_columns(df)
        df = ensure_unique_columns(df)

        st.sidebar.caption(f"ê²½ë¡œ: `{Path(data_path).resolve()}`")

# ë°ì´í„° ì—†ìœ¼ë©´ ì¤‘ë‹¨
if df is None or df.empty:
    st.stop()

# í•„ìˆ˜ ì»¬ëŸ¼ ì§„ë‹¨
REQ = ["íŒŒì¼ëª…","ëŒ€ìƒêµ­","ëŒ€ìƒê¸°ê´€","ì£¼ìš” ë¶„ì•¼","ì§€ì›ê¸°ê´€","ì£¼ìš” ë‚´ìš©","ê¸°ëŒ€ íš¨ê³¼",
       "ìš”ì•½","ICT ìœ í˜•","ì£¼ì œë¶„ë¥˜(ëŒ€)","Hashtag","Hashtag_str","full_text"]
missing = [c for c in REQ if c not in df.columns]
if missing:
    st.warning(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")

with st.expander("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° / ì§„ë‹¨", expanded=False):
    st.write(f"í–‰ ìˆ˜: {len(df):,}  |  ê³ ìœ  ëŒ€ìƒêµ­: {df['ëŒ€ìƒêµ­'].nunique()}  |  ê³ ìœ  ICT ìœ í˜•: {df['ICT ìœ í˜•'].nunique()}")
    st.dataframe(df.head(25), use_container_width=True)
# --------------------- ë°ì´í„° ì…ë ¥ (ë) ---------------------
# ========================= ì „ì—­ ì»¬ëŸ¬ íŒ”ë ˆíŠ¸ =========================


# ---------- 1) df ë¡œë“œ ì´í›„ ----------
# ë°˜ë“œì‹œ dfê°€ ì´ë¯¸ ë¡œë“œëœ ë’¤ ì‹¤í–‰!
WB_ORDER   = [str(v).strip() for v in df["ICT ìœ í˜•"].fillna("ë¯¸ë¶„ë¥˜").astype(str).unique().tolist()]
SUBJ_ORDER = [str(v).strip() for v in df["ì£¼ì œë¶„ë¥˜(ëŒ€)"].fillna("ë¯¸ë¶„ë¥˜").astype(str).unique().tolist()]

# ---------- 2) ê¸°ë³¸ íŒ”ë ˆíŠ¸ ----------
_BASE_QUALS = (
    px.colors.qualitative.Set1
    + px.colors.qualitative.Set2
    + px.colors.qualitative.Set3
    + px.colors.qualitative.Dark24
    + px.colors.qualitative.Bold
    + px.colors.qualitative.Vivid
)

# ---------- 3) ìƒ‰ íŒŒì‹±/ë³´ì • ----------
def _parse_color_to_rgb01(c: str) -> Optional[Tuple[float, float, float]]:
    if not isinstance(c, str):
        return None
    s = c.strip()
    if s.startswith("#"):
        h = s[1:]
        if len(h) == 3: h = "".join(ch*2 for ch in h)
        if len(h) == 6:
            try:
                r = int(h[0:2], 16)/255; g = int(h[2:4], 16)/255; b = int(h[4:6], 16)/255
                return (r,g,b)
            except Exception: return None
    if s.lower().startswith("rgb"):
        nums = re.findall(r"[\d\.]+", s)
        if len(nums)>=3:
            r,g,b = [float(x) for x in nums[:3]]
            if max(r,g,b)>1: r,g,b = r/255,g/255,b/255
            return (r,g,b)
    try:
        from matplotlib.colors import to_rgb
        return to_rgb(s)
    except Exception:
        return None

def _to_hex_from_rgb01(rgb):
    r,g,b=[int(max(0,min(1,v))*255+0.5) for v in rgb]
    return f"#{r:02x}{g:02x}{b:02x}"

def _brighten_color(c: str, s_scale=1.3, l_shift=-0.05) -> str:
    rgb = _parse_color_to_rgb01(c)
    if rgb is None: return c
    h,l,s = colorsys.rgb_to_hls(*rgb)
    s = min(1, s*s_scale); l = min(1, max(0, l+l_shift))
    r2,g2,b2 = colorsys.hls_to_rgb(h,l,s)
    return _to_hex_from_rgb01((r2,g2,b2))

# ---------- 4) ì»¬ëŸ¬ ë§µ ----------
def make_color_map(names, base_colors=None, s_scale=1.3, l_shift=-0.05):
    if base_colors is None:
        base_colors = _BASE_QUALS
    if not base_colors:
        import plotly.express as px
        base_colors = px.colors.qualitative.Plotly
    cmap = {}
    cycle = itertools.cycle(base_colors)
    for n in names:
        if n not in cmap:
            raw = next(cycle)
            cmap[n] = _brighten_color(raw, s_scale=s_scale, l_shift=l_shift)
    return cmap

# ---------- 5) ìµœì¢… ìƒì„± ----------
COLOR_WB   = make_color_map(WB_ORDER)
COLOR_SUBJ = make_color_map(SUBJ_ORDER)


def _font_path_safe():
    return GLOBAL_FONT_PATH or find_korean_font()  # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ None

SENT_SPLIT_RE = re.compile(r"(?<=[\.!\?]|[ã€‚ï¼ï¼Ÿ]|[â€¦]|[;]|[ã†]|[Â·]|[Â·\s]|[â€â€™\"\'])\s+|(?<=[\.\?])(?=[ê°€-í£A-Za-z0-9])")
KOR_END = "ë‹¤ë‹¤ìš”ìš”í•¨ìŒì„ë‹ˆê¹Œë‹ˆê°€ë¼ë¥¼ì—ì—ì„œì˜ìœ¼ë¡œë¡œë‹¤ë˜ì—ˆìœ¼ë©°í–ˆê³ í•˜ë©°"

def split_sentences(txt: str, max_len: int = 500) -> list[str]:
    """í•œêµ­ì–´/ì˜ë¬¸ í˜¼í•© ë¬¸ì¥ ë¶„í•  + ê³¼ë„í•˜ê²Œ ê¸´ ë¬¸ì¥ ìë¥´ê¸°"""
    if not isinstance(txt, str) or not txt.strip():
        return []
    # 1ì°¨ ë¶„í• 
    parts = re.split(r'(?<=[\.!\?])\s+|[ã€‚]|[ï¼]|[ï¼Ÿ]|\n+', txt)
    out = []
    for p in parts:
        p = p.strip()
        if not p: 
            continue
        # ë„ˆë¬´ ê¸¸ë©´ í‚¤ì›Œë“œ ë§¤ì¹­ ì „ì— 2ì°¨ ë¶„í•  ì‹œë„
        if len(p) > max_len:
            chunks = re.split(r'[,;Â·]|(?<=\))\s+|(?<=\])\s+', p)
            for c in chunks:
                c = c.strip()
                if 30 <= len(c) <= max_len:
                    out.append(c)
        else:
            out.append(p)
    return [s for s in out if len(s) >= 20]

def shorten_around_keyword(sent: str, kw: str, half: int = 140) -> str:
    """í‚¤ì›Œë“œ ê¸°ì¤€ ì¢Œìš°ë¡œ ë¬¸ë§¥ë§Œ ë‚¨ê²¨ 280ì ë‚´ë¡œ ì¶•ì•½"""
    i = sent.lower().find(kw.lower())
    if i < 0:
        return sent[:280] + ("â€¦" if len(sent) > 280 else "")
    left = max(0, i - half)
    right = min(len(sent), i + len(kw) + half)
    clip = (("â€¦" if left > 0 else "") + sent[left:right] + ("â€¦" if right < len(sent) else ""))
    return clip

def highlight(text: str, kw: str) -> str:
    pat = re.compile(re.escape(kw), re.IGNORECASE)
    return pat.sub(lambda m: f"<mark style='background:#fff3a1; padding:0 2px; border-radius:4px'>{m.group(0)}</mark>", text)

# === êµì²´: sample_sentences_for_keyword ===
def sample_sentences_for_keyword(df_in: pd.DataFrame, kw: str, text_cols: list[str], 
                                 per_kw: int = 3, seed: int = 42) -> list[tuple[str, str]]:
    """
    kwë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì¥ì„ ìµœëŒ€ per_kwê°œ ìƒ˜í”Œë§.
    ë°˜í™˜: [(íŒŒì¼ëª…, ë¬¸ì¥_HTML), ...]
    """
    # âœ” ì§€ì—­ ì„í¬íŠ¸ë¡œ NameError ë°©ì§€
    from random import Random

    # âœ” ì‹œë“œ ìºìŠ¤íŒ…(ìˆ«ì/ë¬¸ì ìƒê´€ì—†ì´ ì•ˆì „)
    try:
        base_seed = int(seed)
    except Exception:
        base_seed = 42

    rng = Random(base_seed + (hash(str(kw)) % 10000))

    texts = []
    cols = [c for c in text_cols if c in df_in.columns]
    if not cols:
        return []

    for _, row in df_in.iterrows():
        blob = " ".join(str(row.get(c, "") or "") for c in cols).strip()
        if not blob:
            continue
        sents = split_sentences(blob)
        hits = [s for s in sents if kw.lower() in s.lower()]
        if hits:
            fname = str(row.get("íŒŒì¼ëª…") or row.get("Filename") or "").strip()
            rng.shuffle(hits)
            for s in hits[:per_kw * 2]:   # ì•½ê°„ ë„‰ë„‰íˆ ê°€ì ¸ì™€ ì¤‘ë³µ ì œê±°/ì¶•ì•½ í›„ ì„ íƒ
                texts.append((fname, s))

    # ì¤‘ë³µ ì œê±°
    seen, uniq = set(), []
    for fn, s in texts:
        key = (fn, s.strip().lower())
        if key in seen:
            continue
        seen.add(key)
        uniq.append((fn, s))

    # ìµœì¢… ìƒ˜í”Œ
    rng.shuffle(uniq)
    uniq = uniq[:per_kw]

    out = []
    for fn, s in uniq:
        clip = shorten_around_keyword(s, kw, half=140)
        out.append((fn, highlight(clip, kw)))
    return out




USE_NOUN_FILTER: bool = True   # â† ëª…ì‚¬ í•„í„° ì‚¬ìš© ì—¬ë¶€(ì‚¬ì´ë“œë°” í† ê¸€ë¡œ ë°”ê¿”ë„ ë¨)

# ==== ê·œì¹™ ê¸°ë°˜ ëª…ì‚¬ í•„í„° (kiwi ë¶ˆí•„ìš”) ====

_HANGUL_RE = re.compile(r"[ê°€-í£]+")
_TOKEN_RE  = re.compile(r"[A-Za-z]+(?:[-_][A-Za-z0-9]+)*|[0-9]+(?:\.[0-9]+)?|[ê°€-í£]+")
_KO_POSTFIX_DROP = (
    "í•˜ë‹¤","ì ì¸","ìŠ¤ëŸ¬ìš´","ìŠ¤ëŸ¬ì›€","ìŠ¤ëŸ½ë‹¤","ë˜ë‹¤","ì‹œí‚¤ë‹¤","ë˜ë©°","í•˜ë©°","í•˜ë‹¤ê°€",
    "ìœ¼ë¡œ","ë¶€í„°","ì²˜ëŸ¼","ê¹Œì§€","ëŒ€ë¡œ","ë¼ì„œ","ë©´ì„œ","ë©´ì„œë„","í•˜ë©´ì„œ",
    "ì—ì„œ","ì—ê²Œ","ì—ê²Œì„œ","í•œí…Œ","ì´ë¼ì„œ","ì´ë¼ë„",
)
_KO_SINGLE_PARTICLE = set(list("ì€ëŠ”ì´ê°€ì„ë¥¼ì˜ì—ì™€ê³¼ë„ë§Œ"))
_EN_SHORT_MIN = 2

def _strip_ko_suffix(tok: str) -> str:
    for suf in _KO_POSTFIX_DROP:
        if tok.endswith(suf) and len(tok) > len(suf) + 1:
            tok = tok[:-len(suf)]
            break
    if len(tok) >= 3 and tok[-1] in _KO_SINGLE_PARTICLE:
        tok = tok[:-1]
    return tok

def _valid_token(tok: str) -> bool:
    if not tok: return False
    if re.fullmatch(r"[0-9]+(?:\.[0-9]+)?", tok): return False
    if len(tok) == 1: return False
    return True

def extract_nouns_korean(text: str) -> str:
    if not isinstance(text, str) or not text.strip(): return ""
    toks = _TOKEN_RE.findall(text)
    out = []
    for t in toks:
        if _HANGUL_RE.fullmatch(t):
            t = _strip_ko_suffix(t).strip()
            if not t: continue
            if t.lower() in STOP_ALL: continue
            if _valid_token(t): out.append(t)
        else:
            tl = t.lower()
            if len(tl) < _EN_SHORT_MIN: continue
            if tl in STOP_ALL: continue
            out.append(tl)
    return " ".join(out)

def _prep_docs(df_in: pd.DataFrame, text_cols: list[str]) -> list[str]:
    cols = [c for c in (text_cols or []) if c in df_in.columns]
    out = []
    for _, r in df_in.iterrows():
        t = " ".join(str(r.get(c, "") or "") for c in cols).strip()
        if not t: continue
        if USE_NOUN_FILTER:
            t = extract_nouns_korean(t)
        out.append(t)
    return out




# ===== ê°•í•œ stop/í•„í„° =====
GENERIC_KO = {
    "ê²½ì œ","ì‚¬íšŒ","ì •ì±…","ë°ì´í„°","ë””ì§€í„¸","ì„œë¹„ìŠ¤","ì‹œì¥","ìš´ì˜","í˜„í™©","ì „ëµ","ë°©ì•ˆ","ë„ì…","ê°œì„ ","êµ¬ì¶•","ì²´ê³„",
    "ê¸°ë°˜","ì¤‘ì¥ê¸°","ë³´ê³ ","ë¶„ì„","ì§€ì›","ì •ë¶€","ê³µê³µ","í”„ë¡œì íŠ¸","ë¡œë“œë§µ","ë¹„ì „","í™œìš©","ê°•í™”","í™•ëŒ€","í‰ê°€","ê³„íš",
    "ì‚¬ë¡€","í˜„ì§€","ê³¼ì œ","ì¸í”„ë¼","í”Œë«í¼","ì‹œìŠ¤í…œ","í¬í„¸","ì¡°ë‹¬","ë²•ì œ","ì œë„","ê°€ì´ë“œë¼ì¸","ê¸°íš","ì¶”ì§„","ì„±ê³¼",
    "ê³¼í•™ê¸°ìˆ ","êµìœ¡","ë³´ê±´","ì•ˆì „","ë³´ì•ˆ","ì „ìì •ë¶€","ìŠ¤ë§ˆíŠ¸","í˜ì‹ ","ì—°êµ¬","ì¤‘ì†Œê¸°ì—…","ì‚°ì—…","ë„ì‹œ","ì„¼í„°","í”Œë ›í¼",
    "í˜„ì•ˆ","ìë£Œ","ë¶„ì•¼","ì§€ì›ê¸°ê´€","ëŒ€ìƒê¸°ê´€","ì£¼ì œë¶„ë¥˜","ict","ICT","AI","ì¸ê³µì§€ëŠ¥","ë¹…ë°ì´í„°","í´ë¼ìš°ë“œ", "ê¸°ëŒ€ë©ë‹ˆë‹¤",
    "ìƒì‚°ì„±", "IT", "ëŒ€í•œ", "ìì›", "íˆ¬ì", "ë””ì§€í„¸í™”", "ë¬´ì—­", "ë²•ì •", "ì¬ì •", "ì •ë³´í™”", "ë²•ì ", "ì¸ë ¥", "ë¯¼ê°„", "ë§ì¶¤í˜•", "í–‰ì •", "ë¹„ì¦ˆë‹ˆìŠ¤", "ì œì¡°ì—…", "ê±´ì„¤", "ê´‘ì—…", "BIM", "ì—ë„ˆì§€", "ë¶ˆê°€ë¦¬ì•„", "ì§€ì†ê°€ëŠ¥í•œ", "IP", "ì¤‘ë‚¨ë¯¸", "ê³µì¥", "ì–‘ì„±", "ìš°ì¦ˆë² í‚¤ìŠ¤íƒ„", "ë†’ì´ê³ ", "ì´ëŸ¬í•œ", "ìœ ì¹˜", "ì „ë¬¸",
    "ì •ì±…ì ", "ì´‰ì§„í• ", "ì„±ê³µ", "ë£¨ë§ˆë‹ˆì•„", "íŠ¹í—ˆ", "ìƒí™©ì…ë‹ˆë‹¤", "ê²€í† í•˜ì—¬", "í™˜ê²½", "ìƒíƒœê³„", "ì˜¨ë‘ë¼ìŠ¤", "êµ¬ì¶•í•˜ì—¬", "ê±°ë²„ë„ŒìŠ¤", "í•„ë¦¬í•€", "ì‹œë²”", "ì‹¬ì‚¬", "ì—…ë¬´", "ë°ì´í„°ë² ì´ìŠ¤", "ì˜¨ë¼ì¸", "ìˆìœ¼ë©°", "ì‚°í•™ì—°", "ì „ì", "ì‚¬ì´ë²„", "ë‹¤ë£¹ë‹ˆë‹¤", "ê°ì‚¬", "êµí†µ", "ë‹´ê³ ", "ì¡°ì„¸", "ì˜ˆì‚°", "ê°•ì¡°í•©ë‹ˆë‹¤", "ì„¸ìˆ˜",
    "íˆ¬ëª…ì„±", "ì¸ì„¼í‹°ë¸Œ", "ë²•ë¥ ", "ê¸°ìˆ ì ", "í•¨ê»˜", "ì¸ë„ë„¤ì‹œì•„", "ì„¸ë¬´", "ì¡°ì§", "ë†’ì—¬", "ë°©ê¸€ë¼ë°ì‹œ", "ìˆ˜ì§‘", "í™•ë³´í•˜ê³ ", "ë©•ì‹œì½”", "íš¨ìœ¨ì„±", "ì œê³µí•©ë‹ˆë‹¤", "ì í•©í•œ", "êµ­ì œ", "ì»¨ì„¤íŒ…", "ë¶„ì„í•œë‹¤", "ê³µë¬´ì›", "ë‚©ì„¸ì", "ë‚©ì„¸", "ê°€ëŠ¥í•˜ê²Œ", "í¬ê²Œ", "ì„ ì§„", "í–¥ìƒ", "ê³µê²©", "ì´ì§‘íŠ¸", "ì•…ì„±", "ì§•ìˆ˜",
    "ì½”ë“œ", "ì¸ì ìì›", "ê°•ì¡°í•¨", "ì œì‹œí•¨", "ì¬í™œìš©", "ìš”ì•½ë¨", "ì‹œìŠ¤í…œì ", "ìŠ¬ë¡œë°”í‚¤ì•„", "ì¬ì„¤ê³„", "ì¤‘ë³µ", "ìˆœí™˜", "í¬í•¨ë¨", "ë¬¼ë¥˜", "ì´ë£¨ì–´ì§€ê³ ", "ì§„í–‰", "ì§„ë£Œ", "ë„ì…í•˜ê¸°", "í†µì‹ ", "ì›ê²©", "ë¹ ë¥´ê²Œ", "ë„ì¶œí•©ë‹ˆë‹¤", "í† ì§€", "ë¹„í˜„ê¸ˆ", "ë†ì—…", "ë°©ì†¡", "ì¶•ì‚°ë¬¼", "ê²½ë§¤", "ê²½ì˜", "ì§„ë£Œë¹„", "ì›ì‚°ì§€", "ê³µê¸°ì—…",
    "ê´€ê´‘", "êµì‚¬", "ì£¼íŒŒìˆ˜", "ê²½ë³´", "ë§Œì„±ì§ˆí™˜", "ìš”ë¥´ë‹¨", "ì¬í•´ë³µêµ¬", "Konza", "í™ìˆ˜", "ê°€ë­„", "ì‹ëŸ‰", "JONEPS", "ì¦ëª…", "ë¹„ê¸°ìˆ ì ", "ë¯¸ë””ì–´", "ì „ìíšŒê³„ê°ì‚¬", "ìˆ˜ìì›", "KLIS", "ëˆ„ë½", "ê±´ì„¤ê¸°ìˆ ", "ì˜ë£Œì§„", "ê³µê³µì˜ë£Œ", "ì„¤ê³„í•˜ì˜€ìŠµë‹ˆë‹¤", "í˜„ê¸ˆì˜ìˆ˜ì¦", "ê³µê³µì¬ì‚°", "ì²˜ë¶„", "ì´ì¤‘í™”", "ì•„ë‚ ë¡œê·¸",
    "êµ­ì„¸ì²­", "ìë¬¸", "ì‹œë¯¼ë“¤", "ë¬¸ì„œ", "ì„¸ë¥´ë¹„ì•„", "ê°€ì…", "ì°½ì¶œ", "ê¸°ì—…ë“¤", "crm", "ìœ¡ì„±", "ì¥ì• ", "ì¸ì¬", "í˜‘ì—…", "ê¸°ì´ˆ", "ì—…ê·¸ë ˆì´ë“œ", "ë°±ì—…", "ê¸°ê´€ë³„", "ìœ ì—­", "ì˜ë£Œ", "í™˜ì", "ì•ˆë³´", "ì§€ì†ê°€ëŠ¥ì„±", "ì½˜í…ì¸ ", "ë„˜ì–´", "ì¦ì§„", "ê³µì •í•œ", "ë¶ˆí‰ë“±", "í”¼í•´", "ì¹ ë ˆ", "ê·¹ë³µí•˜ê¸°", "íšŒê³„", "ì¼ë¶€", "í–¥í›„", "ì œê³µí•œë‹¤", "ì¬ì •ê´€ë¦¬",
    "ì§€ì›í•œë‹¤", "ë¶€ê°€ê°€ì¹˜", "ìƒì‚°", "êµ­ê°€ë¥¼", "ë¶„ì„í•©ë‹ˆë‹¤", "ìš”ì¸", "ì¤‘ì ", "ë¶€ì¡±", "ê³µê³µë¶€ë¬¸", "ê³µê³µì¡°ë‹¬", "ê°•í™”í•˜", "ì˜¤í”„ë¼ì¸", "íƒˆì„¸", "ì¦ëŒ€", "íƒ€ì§€í‚¤ìŠ¤íƒ„", "ë‹¨ì§€", "ë¦¬íˆ¬ì•„ë‹ˆì•„", "ë§ë ˆì´ì‹œì•„", "ëŒ€ì‘ì±…", "íƒ„ìë‹ˆì•„", "db", "ì´í–‰ê³„íš", "ë²¨ë¼ë£¨ìŠ¤", "ëª¨ë¡œì½”", "ë¸Œë¼ì§ˆ", "ëª°ë„ë°”", "ê²½ì˜í‰", "ìš°í¬ë¼ì´ë‚˜", "ì¡°ì§€ì•„", "ì¹´íƒ€ë¥´", "ë¥´ì™„ë‹¤",
    "ì¼€ëƒ", "í˜¸ì£¼", "íƒœêµ­", "ê±´ì„¤ì‚°ì—…", "ì•„í¬ë¼", "ëŒ€ì—­", "í•™ìƒ", "DMA", "ì§€ì§ˆ", "ì†Œë“ì„¸", "ìš´ì˜ì", "ìë©”ì´ì¹´", "í•˜ë“œì›¨ì–´", "ìˆ˜ìˆ˜ë£Œ", "ìœ¤ë¦¬", "ì•„í”„ë¦¬ì¹´", "ë„ì…ë¥ ", "ìœ„í—˜ê¸°ë°˜", "ê¸ˆìœµê²°ì œì›", "ë¬¸í•´ë ¥", "D-IP", "ë²•ì¸ì„¸", "ì˜¤í”„", "ID", "ìƒì•”"
}
GENERIC_EN = {
    "data","digital","service","services","system","systems","platform","portal","project","program","policy","policies",
    "plan","roadmap","model","models","evaluation","implementation","phase","final","interim","infrastructure","innovation"
}
STRONG_STOP = {s.lower() for s in (STOP | BASE_STOP | STOP_CUSTOM | GENERIC_KO | GENERIC_EN)}

# ì´ë¯¸ ì •ì˜ëœ STOP/BASE_STOP/STOP_CUSTOM/GENERIC_KO/GENERIC_ENì„ í•œë° ëª¨ì•„ í†µí•©
def _collect_stop_all():
    STOP_ALL = set()
    for s in [globals().get("STOP"), globals().get("BASE_STOP"),
              globals().get("STOP_CUSTOM"), globals().get("GENERIC_KO"),
              globals().get("GENERIC_EN"), globals().get("STOP_LOW_ALL")]:
        if s:
            STOP_ALL |= {str(w).lower() for w in s}
    return STOP_ALL

STOP_ALL = _collect_stop_all()

def _normalize_token(t: str) -> str:
    t = re.sub(r"[\"'â€™â€œâ€()\[\]{}<>]", "", str(t)).strip()
    t = re.sub(r"\s{2,}", " ", t)
    return t

def _is_valid_kw(t: str) -> bool:
    if not t or len(t) < 2: return False
    if re.fullmatch(r"\d+(\.\d+)?", t): return False
    # í’ˆì‚¬ì  íŒ¨í„´ ì œê±°: 'í•˜ë‹¤','ì ì¸','ìœ¼ë¡œ','í•˜ì—¬' ë“±
    if re.search(r"(í•˜ë‹¤|ì ì¸|ìœ¼ë¡œ|í•˜ë©°|í•˜ê³ |ì—ì„œ|ë˜ì–´|í•˜ê³ ì|ëœë‹¤|ì‹œí‚¤ë‹¤|ìˆë‹¤|ëœë‹¤)$", t):
        return False
    if re.search(r"[ì€ëŠ”ì´ê°€ì„ë¥¼ì˜ì—ëŠ”ë¡œê³¼ì™€ë„ë§Œ]$", t): return False
    return (t.lower() not in STRONG_STOP)


def contrastive_keywords_tfidf(
    docs_class: list[str],
    docs_neg: list[str],
    top_n: int = 60,
    ngram_bonus=(0.10, 0.20),
    eps: float = 1e-6,
) -> list[tuple[str, float]]:
    """
    score = log((tf_c/len_c + eps) / (tf_n/len_n + eps)) * log(1 + N / df)
            + n-ê·¸ë¨ ë³´ë„ˆìŠ¤
    - ë¶ˆìš©ì–´(STOP_ALL) ì ìš©
    - KeyBERT/ì„ë² ë”© ì—†ì´ 'í´ë˜ìŠ¤ vs ë‚˜ë¨¸ì§€' ëŒ€ë¹„ë¡œ êµ¬ë¶„ë ¥ í™•ë³´
    - ì˜ì–´ í‚¤ì›Œë“œëŠ” ìµœì¢… ì¶œë ¥ ì‹œ ëŒ€ë¬¸ìë¡œ ë³€í™˜
    """
    def _tokenize_for_vocab(docs):
        out = []
        for d in docs:
            if not isinstance(d, str) or not d.strip():
                out.append([])
                continue
            toks = re.split(r"\s+", d.strip())
            toks = [t.lower() for t in toks if t and t.lower() not in STOP_ALL]
            out.append(toks)
        return out

    toks_c = _tokenize_for_vocab(docs_class)
    toks_n = _tokenize_for_vocab(docs_neg)

    N_docs = len(toks_c) + len(toks_n)
    df_term, cnt_c, cnt_n = Counter(), Counter(), Counter()
    len_c = len_n = 0

    for toks in toks_c + toks_n:
        if not toks: continue
        for t in set(toks): df_term[t] += 1

    for toks in toks_c:
        cnt_c.update(toks); len_c += len(toks)
    for toks in toks_n:
        cnt_n.update(toks); len_n += len(toks)

    len_c = max(len_c, 1); len_n = max(len_n, 1)

    picked = []
    for t in set(cnt_c.keys()) | set(cnt_n.keys()):
        if t in STOP_ALL: 
            continue
        tfc = cnt_c[t] / len_c
        tfn = cnt_n[t] / len_n
        lift = np.log((tfc + eps) / (tfn + eps))
        idf  = np.log(1.0 + N_docs / max(1, df_term[t]))
        score = lift * idf
        n = len(t.split())
        if n == 2: score += ngram_bonus[0]
        elif n >= 3: score += ngram_bonus[1]
        picked.append((t, float(score)))

    # === â–¼ ì˜ë¬¸ ë‹¨ì–´ëŠ” ëŒ€ë¬¸ìí™” â–¼ ===
    def _upper_if_english(term: str) -> str:
        if re.fullmatch(r"[a-zA-Z0-9\-\_]+", term):  # ì˜ë¬¸/ìˆ«ì/í•˜ì´í”ˆ ì¡°í•©ì´ë©´
            return term.upper()
        return term

    picked.sort(key=lambda x: x[1], reverse=True)
    uniq, seen = [], []
    for term, sc in picked:
        low = term
        if any(low in s or s in low for s in seen):
            continue
        seen.append(low)
        uniq.append((_upper_if_english(term), sc))   # â† ì—¬ê¸°ì— ì ìš©
        if len(uniq) >= top_n:
            break
    return uniq




def mmr_select_text(candidates: list[tuple[str, float]], k: int, lambda_div: float = 0.65) -> list[str]:
    if not candidates: return []
    candidates = sorted(candidates, key=lambda x: x[1], reverse=True)
    toks = {t: set(t.lower().split()) for t, _ in candidates}
    def sim(a: str, b: str) -> float:
        A, B = toks[a], toks[b]
        if not A or not B: return 0.0
        inter = len(A & B); union = len(A | B)
        return inter/union if union else 0.0
    selected = [candidates[0][0]]
    rest = [t for t, _ in candidates[1:]]
    while len(selected) < min(k, len(candidates)) and rest:
        best, best_score = None, -1e9
        for t in rest:
            rel = next(s for (tt, s) in candidates if tt == t)
            max_sim = max(sim(t, s) for s in selected) if selected else 0.0
            mmr = (1 - lambda_div) * rel - lambda_div * max_sim
            if mmr > best_score:
                best, best_score = t, mmr
        selected.append(best); rest.remove(best)
    return selected[:k]



def _docs_texts(df_in: pd.DataFrame, text_cols: List[str]) -> List[str]:
    cols = [c for c in (text_cols or []) if c in df_in.columns]
    if not cols:
        return []
    out = []
    for _, r in df_in.iterrows():
        blob = " ".join(str(r.get(c, "") or "") for c in cols).strip()
        if blob:
            out.append(blob)
    return out








def _contains_kw_doclevel(txt: str, kw: str) -> bool:
    # ì˜ë¬¸ì€ ë‹¨ì–´ê²½ê³„, í•œêµ­ì–´/í˜¼í•©ì€ ì„œë¸ŒìŠ¤íŠ¸ë§ë„ í—ˆìš©
    pat = re.compile(rf"(?i)(\b{re.escape(kw)}\b)|({re.escape(kw)})")
    return bool(pat.search(txt))

def _doc_share(docs: list[str], kw: str) -> tuple[int, int, float]:
    n = len(docs) or 1
    c = sum(1 for t in docs if _contains_kw_doclevel(t, kw))
    return c, n, c / n

def _monroe_log_odds_z(c_a, n_a, c_b, n_b, alpha=0.5):
    pa = (c_a + alpha) / (n_a + 2*alpha)
    pb = (c_b + alpha) / (n_b + 2*alpha)
    logodds = math.log(pa/(1-pa+1e-12) + 1e-12) - math.log(pb/(1-pb+1e-12) + 1e-12)
    va = 1.0 / max(1e-9, (c_a + alpha)) + 1.0 / max(1e-9, (n_a - c_a + alpha))
    vb = 1.0 / max(1e-9, (c_b + alpha)) + 1.0 / max(1e-9, (n_b - c_b + alpha))
    return logodds / math.sqrt(va + vb)

@lru_cache(maxsize=2048)
def _embed_phrase(phrase: str):
    m = get_sbert()
    if m is None: return None
    return m.encode(phrase, normalize_embeddings=True)

def _centroid(docs: list[str]):
    m = get_sbert()
    if m is None or not docs: return None
    import numpy as _np
    embs = m.encode(docs, normalize_embeddings=True)
    return _np.mean(embs, axis=0)

def _cos(a, b):
    import numpy as _np
    if a is None or b is None: return 0.0
    return float(_np.clip(_np.dot(a, b), -1.0, 1.0))


def rerank_with_negative_contrast(
    candidates: list[tuple[str, float]],  # (kw, tfidf_score)
    df_all: pd.DataFrame,
    df_class: pd.DataFrame,
    df_negative: pd.DataFrame,
    text_cols: list[str],
    w_lift=0.65,
    w_logodds=0.35,
    # w_embed ì œê±°
    unigram_penalty=0.25,
    bigram_bonus=0.10,
    trigram_bonus=0.15
) -> list[tuple[str, float, float, float, float, float]]:
    """
    ë°˜í™˜: [(kw, final, lift, logodds_z, emb_delta(=0), kb_like_score)]
    ì—¬ê¸°ì„œ kb_like_scoreëŠ” TF-IDF scoreë¥¼ ëŒ€ì….
    """
    docs_cls = _prep_docs(df_class, text_cols)
    docs_neg = _prep_docs(df_negative, text_cols)

    out = []
    for kw, tfidf_sc in candidates:
        if not _is_valid_kw(kw):
            continue

        # ë¬¸ì„œ ë‚´ í¬í•¨ ë¹„ìœ¨ (í´ë˜ìŠ¤/ë‚˜ë¨¸ì§€)
        hit_c, n_c, share_c = _doc_share(docs_cls, kw)
        hit_n, n_n, share_n = _doc_share(docs_neg, kw)
        lift = (share_c + 1e-6) / (share_n + 1e-6)
        z = _monroe_log_odds_z(hit_c, n_c, hit_n, n_n, alpha=0.5)

        ngram = len(kw.split())
        gram_bonus = (trigram_bonus if ngram >= 3 else bigram_bonus if ngram == 2 else -unigram_penalty)

        # ì„ë² ë”© í•­ì€ 0ìœ¼ë¡œ
        emb_delta = 0.0
        final = w_lift * float(np.log(max(lift, 1e-6))) + w_logodds * z + gram_bonus + 0.03 * tfidf_sc

        out.append((kw, final, lift, z, emb_delta, tfidf_sc))

    out.sort(key=lambda x: x[1], reverse=True)
    return out





# ========================= êµ­ê°€ ë¸Œë¦¬í”„(ìš”ì•½) ì…ë ¥ =========================
st.sidebar.header("êµ­ê°€ ë¸Œë¦¬í”„(ìš”ì•½)")

@st.cache_data(show_spinner=False)
def load_country_briefs_from_ipynb_bytes(b: bytes) -> dict:
    """ipynb ì•ˆì˜ code cellì—ì„œ 'briefs = {...}' ë”•ì…”ë„ˆë¦¬ë¥¼ ì°¾ì•„ ë°˜í™˜"""
    import json
    nb = json.loads(b.decode("utf-8"))
    for cell in nb.get("cells", []):
        if cell.get("cell_type") == "code":
            code = "".join(cell.get("source", []))
            if "briefs" in code:
                ns = {}
                try:
                    exec(code, {}, ns)
                except Exception:
                    ns = {}
                briefs = ns.get("briefs", {})
                if isinstance(briefs, dict):
                    return briefs
    return {}

@st.cache_data(show_spinner=False)
def load_country_briefs_auto(app_dir: Path) -> tuple[dict, str | None]:
    """
    ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”(ë˜ëŠ” ê´€ìš© ì„œë¸Œí´ë”)ì—ì„œ CountryBriefs.ipynb ìë™ íƒìƒ‰
    ë°˜í™˜: (briefs_map, ì‚¬ìš©í•œ ê²½ë¡œ ë˜ëŠ” None)
    """
    candidates = [
        app_dir / "CountryBriefs.ipynb",
        app_dir / "assets" / "CountryBriefs.ipynb",
        app_dir / "data" / "CountryBriefs.ipynb",
    ]
    for p in candidates:
        if p.exists():
            try:
                return load_country_briefs_from_ipynb_bytes(p.read_bytes()), str(p)
            except Exception:
                pass
    return {}, None

# í˜„ì¬ ì•± ë””ë ‰í„°ë¦¬(ìŠ¤íŠ¸ë¦¼ë¦¿ì—ì„œ __file__ì´ ì •ìƒì ìœ¼ë¡œ ë“¤ì–´ì˜¨ë‹¤. ë…¸íŠ¸ë¶/REPL ëŒ€ë¹„ fallback ë™ì‘)
APP_DIR = Path(__file__).resolve().parent if "__file__" in globals() else Path.cwd()

brief_mode = st.sidebar.radio("ì†ŒìŠ¤", ["ìë™(ê°™ì€ í´ë”)", "íŒŒì¼ ì—…ë¡œë“œ", "ë¹„í™œì„±í™”"], index=0, horizontal=True)
if st.sidebar.button("ë¸Œë¦¬í”„ ë¦¬ë¡œë“œ", use_container_width=True):
    st.cache_data.clear()

briefs_map: dict = {}
brief_path_used: str | None = None

if brief_mode == "ìë™(ê°™ì€ í´ë”)":
    briefs_map, brief_path_used = load_country_briefs_auto(APP_DIR)
    if brief_path_used:
        st.sidebar.caption(f"ê²½ë¡œ: `{brief_path_used}`")
    else:
        st.sidebar.info("ê°™ì€ í´ë”ì—ì„œ `CountryBriefs.ipynb`ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
elif brief_mode == "íŒŒì¼ ì—…ë¡œë“œ":
    upb = st.sidebar.file_uploader("CountryBriefs.ipynb ì—…ë¡œë“œ", type=["ipynb"])
    if upb is not None:
        briefs_map = load_country_briefs_from_ipynb_bytes(upb.read())
# ë¹„í™œì„±í™”ë©´ briefs_map == {}


# ========================= ICT ìœ í˜• ë¸Œë¦¬í”„(ìš”ì•½) ì…ë ¥ =========================
st.sidebar.header("ICT ìœ í˜• ë¸Œë¦¬í”„(ìš”ì•½)")

@st.cache_data(show_spinner=False)
def load_wb_briefs_from_ipynb_bytes(b: bytes) -> dict:
    """ipynb ì•ˆì˜ code cellì—ì„œ 'wb_briefs' (ë˜ëŠ” 'briefs', 'class_briefs') ë”•ì…”ë„ˆë¦¬ ì°¾ì•„ ë°˜í™˜"""
    import json
    nb = json.loads(b.decode("utf-8"))
    for cell in nb.get("cells", []):
        if cell.get("cell_type") != "code":
            continue
        code = "".join(cell.get("source", []))
        ns = {}
        try:
            exec(code, {}, ns)
        except Exception:
            continue

        # ìš°ì„ ìˆœìœ„: wb_briefs > briefs > class_briefs > ê·¸ ì™¸ 'dict' í›„ë³´
        for key in ["wb_briefs", "briefs", "class_briefs"]:
            obj = ns.get(key)
            if isinstance(obj, dict):
                return obj

        # í˜¹ì‹œ ëª¨ë¥¼ ê¸°íƒ€ ë”•ì…”ë„ˆë¦¬ë„ ìŠ¤ìº”
        for k, v in ns.items():
            if isinstance(v, dict) and k.lower().endswith("briefs"):
                return v
    return {}


@st.cache_data(show_spinner=False)
def load_wb_briefs_auto(app_dir: Path) -> tuple[dict, str | None]:
    """
    ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”/ìì£¼ ì“°ëŠ” ì„œë¸Œí´ë”ì—ì„œ WB_ClassBriefs ë…¸íŠ¸ë¶ ìë™ íƒìƒ‰
    ë°˜í™˜: (wb_briefs_map, ì‚¬ìš©í•œ ê²½ë¡œ ë˜ëŠ” None)
    """
    candidates = []
    for base in [app_dir, app_dir / "assets", app_dir / "data"]:
        for pat in [
            "WB_ClassBriefs.ipynb", "WBClassBriefs.ipynb",
            "wb_class_briefs.ipynb", "wbclass_briefs.ipynb",
            "*WB*Class*Brief*.ipynb",
        ]:
            candidates += list(base.glob(pat))

    for p in candidates:
        try:
            return load_wb_briefs_from_ipynb_bytes(p.read_bytes()), str(p)
        except Exception:
            continue
    return {}, None


wb_brief_mode = st.sidebar.radio("ì†ŒìŠ¤ (ICT ìœ í˜•)", ["ìë™(ê°™ì€ í´ë”)", "íŒŒì¼ ì—…ë¡œë“œ", "ë¹„í™œì„±í™”"],
                                 index=0, horizontal=True)

# 'ë¸Œë¦¬í”„ ë¦¬ë¡œë“œ' ë²„íŠ¼ì€ ìœ„ì—ì„œ st.cache_data.clear()ë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ì—¬ê¸°ì—ë„ ì ìš©ë¨
wb_briefs_map: dict = {}
wb_brief_path_used: str | None = None

if wb_brief_mode == "ìë™(ê°™ì€ í´ë”)":
    wb_briefs_map, wb_brief_path_used = load_wb_briefs_auto(APP_DIR)
    if wb_brief_path_used:
        st.sidebar.caption(f"WB ë¸Œë¦¬í”„ ê²½ë¡œ: `{wb_brief_path_used}`")
    else:
        st.sidebar.info("ê°™ì€ í´ë”ì—ì„œ `WB_ClassBriefs.ipynb`ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
elif wb_brief_mode == "íŒŒì¼ ì—…ë¡œë“œ":
    up_wb = st.sidebar.file_uploader("WB_ClassBriefs.ipynb ì—…ë¡œë“œ", type=["ipynb"])
    if up_wb is not None:
        wb_briefs_map = load_wb_briefs_from_ipynb_bytes(up_wb.read())
# ë¹„í™œì„±í™”ë©´ wb_briefs_map == {}


# --------------------- êµ­ê°€ ë§¤í•‘ ---------------------
COUNTRY_MAP = {
    # ğŸŒ ì•„ì‹œì•„
    "ëŒ€í•œë¯¼êµ­": ("KOR","Korea, Republic of","ëŒ€í•œë¯¼êµ­"), "í•œêµ­": ("KOR","Korea, Republic of","ëŒ€í•œë¯¼êµ­"),
    "ë¶í•œ": ("PRK","Korea, Democratic People's Republic of","ë¶í•œ"),
    "ì¼ë³¸": ("JPN","Japan","ì¼ë³¸"), "ì¤‘êµ­": ("CHN","China","ì¤‘êµ­"), "ëª½ê³¨": ("MNG","Mongolia","ëª½ê³¨"),
    "ë² íŠ¸ë‚¨": ("VNM","Vietnam","ë² íŠ¸ë‚¨"), "ë¼ì˜¤ìŠ¤": ("LAO","Laos","ë¼ì˜¤ìŠ¤"), "ìº„ë³´ë””ì•„": ("KHM","Cambodia","ìº„ë³´ë””ì•„"),
    "íƒœêµ­": ("THA","Thailand","íƒœêµ­"), "ë¯¸ì–€ë§ˆ": ("MMR","Myanmar","ë¯¸ì–€ë§ˆ"),
    "ë§ë ˆì´ì‹œì•„": ("MYS","Malaysia","ë§ë ˆì´ì‹œì•„"), "ì‹±ê°€í¬ë¥´": ("SGP","Singapore","ì‹±ê°€í¬ë¥´"),
    "ì¸ë„ë„¤ì‹œì•„": ("IDN","Indonesia","ì¸ë„ë„¤ì‹œì•„"), "í•„ë¦¬í•€": ("PHL","Philippines","í•„ë¦¬í•€"),
    "ë¸Œë£¨ë‚˜ì´": ("BRN","Brunei Darussalam","ë¸Œë£¨ë‚˜ì´"), "ë™í‹°ëª¨ë¥´": ("TLS","Timor-Leste","ë™í‹°ëª¨ë¥´"),
    "ì¸ë„": ("IND","India","ì¸ë„"), "íŒŒí‚¤ìŠ¤íƒ„": ("PAK","Pakistan","íŒŒí‚¤ìŠ¤íƒ„"), "ë„¤íŒ”": ("NPL","Nepal","ë„¤íŒ”"),
    "ë¶€íƒ„": ("BTN","Bhutan","ë¶€íƒ„"), "ìŠ¤ë¦¬ë‘ì¹´": ("LKA","Sri Lanka","ìŠ¤ë¦¬ë‘ì¹´"), "ëª°ë””ë¸Œ": ("MDV","Maldives","ëª°ë””ë¸Œ"),
    "ì¹´ìíìŠ¤íƒ„": ("KAZ","Kazakhstan","ì¹´ìíìŠ¤íƒ„"), "ìš°ì¦ˆë² í‚¤ìŠ¤íƒ„": ("UZB","Uzbekistan","ìš°ì¦ˆë² í‚¤ìŠ¤íƒ„"),
    "í‚¤ë¥´ê¸°ìŠ¤ìŠ¤íƒ„": ("KGZ","Kyrgyzstan","í‚¤ë¥´ê¸°ìŠ¤ìŠ¤íƒ„"), "íƒ€ì§€í‚¤ìŠ¤íƒ„": ("TJK","Tajikistan","íƒ€ì§€í‚¤ìŠ¤íƒ„"),
    "íˆ¬ë¥´í¬ë©”ë‹ˆìŠ¤íƒ„": ("TKM","Turkmenistan","íˆ¬ë¥´í¬ë©”ë‹ˆìŠ¤íƒ„"), "ì•„í”„ê°€ë‹ˆìŠ¤íƒ„": ("AFG","Afghanistan","ì•„í”„ê°€ë‹ˆìŠ¤íƒ„"),
    "ì´ë€": ("IRN","Iran","ì´ë€"), "ì´ë¼í¬": ("IRQ","Iraq","ì´ë¼í¬"), "ì‹œë¦¬ì•„": ("SYR","Syrian Arab Republic","ì‹œë¦¬ì•„"),
    "ë ˆë°”ë…¼": ("LBN","Lebanon","ë ˆë°”ë…¼"), "ì´ìŠ¤ë¼ì—˜": ("ISR","Israel","ì´ìŠ¤ë¼ì—˜"), "íŒ”ë ˆìŠ¤íƒ€ì¸": ("PSE","Palestine","íŒ”ë ˆìŠ¤íƒ€ì¸"),
    "ìš”ë¥´ë‹¨": ("JOR","Jordan","ìš”ë¥´ë‹¨"), "ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„": ("SAU","Saudi Arabia","ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„"),
    "ì˜ˆë©˜": ("YEM","Yemen","ì˜ˆë©˜"), "ì˜¤ë§Œ": ("OMN","Oman","ì˜¤ë§Œ"), "ì•„ëì—ë¯¸ë¦¬íŠ¸": ("ARE","United Arab Emirates","ì•„ëì—ë¯¸ë¦¬íŠ¸"),
    "ì¹´íƒ€ë¥´": ("QAT","Qatar","ì¹´íƒ€ë¥´"), "ë°”ë ˆì¸": ("BHR","Bahrain","ë°”ë ˆì¸"), "ì¿ ì›¨ì´íŠ¸": ("KWT","Kuwait","ì¿ ì›¨ì´íŠ¸"),

    # ğŸŒ ìœ ëŸ½
    "ì˜êµ­": ("GBR","United Kingdom","ì˜êµ­"), "ì•„ì¼ëœë“œ": ("IRL","Ireland","ì•„ì¼ëœë“œ"), "í”„ë‘ìŠ¤": ("FRA","France","í”„ë‘ìŠ¤"),
    "ë…ì¼": ("DEU","Germany","ë…ì¼"), "ì´íƒˆë¦¬ì•„": ("ITA","Italy","ì´íƒˆë¦¬ì•„"), "ìŠ¤í˜ì¸": ("ESP","Spain","ìŠ¤í˜ì¸"),
    "í¬ë¥´íˆ¬ê°ˆ": ("PRT","Portugal","í¬ë¥´íˆ¬ê°ˆ"), "ë„¤ëœë€ë“œ": ("NLD","Netherlands","ë„¤ëœë€ë“œ"),
    "ë²¨ê¸°ì—": ("BEL","Belgium","ë²¨ê¸°ì—"), "ë£©ì…ˆë¶€ë¥´í¬": ("LUX","Luxembourg","ë£©ì…ˆë¶€ë¥´í¬"),
    "ìŠ¤ìœ„ìŠ¤": ("CHE","Switzerland","ìŠ¤ìœ„ìŠ¤"), "ì˜¤ìŠ¤íŠ¸ë¦¬ì•„": ("AUT","Austria","ì˜¤ìŠ¤íŠ¸ë¦¬ì•„"),
    "ë´ë§ˆí¬": ("DNK","Denmark","ë´ë§ˆí¬"), "ë…¸ë¥´ì›¨ì´": ("NOR","Norway","ë…¸ë¥´ì›¨ì´"), "ìŠ¤ì›¨ë´": ("SWE","Sweden","ìŠ¤ì›¨ë´"),
    "í•€ë€ë“œ": ("FIN","Finland","í•€ë€ë“œ"), "ì•„ì´ìŠ¬ë€ë“œ": ("ISL","Iceland","ì•„ì´ìŠ¬ë€ë“œ"),
    "ì²´ì½”": ("CZE","Czechia","ì²´ì½”"), "í´ë€ë“œ": ("POL","Poland","í´ë€ë“œ"), "í—ê°€ë¦¬": ("HUN","Hungary","í—ê°€ë¦¬"),
    "ìŠ¬ë¡œë°”í‚¤ì•„": ("SVK","Slovakia","ìŠ¬ë¡œë°”í‚¤ì•„"), "ìŠ¬ë¡œë² ë‹ˆì•„": ("SVN","Slovenia","ìŠ¬ë¡œë² ë‹ˆì•„"),
    "í¬ë¡œì•„í‹°ì•„": ("HRV","Croatia","í¬ë¡œì•„í‹°ì•„"), "ì„¸ë¥´ë¹„ì•„": ("SRB","Serbia","ì„¸ë¥´ë¹„ì•„"),
    "ëª¬í…Œë„¤ê·¸ë¡œ": ("MNE","Montenegro","ëª¬í…Œë„¤ê·¸ë¡œ"), "ë³´ìŠ¤ë‹ˆì•„í—¤ë¥´ì²´ê³ ë¹„ë‚˜": ("BIH","Bosnia and Herzegovina","ë³´ìŠ¤ë‹ˆì•„í—¤ë¥´ì²´ê³ ë¹„ë‚˜"),
    "ë¶ë§ˆì¼€ë„ë‹ˆì•„": ("MKD","North Macedonia","ë¶ë§ˆì¼€ë„ë‹ˆì•„"), "ì•Œë°”ë‹ˆì•„": ("ALB","Albania","ì•Œë°”ë‹ˆì•„"),
    "ê·¸ë¦¬ìŠ¤": ("GRC","Greece","ê·¸ë¦¬ìŠ¤"), "í„°í‚¤": ("TUR","TÃ¼rkiye","í„°í‚¤"),
    "ë£¨ë§ˆë‹ˆì•„": ("ROU","Romania","ë£¨ë§ˆë‹ˆì•„"), "ë¶ˆê°€ë¦¬ì•„": ("BGR","Bulgaria","ë¶ˆê°€ë¦¬ì•„"),
    "ëª°ë„ë°”": ("MDA","Moldova","ëª°ë„ë°”"), "ìš°í¬ë¼ì´ë‚˜": ("UKR","Ukraine","ìš°í¬ë¼ì´ë‚˜"), "ë²¨ë¼ë£¨ìŠ¤": ("BLR","Belarus","ë²¨ë¼ë£¨ìŠ¤"),
    "ë¦¬íˆ¬ì•„ë‹ˆì•„": ("LTU","Lithuania","ë¦¬íˆ¬ì•„ë‹ˆì•„"), "ë¼íŠ¸ë¹„ì•„": ("LVA","Latvia","ë¼íŠ¸ë¹„ì•„"), "ì—ìŠ¤í† ë‹ˆì•„": ("EST","Estonia","ì—ìŠ¤í† ë‹ˆì•„"),
    "ì¡°ì§€ì•„": ("GEO","Georgia","ì¡°ì§€ì•„"), "ì•„ë¥´ë©”ë‹ˆì•„": ("ARM","Armenia","ì•„ë¥´ë©”ë‹ˆì•„"), "ì•„ì œë¥´ë°”ì´ì”": ("AZE","Azerbaijan","ì•„ì œë¥´ë°”ì´ì”"),
    "ëŸ¬ì‹œì•„": ("RUS","Russian Federation","ëŸ¬ì‹œì•„"),

    # ğŸŒ ì•„í”„ë¦¬ì¹´
    "ì´ì§‘íŠ¸": ("EGY","Egypt","ì´ì§‘íŠ¸"), "ë¦¬ë¹„ì•„": ("LBY","Libya","ë¦¬ë¹„ì•„"), "ì•Œì œë¦¬": ("DZA","Algeria","ì•Œì œë¦¬"),
    "ëª¨ë¡œì½”": ("MAR","Morocco","ëª¨ë¡œì½”"), "íŠ€ë‹ˆì§€": ("TUN","Tunisia","íŠ€ë‹ˆì§€"), "ìˆ˜ë‹¨": ("SDN","Sudan","ìˆ˜ë‹¨"),
    "ë‚¨ìˆ˜ë‹¨": ("SSD","South Sudan","ë‚¨ìˆ˜ë‹¨"), "ì—í‹°ì˜¤í”¼ì•„": ("ETH","Ethiopia","ì—í‹°ì˜¤í”¼ì•„"),
    "ì—ë¦¬íŠ¸ë ˆì•„": ("ERI","Eritrea","ì—ë¦¬íŠ¸ë ˆì•„"), "ì§€ë¶€í‹°": ("DJI","Djibouti","ì§€ë¶€í‹°"),
    "ì†Œë§ë¦¬ì•„": ("SOM","Somalia","ì†Œë§ë¦¬ì•„"), "ì¼€ëƒ": ("KEN","Kenya","ì¼€ëƒ"), "íƒ„ìë‹ˆì•„": ("TZA","Tanzania","íƒ„ìë‹ˆì•„"),
    "ìš°ê°„ë‹¤": ("UGA","Uganda","ìš°ê°„ë‹¤"), "ë¥´ì™„ë‹¤": ("RWA","Rwanda","ë¥´ì™„ë‹¤"), "ë¶€ë£¬ë””": ("BDI","Burundi","ë¶€ë£¬ë””"),
    "ì½©ê³ ë¯¼ì£¼ê³µí™”êµ­": ("COD","Democratic Republic of the Congo","ì½©ê³ ë¯¼ì£¼ê³µí™”êµ­"),
    "ì½©ê³ ê³µí™”êµ­": ("COG","Republic of the Congo","ì½©ê³ ê³µí™”êµ­"),
    "ì•™ê³¨ë¼": ("AGO","Angola","ì•™ê³¨ë¼"), "ì ë¹„ì•„": ("ZMB","Zambia","ì ë¹„ì•„"), "ì§ë°”ë¸Œì›¨": ("ZWE","Zimbabwe","ì§ë°”ë¸Œì›¨"),
    "ë§ë¼ìœ„": ("MWI","Malawi","ë§ë¼ìœ„"), "ëª¨ì ë¹„í¬": ("MOZ","Mozambique","ëª¨ì ë¹„í¬"), "ë§ˆë‹¤ê°€ìŠ¤ì¹´ë¥´": ("MDG","Madagascar","ë§ˆë‹¤ê°€ìŠ¤ì¹´ë¥´"),
    "ë‚¨ì•„í”„ë¦¬ì¹´ê³µí™”êµ­": ("ZAF","South Africa","ë‚¨ì•„í”„ë¦¬ì¹´ê³µí™”êµ­"), "ë³´ì¸ ì™€ë‚˜": ("BWA","Botswana","ë³´ì¸ ì™€ë‚˜"),
    "ë‚˜ë¯¸ë¹„ì•„": ("NAM","Namibia","ë‚˜ë¯¸ë¹„ì•„"), "ë ˆì†Œí† ": ("LSO","Lesotho","ë ˆì†Œí† "), "ì—ìŠ¤ì™€í‹°ë‹ˆ": ("SWZ","Eswatini","ì—ìŠ¤ì™€í‹°ë‹ˆ"),
    "ê°€ë‚˜": ("GHA","Ghana","ê°€ë‚˜"), "ì½”íŠ¸ë””ë¶€ì•„ë¥´": ("CIV","CÃ´te d'Ivoire","ì½”íŠ¸ë””ë¶€ì•„ë¥´"), "ë‚˜ì´ì§€ë¦¬ì•„": ("NGA","Nigeria","ë‚˜ì´ì§€ë¦¬ì•„"),
    "ì„¸ë„¤ê°ˆ": ("SEN","Senegal","ì„¸ë„¤ê°ˆ"), "ë§ë¦¬": ("MLI","Mali","ë§ë¦¬"), "ë‹ˆì œë¥´": ("NER","Niger","ë‹ˆì œë¥´"),
    "ì°¨ë“œ": ("TCD","Chad","ì°¨ë“œ"), "ì¹´ë©”ë£¬": ("CMR","Cameroon","ì¹´ë©”ë£¬"), "ê°€ë´‰": ("GAB","Gabon","ê°€ë´‰"),
    "ì ë„ê¸°ë‹ˆ": ("GNQ","Equatorial Guinea","ì ë„ê¸°ë‹ˆ"),

    # ğŸŒ ì•„ë©”ë¦¬ì¹´
    "ë¯¸êµ­": ("USA","United States of America","ë¯¸êµ­"), "ìºë‚˜ë‹¤": ("CAN","Canada","ìºë‚˜ë‹¤"),
    "ë©•ì‹œì½”": ("MEX","Mexico","ë©•ì‹œì½”"), "ë¸Œë¼ì§ˆ": ("BRA","Brazil","ë¸Œë¼ì§ˆ"), "ì•„ë¥´í—¨í‹°ë‚˜": ("ARG","Argentina","ì•„ë¥´í—¨í‹°ë‚˜"),
    "ì¹ ë ˆ": ("CHL","Chile","ì¹ ë ˆ"), "í˜ë£¨": ("PER","Peru","í˜ë£¨"), "ì½œë¡¬ë¹„ì•„": ("COL","Colombia","ì½œë¡¬ë¹„ì•„"),
    "ì—ì½°ë„ë¥´": ("ECU","Ecuador","ì—ì½°ë„ë¥´"), "ìš°ë£¨ê³¼ì´": ("URY","Uruguay","ìš°ë£¨ê³¼ì´"), "íŒŒë¼ê³¼ì´": ("PRY","Paraguay","íŒŒë¼ê³¼ì´"),
    "ë³¼ë¦¬ë¹„ì•„": ("BOL","Bolivia","ë³¼ë¦¬ë¹„ì•„"), "ë² ë„¤ìˆ˜ì—˜ë¼": ("VEN","Venezuela","ë² ë„¤ìˆ˜ì—˜ë¼"),
    "ì¿ ë°”": ("CUB","Cuba","ì¿ ë°”"), "ë„ë¯¸ë‹ˆì¹´ê³µí™”êµ­": ("DOM","Dominican Republic","ë„ë¯¸ë‹ˆì¹´ê³µí™”êµ­"),
    "ìë©”ì´ì¹´": ("JAM","Jamaica","ìë©”ì´ì¹´"), "ì•„ì´í‹°": ("HTI","Haiti","ì•„ì´í‹°"),
    "ì½”ìŠ¤íƒ€ë¦¬ì¹´": ("CRI","Costa Rica","ì½”ìŠ¤íƒ€ë¦¬ì¹´"), "íŒŒë‚˜ë§ˆ": ("PAN","Panama","íŒŒë‚˜ë§ˆ"),
    "ì˜¨ë‘ë¼ìŠ¤": ("HND","Honduras","ì˜¨ë‘ë¼ìŠ¤"), "ì—˜ì‚´ë°”ë„ë¥´": ("SLV","El Salvador","ì—˜ì‚´ë°”ë„ë¥´"),
    "ë‹ˆì¹´ë¼ê³¼": ("NIC","Nicaragua","ë‹ˆì¹´ë¼ê³¼"), "ê³¼í…Œë§ë¼": ("GTM","Guatemala","ê³¼í…Œë§ë¼"),

    # ğŸŒŠ ì˜¤ì„¸ì•„ë‹ˆì•„
    "í˜¸ì£¼": ("AUS","Australia","í˜¸ì£¼"), "ë‰´ì§ˆëœë“œ": ("NZL","New Zealand","ë‰´ì§ˆëœë“œ"),
    "íŒŒí‘¸ì•„ë‰´ê¸°ë‹ˆ": ("PNG","Papua New Guinea","íŒŒí‘¸ì•„ë‰´ê¸°ë‹ˆ"), "í”¼ì§€": ("FJI","Fiji","í”¼ì§€"),
    "ì‚¬ëª¨ì•„": ("WSM","Samoa","ì‚¬ëª¨ì•„"), "í†µê°€": ("TON","Tonga","í†µê°€"), "ë°”ëˆ„ì•„íˆ¬": ("VUT","Vanuatu","ë°”ëˆ„ì•„íˆ¬"),
}

REGION_RULES = {
    "ë©”ì½©ê°•ìœ„ì›íšŒ": [
        ("KHM","Cambodia","ìº„ë³´ë””ì•„"),
        ("LAO","Laos","ë¼ì˜¤ìŠ¤"),
        ("THA","Thailand","íƒœêµ­"),
        ("VNM","Vietnam","ë² íŠ¸ë‚¨"),
    ],
    "í˜¸ì£¼Â·í•œêµ­": [
        ("AUS","Australia","í˜¸ì£¼"),
        ("KOR","Korea, Republic of","ëŒ€í•œë¯¼êµ­"),
    ],
    "ì¤‘ë‚¨ë¯¸ ì§€ì—­": [
        ("ARG","Argentina","ì•„ë¥´í—¨í‹°ë‚˜"),("BRA","Brazil","ë¸Œë¼ì§ˆ"),("CHL","Chile","ì¹ ë ˆ"),
        ("URY","Uruguay","ìš°ë£¨ê³¼ì´"),("PRY","Paraguay","íŒŒë¼ê³¼ì´"),("BOL","Bolivia","ë³¼ë¦¬ë¹„ì•„"),
        ("PER","Peru","í˜ë£¨"),("ECU","Ecuador","ì—ì½°ë„ë¥´"),("COL","Colombia","ì½œë¡¬ë¹„ì•„"),
        ("VEN","Venezuela","ë² ë„¤ìˆ˜ì—˜ë¼"),("GUY","Guyana","ê°€ì´ì•„ë‚˜"),("SUR","Suriname","ìˆ˜ë¦¬ë‚¨"),
        ("MEX","Mexico","ë©•ì‹œì½”"),("GTM","Guatemala","ê³¼í…Œë§ë¼"),("BLZ","Belize","ë²¨ë¦¬ì¦ˆ"),
        ("HND","Honduras","ì˜¨ë‘ë¼ìŠ¤"),("SLV","El Salvador","ì—˜ì‚´ë°”ë„ë¥´"),("NIC","Nicaragua","ë‹ˆì¹´ë¼ê³¼"),
        ("CRI","Costa Rica","ì½”ìŠ¤íƒ€ë¦¬ì¹´"),("PAN","Panama","íŒŒë‚˜ë§ˆ"),
        ("CUB","Cuba","ì¿ ë°”"),("DOM","Dominican Republic","ë„ë¯¸ë‹ˆì¹´ê³µí™”êµ­"),("HTI","Haiti","ì•„ì´í‹°"),
        ("JAM","Jamaica","ìë©”ì´ì¹´"),("BRB","Barbados","ë°”ë² ì´ë„ìŠ¤"),("BHS","Bahamas","ë°”í•˜ë§ˆ"),
        ("TTO","Trinidad and Tobago","íŠ¸ë¦¬ë‹ˆë‹¤ë“œí† ë°”ê³ "),("LCA","Saint Lucia","ì„¸ì¸íŠ¸ë£¨ì‹œì•„"),
        ("VCT","Saint Vincent and the Grenadines","ì„¸ì¸íŠ¸ë¹ˆì„¼íŠ¸ê·¸ë ˆë‚˜ë”˜"),
        ("KNA","Saint Kitts and Nevis","ì„¸ì¸íŠ¸í‚¤ì¸ ë„¤ë¹„ìŠ¤"),
        ("GRD","Grenada","ê·¸ë ˆë‚˜ë‹¤"),("DMA","Dominica","ë„ë¯¸ë‹ˆì¹´ì—°ë°©"),
        ("ATG","Antigua and Barbuda","ì•¤í‹°ê°€ë°”ë¶€ë‹¤"),("PRI","Puerto Rico","í‘¸ì—ë¥´í† ë¦¬ì½”"),
        ("VIR","Virgin Islands (U.S.)","ë¯¸êµ­ë ¹ ë²„ì§„ ì•„ì¼ëœë“œ"),
        ("CYM","Cayman Islands","ì¼€ì´ë§¨ ì œë„"),("TCA","Turks and Caicos Islands","í„°í¬ìŠ¤ ì¼€ì´ì»¤ìŠ¤ ì œë„"),
        ("ABW","Aruba","ì•„ë£¨ë°”"),("CUW","CuraÃ§ao","í€´ë¼ì†Œ"),("SXM","Sint Maarten","ì‹ íŠ¸ë§ˆë¥´í„´"),
        ("MAF","Saint Martin (French part)","ìƒë§ˆë¥´íƒ±"),
    ],
}

def split_countries(x: str):
    if pd.isna(x): return []
    return [tok for tok in re.split(r"[Â·/,;|&]+|\s*,\s*|\s*&\s*", str(x).strip()) if tok]

def map_country_token(token: str):
    tkn = token.strip()
    if tkn in COUNTRY_MAP: return [COUNTRY_MAP[tkn]]
    if tkn in REGION_RULES: return REGION_RULES[tkn]
    out = []
    for s in re.split(r"[Â·/,;|&]+", tkn):
        s = s.strip()
        if s in COUNTRY_MAP: out.append(COUNTRY_MAP[s])
    return out

def expand_by_country(df_in: pd.DataFrame) -> pd.DataFrame:
    rows = []
    for _, row in df_in.iterrows():
        tokens = split_countries(row.get("ëŒ€ìƒêµ­",""))
        mapped = []
        for tk in tokens: mapped.extend(map_country_token(tk))
        if not mapped: continue
        for iso3, en, ko in mapped:
            d = row.to_dict()
            d.update({"iso3": iso3, "country_en": en, "country_ko": ko})
            rows.append(d)
    return pd.DataFrame(rows)

dfx = expand_by_country(df)

# --------------------- ì„¸ê³„ ê²½ê³„ + key_on ìë™ ---------------------
@st.cache_data(show_spinner=False)
def get_world_geojson_auto() -> Dict:
    url = "https://raw.githubusercontent.com/python-visualization/folium/master/examples/data/world-countries.json"
    cache_dir = pathlib.Path(".ksp_cache"); cache_dir.mkdir(exist_ok=True)
    local = cache_dir / ("world-countries." + hashlib.md5(url.encode()).hexdigest() + ".json")
    if not local.exists():
        with urllib.request.urlopen(url, timeout=20) as resp: local.write_bytes(resp.read())
    return json.loads(local.read_text(encoding="utf-8"))

world_geojson = get_world_geojson_auto()

def resolve_geojson_key_on(gj: dict):
    feat0 = gj["features"][0]; props = feat0.get("properties", {})
    for c in ["iso_a3","ISO_A3","adm0_a3","ADM0_A3","wb_a3","WB_A3","ISO3","id"]:
        if c in props: return f"feature.properties.{c}", c, True
    if "id" in feat0: return "feature.id", "id", False
    raise ValueError("GeoJSONì—ì„œ ISO3 í‚¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

key_on_info = resolve_geojson_key_on(world_geojson)

def augment_geojson_values(gj: dict, key_on_info, value_map: dict, value_prop: str):
    key_on_str, iso_key, in_props = key_on_info
    new_gj = copy.deepcopy(gj)
    for feat in new_gj["features"]:
        props = feat.setdefault("properties", {})
        iso = props.get(iso_key) if in_props else feat.get("id")
        props["ISO3"] = iso
        props[value_prop] = value_map.get(str(iso), 0)
    return new_gj

# --------------------- ì§€ë„/í´ë¦­ ìœ í‹¸ ---------------------
def make_base_map(center=[15,10], zoom=3):
    m = folium.Map(
        location=center, zoom_start=zoom, tiles=None,
        control_scale=True, prefer_canvas=True,
        world_copy_jump=False, max_bounds=True, max_bounds_viscosity=1.0,
        min_zoom=2
    )
    folium.TileLayer(tiles="CartoDB Positron", name="Base", control=False, no_wrap=True).add_to(m)
    return m

def extract_iso_from_stfolium(ret: dict):
    if not ret: return None
    iso_keys = ["ISO3","id","iso_a3","ISO_A3","adm0_a3","ADM0_A3","wb_a3","WB_A3"]
    obj = ret.get("last_object_clicked")
    if isinstance(obj, dict):
        props = obj.get("properties") or {}
        for k in iso_keys:
            if props.get(k): return props.get(k)
    lad = ret.get("last_active_drawing") or ret.get("last_active_drawing_geojson")
    if isinstance(lad, dict):
        props = lad.get("properties") or lad.get("feature", {}).get("properties") or {}
        for k in iso_keys:
            if props.get(k): return props.get(k)
    s = ret.get("last_object_clicked_popup")
    if isinstance(s, str):
        m = re.search(r"([A-Z]{3})", s)
        if m: return m.group(1)
    return None

# --------------------- ì—°ë„ íŒŒì„œ ---------------------
# === KEEP ONLY THIS ===
YEAR_RE = re.compile(r"(?:19|20)\d{2}")

def years_from_span(text):
    """
    '2025-2026' â†’ [2025,2026], '2025' â†’ [2025]
    ìˆ«ì(ì •ìˆ˜/ì‹¤ìˆ˜)ë„ í—ˆìš©. ë²”ìœ„ê°€ ë’¤ì§‘í˜€ë„ ì •ìƒí™”.
    """
    if pd.isna(text):
        return []

    if isinstance(text, (int, np.integer, float, np.floating)):
        y = int(text)
        return [y] if 1990 <= y <= 2035 else []

    t = str(text)
    t = t.replace("~", "-").replace("â€“", "-").replace("â€”", "-")
    t = re.sub(r"[()]", " ", t)

    years = [int(y) for y in YEAR_RE.findall(t)]
    years = [y for y in years if 1990 <= y <= 2035]

    # ë²”ìœ„ í™•ì¥
    for a, b in re.findall(r"((?:19|20)\d{2})\s*-\s*((?:19|20)\d{2})", t):
        a, b = int(a), int(b)
        lo, hi = min(a, b), max(a, b)
        years.extend(range(lo, hi + 1))

    years = sorted(set(years))
    return years


# === ì—°ë„ í…ìŠ¤íŠ¸ ì‹œë¦¬ì¦ˆ ì„ íƒ ===
def _year_text_series(df_in: pd.DataFrame) -> pd.Series:
    """ì—°ë„ ì›ì²œ: ì§€ì • ì»¬ëŸ¼ > ê´€ìš© ì»¬ëŸ¼ë“¤ > ìš”ì•½/ë³¸ë¬¸ ë“± í…ìŠ¤íŠ¸ ê²°í•© â†’ ë¬¸ìì—´ ì‹œë¦¬ì¦ˆ ë°˜í™˜"""
    ys_col = globals().get("YEAR_SOURCE", None)
    if ys_col and ys_col in df_in.columns:
        return df_in[ys_col].astype(str)

    for c in ["ì‚¬ì—… ê¸°ê°„","ì—°ë„","ê¸°ê°„","Project Period","Years","Year","year"]:
        if c in df_in.columns:
            return df_in[c].astype(str)

    pool = [c for c in ["ìš”ì•½","ì£¼ìš” ë‚´ìš©","íŒŒì¼ëª…"] if c in df_in.columns]
    if pool:
        return df_in[pool].fillna("").astype(str).agg(" ".join, axis=1)

    # ìµœí›„: ë¹ˆ ë¬¸ìì—´ ì‹œë¦¬ì¦ˆ (ê¸¸ì´ ë§ì¶°ì„œ ë°˜í™˜)
    return pd.Series([""] * len(df_in), index=df_in.index, dtype=str)


@st.cache_data(show_spinner=False)
def expand_years(df_in: pd.DataFrame) -> pd.DataFrame:
    """
    - YEAR_SOURCE ì§€ì •/ìë™ íƒìƒ‰(_year_text_series)ë¡œ ì—°ë„ í…ìŠ¤íŠ¸ë¥¼ í™•ë³´
    - years_from_spanìœ¼ë¡œ ì—°ë„ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ í›„ explode
    - 'ì—°ë„'ê°€ DataFrame(ì¤‘ë³µëª…)ìœ¼ë¡œ ìƒê²¨ë„ ì•ˆì „í•˜ê²Œ 1-Dë¡œ ê°•ì œ
    """
    if df_in is None or df_in.empty:
        return pd.DataFrame({"ì—°ë„": pd.Series([], dtype="Int64")})

    # â‘  ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
    df1 = df_in.loc[:, ~df_in.columns.duplicated()].copy()

    # â‘¡ ì—°ë„ ì›ì²œ ì‹œë¦¬ì¦ˆ í™•ë³´ (ì§€ì • ì»¬ëŸ¼ > ê´€ìš© ì»¬ëŸ¼ë“¤ > í…ìŠ¤íŠ¸ ê²°í•©)
    ser = _year_text_series(df1)  # â† ì•ì„œ ì¶”ê°€í•œ í—¬í¼

    # â‘¢ ì—°ë„ íŒŒì‹±
    years_list = ser.apply(years_from_span)
    if not years_list.apply(lambda x: bool(x)).any():
        return pd.DataFrame({"ì—°ë„": pd.Series([], dtype="Int64")})

    # â‘£ explode
    dfy = df1.assign(__years=years_list).explode("__years").rename(columns={"__years": "ì—°ë„"})

    # â‘¤ 'ì—°ë„'ë¥¼ ë°˜ë“œì‹œ 1-D Seriesë¡œ ê°•ì œ
    y = dfy["ì—°ë„"]
    if isinstance(y, pd.DataFrame):  # í˜¹ì‹œë¼ë„ ë˜ ì¤‘ë³µë˜ë©´ ì²« ì—´ ì‚¬ìš©
        y = y.iloc[:, 0]
    y = pd.to_numeric(y, errors="coerce").astype("Int64")

    # ë™ì¼ ì´ë¦„ ì»¬ëŸ¼ë“¤ ì •ë¦¬ í›„ ì‚½ì…
    dup_cols = [c for c in dfy.columns if c == "ì—°ë„"]
    dfy = dfy.drop(columns=dup_cols)
    dfy.insert(0, "ì—°ë„", y.values)

    return dfy





dfy = expand_years(df)     # í‚¤ì›Œë“œ/ì£¼ì œ ìƒëŒ€ íŠ¸ë Œë“œëŠ” 'êµ­ê°€ ì¤‘ë³µ ì—†ëŠ”' ì›ë³¸ df ê¸°ì¤€

# --------------------- ë³´ê¸° ëª¨ë“œ ---------------------
st.sidebar.header("ë³´ê¸° ëª¨ë“œ")
mode = st.sidebar.radio("ì§€ë„ ìœ í˜•", ["êµ­ê°€ë³„ ì´ê³„", "ICT ìœ í˜• ë‹¨ì¼í´ë˜ìŠ¤"], index=0)

# ì—°ë„ ì‹œê°í™” ì˜µì…˜ (íˆíŠ¸ë§µ ì œê±°)
st.sidebar.header("ì—°ë„ ì‹œê°í™” ë°©ì‹")
YEAR_OPTIONS = ["Line Bump", "ìˆœìœ„ Bump"]
year_mode = st.sidebar.selectbox("í‘œí˜„ ë°©ì‹", YEAR_OPTIONS, index=0, key="year_mode")


clicked_iso = None

# ===================== â‘  êµ­ê°€ë³„ ì´ê³„ (í´ë¦­) =====================
if mode == "êµ­ê°€ë³„ ì´ê³„":
    st.subheader("êµ­ê°€ë³„ ì´ í”„ë¡œì íŠ¸ ìˆ˜")
    agg_country = dfx.groupby(["iso3","country_ko"], as_index=False).agg(n_docs=("íŒŒì¼ëª…","nunique"))
    base = make_base_map()
    value_map = {r.iso3: int(r.n_docs) for _, r in agg_country.iterrows()}
    gj = augment_geojson_values(world_geojson, key_on_info, value_map, "ksp_docs")

    ch = folium.Choropleth(
        geo_data=gj, data=agg_country, columns=["iso3","n_docs"],
        key_on=key_on_info[0],
        fill_color="YlGnBu", fill_opacity=0.88,
        line_opacity=0.55, line_color="#7f7f7f",
        legend_name="ë³´ê³ ì„œ ìˆ˜", nan_fill_color="#f0f0f0",
        highlight=True
    ); ch.add_to(base)
    ch.geojson.add_child(folium.features.GeoJsonTooltip(
        fields=["ISO3", "name" if "name" in gj["features"][0]["properties"] else "ISO3", "ksp_docs"],
        aliases=["ISO3", "êµ­ê°€", "ë³´ê³ ì„œ ìˆ˜"], sticky=False
    ))
    ch.geojson.add_child(folium.features.GeoJsonPopup(fields=["ISO3"], aliases=["ISO3"]))
    ret = st_folium(base, height=560, use_container_width=True)
    clicked_iso = extract_iso_from_stfolium(ret)



# --------------------- ìƒì„¸ íŒ¨ë„ ---------------------
def find_korean_font() -> str | None:
    candidates = [
        r"C:\\Windows\\Fonts\\malgun.ttf", r"C:\\Windows\\Fonts\\Malgun.ttf",
        r"C:\\Windows\\Fonts\\NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/System/Library/Fonts/AppleGothic.ttf",
    ]
    for p in candidates:
        if os.path.exists(p): return p
    return None

# ======== Color utils ========
def _hex_to_rgb(hexstr: str) -> tuple[int,int,int]:
    s = hexstr.strip().lstrip("#")
    if len(s) == 3: s = "".join(c*2 for c in s)
    return int(s[0:2],16), int(s[2:4],16), int(s[4:6],16)

def rgba_str(color: str, alpha: float=0.5) -> str:
    if not color: return "rgba(0,0,0,0)"
    if color.startswith("rgb"):  # already rgba/rgb string
        return color if "rgba" in color else color.replace("rgb(", "rgba(").replace(")", f", {alpha})")
    r,g,b = _hex_to_rgb(color); return f"rgba({r},{g},{b},{alpha})"


# ê³µí†µ Plotly ìŠ¤íƒ€ì¼ (í°íŠ¸ í¬ê²Œ)
# ê³µí†µ Plotly ìŠ¤íƒ€ì¼ (í°íŠ¸ í¬ê²Œ) â€” ë°°ê²½ íˆ¬ëª… + ë²”ë¡€ ì¤„ë°”ê¿ˆ ì‹œ ìƒë‹¨ì—¬ë°± ìë™ ë³´ì •
# ---- PATCH B: Upgrade style_fig (consistent, professional charts) ----
# ì•ˆì „í•œ style_fig: titleì´ Noneì´ë©´ ê¸°ì¡´ ì œëª©ì„ ë³´ì¡´
def style_fig(fig, title=None, height=None, legend="top", top_margin=96,
              auto_legend_space=True, bg_color=None, bg_alpha=0.5):
    # legend presets
    if legend == "top":
        legend_cfg = dict(orientation="h", y=1.12, yanchor="bottom", x=0, xanchor="left", bgcolor="rgba(0,0,0,0)")
        m = dict(l=16, r=24, b=72, t=top_margin)
    elif legend == "bottom":
        legend_cfg = dict(orientation="h", y=-0.22, yanchor="top", x=0, xanchor="left", bgcolor="rgba(0,0,0,0)")
        m = dict(l=16, r=24, b=120, t=72)
    elif legend == "right":
        legend_cfg = dict(orientation="v", y=0.5, yanchor="middle", x=1.02, xanchor="left", bgcolor="rgba(0,0,0,0)")
        m = dict(l=16, r=160, b=72, t=top_margin)
    else:
        legend_cfg, m = dict(), dict(l=16, r=24, b=72, t=top_margin)

    # ë°°ê²½ rgba
    def _hex_to_rgb(s):
        s = s.strip().lstrip("#")
        if len(s) == 3: s = "".join(c*2 for c in s)
        return int(s[0:2],16), int(s[2:4],16), int(s[4:6],16)
    def rgba_str(color, a=0.5):
        if not color: return "rgba(0,0,0,0)"
        if color.startswith("rgb"):  # rgb/rgba ë¬¸ìì—´
            return color if "rgba" in color else color.replace("rgb(", "rgba(").replace(")", f", {a})")
        r,g,b = _hex_to_rgb(color); return f"rgba({r},{g},{b},{a})"
    bg_rgba = rgba_str(bg_color, bg_alpha) if bg_color else "rgba(0,0,0,0)"

    # layout kwargsë¥¼ ë™ì ìœ¼ë¡œ êµ¬ì„± (titleì€ Noneì´ë©´ ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
    layout_kwargs = dict(
        template=ui["plotly_template"],
        paper_bgcolor=bg_rgba,
        plot_bgcolor=bg_rgba,
        font=dict(family=_plotly_font_family(), color=ui["text"], size=16),
        height=height,
        margin=m,
        legend=legend_cfg if legend != "none" else None,
        hovermode="x unified",
        hoverlabel=dict(font=dict(size=13, family=_plotly_font_family()),    # â˜… ì—¬ê¸°
                        bgcolor="rgba(255,255,255,0.92)", bordercolor="rgba(0,0,0,0.1)"),
        modebar=dict(bgcolor="rgba(0,0,0,0)", color="#808B98", activecolor=ui["accent"]),
    )

    if title is not None:  # â† ì œëª©ì„ ìƒˆë¡œ ì¤„ ë•Œë§Œ ì„¸íŒ…
        layout_kwargs["title"] = dict(
            text=title, font=dict(size=22, family="Inter, Noto Sans KR"),
            x=0.0, xanchor="left", y=0.98, yanchor="top"
        )

    fig.update_layout(**layout_kwargs)

    # ì¶• ìŠ¤íƒ€ì¼
    fig.update_xaxes(title_font=dict(size=16), tickfont=dict(size=13),
                     showline=True, linewidth=1, linecolor="rgba(0,0,0,0.25)",
                     gridcolor="rgba(127,127,127,0.16)", zeroline=False)
    fig.update_yaxes(title_font=dict(size=16), tickfont=dict(size=13),
                     showline=True, linewidth=1, linecolor="rgba(0,0,0,0.25)",
                     gridcolor="rgba(127,127,127,0.16)", zeroline=True, zerolinewidth=1,
                     zerolinecolor="rgba(127,127,127,0.20)")

    # ë²”ë¡€ ì¤„ë°”ê¿ˆ ì—¬ìœ 
    if auto_legend_space and getattr(fig.layout, "legend", None) and getattr(fig.layout.legend, "orientation", "") == "h":
        import numpy as _np
        n_items = sum(1 for tr in fig.data if getattr(tr, "showlegend", True) and getattr(tr, "name", None))
        rows_est = int(_np.ceil((n_items or 1)/8))
        if rows_est > 1:
            extra = 28 * (rows_est - 1)
            fig.update_layout(margin=dict(l=fig.layout.margin.l, r=fig.layout.margin.r,
                                          b=fig.layout.margin.b, t=max(fig.layout.margin.t or 0, top_margin + extra)))
    return fig



VIZ_BG = {
    "map_total":     "#E8F0FE",   # êµ­ê°€ë³„ ì´ê³„ ì§€ë„ ì¹´ë“œ
    "map_wb":        "#F1ECE3",   # ICT ìœ í˜• ë‹¨ì¼ ì§€ë„ ì¹´ë“œ
    "donut_subj":    "#F6F7FB",   # ì£¼ì œ ë„ë„›
    "donut_wb":      "#E8F6EE",   # WB ë„ë„›
    "stack_100":     "#FFF7ED",   # 100% ëˆ„ì  ë§‰ëŒ€(ì£¼ì œÃ—WB)
    "year_subj":     "#FDF2F8",   # ì—°ë„ë³„ ì£¼ì œ ì‹œê°í™”
    "year_wb":       "#EEF2FF",   # ì—°ë„ë³„ WB ì‹œê°í™”
    "trend_up":      "#E2F2FF",   # í‚¤ì›Œë“œ ìƒìŠ¹ì„¸
    "trend_down":    "#FFE4E6",   # í‚¤ì›Œë“œ í•˜ë½ì„¸
    "theme_up":      "#E5F9F0",   # í…Œë§ˆ ìƒìŠ¹ì„¸
    "theme_down":    "#FFF1F2",   # í…Œë§ˆ í•˜ë½ì„¸
    "wc":            "#EEF1F6",   # ì›Œë“œí´ë¼ìš°ë“œ
    "bar_topk":      "#FAF7F2",   # Top-20 ê°€ë¡œë§‰ëŒ€
}



def render_wordcloud_png(freqs: dict, bg_color: str, alpha: float=0.5,
                         width: int=820, height: int=460, scale: int=2) -> bytes | None:
    """ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ë¥¼ PNG ë°”ì´íŠ¸ë¡œ ë°˜í™˜ (st.image ì•ˆì „ í‘œì‹œìš©)."""
    if not freqs or not WC_FONT_PATH:
        return None

    wc = WordCloud(
        width=width, height=height, scale=scale,
        mode="RGBA", background_color=None,
        max_words=220, prefer_horizontal=0.95,
        max_font_size=108, min_font_size=10,
        font_path=WC_FONT_PATH, random_state=42
    ).generate_from_frequencies(freqs)

    wc_img = wc.to_image().convert("RGBA")  # PIL.Image
    r, g, b = _hex_to_rgb(bg_color)
    base    = Image.new("RGBA", wc_img.size, (r, g, b, int(255*alpha)))
    mixed   = Image.alpha_composite(base, wc_img)

    buf = io.BytesIO()
    mixed.save(buf, format="PNG")  # â˜… í¬ë§· í™•ì •
    return buf.getvalue()


def auto_expand_top_margin_for_wrapped_legend(fig, base_top=100, items_per_row=8, extra_per_row=28):
    """legendë¥¼ ì´í›„ì— top/horizontalë¡œ ë³€ê²½í•œ ê²½ìš° ìƒë‹¨ì—¬ë°±ì„ ìë™ ì¦ë¶„."""
    import math
    leg = getattr(fig.layout, "legend", None)
    if not leg or getattr(leg, "orientation", "") != "h":
        return fig
    n_items = sum(1 for tr in fig.data if getattr(tr, "showlegend", True) and getattr(tr, "name", None))
    rows_est = math.ceil(n_items / max(1, items_per_row)) if n_items else 1
    if rows_est > 1:
        extra = extra_per_row * (rows_est - 1)
        cur = getattr(fig.layout.margin, "t", base_top) or base_top
        fig.update_layout(margin=dict(l=fig.layout.margin.l, r=fig.layout.margin.r,
                                      b=fig.layout.margin.b, t=max(cur, base_top + extra)))
    return fig

def force_legend_top_padding(fig, base_top=120,
                             items_per_row_hard=6,   # í•œ ì¤„ì— 6ê°œë§Œ ìˆ˜ìš©í•œë‹¤ê³  ê°€ì •(ë³´ìˆ˜ì )
                             char_unit=10.0,         # ë¼ë²¨ ê¸¸ì´ ë³´ì •(10ì â‰ˆ 1 ìœ ë‹›)
                             extra_per_row=40,       # ì¤„ ëŠ˜ ë•Œë§ˆë‹¤ ì¶”ê°€í•  ì—¬ë°±(px)
                             y_step=0.05):           # ì¤„ ëŠ˜ ë•Œë§ˆë‹¤ legend yë¥¼ ì–¼ë§ˆë‚˜ ë” ì˜¬ë¦´ì§€
    """
    - ì‹¤ì œ í™”ë©´í­ì„ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ 'í•­ëª© ìˆ˜ + ë¼ë²¨ ê¸¸ì´'ë¡œ ì¤„ ìˆ˜ë¥¼ ê³¼ëŒ€ì¶”ì •í•´ì„œ ì•ˆì „ ì—¬ë°±ì„ í™•ë³´.
    - items_per_row_hard=6 ìœ¼ë¡œ ë‚®ì¶° ë‘ ì¤„ íŒë‹¨ì„ ì‰½ê²Œ ë§Œë“¦(â‡’ í•­ìƒ ë„‰ë„‰í•œ top margin).
    """
    import math

    # ë²”ë¡€ í•­ëª© ìˆ˜ì§‘
    names = [getattr(tr, "name", None) for tr in fig.data if getattr(tr, "showlegend", True)]
    names = [n for n in names if n]
    if not names:
        # ê·¸ë˜ë„ ìµœì†Œ base_topì€ ë³´ì¥
        cur_t = getattr(fig.layout.margin, "t", 0) or 0
        if cur_t < base_top:
            fig.update_layout(margin=dict(l=fig.layout.margin.l, r=fig.layout.margin.r,
                                          b=fig.layout.margin.b, t=base_top))
        return fig

    # ìœ ë‹› ê³„ì‚°(í•­ëª© 1 + ë¼ë²¨ ê¸¸ì´ì— ë¹„ë¡€í•œ ë³´ì •)
    total_units = sum(1.0 + (len(str(n)) / char_unit) for n in names)

    # ë³´ìˆ˜ì  ì¤„ìˆ˜ ì¶”ì •
    rows_est = int(math.ceil(total_units / max(1e-6, items_per_row_hard)))
    rows_est = max(rows_est, 1)

    # ì›í•˜ëŠ” top margin & legend y
    want_top = base_top + (rows_est - 1) * extra_per_row
    y_target = 1.10 + (rows_est - 1) * y_step

    cur_t = getattr(fig.layout.margin, "t", 0) or 0
    fig.update_layout(
        legend=dict(orientation="h", y=y_target, yanchor="bottom", x=0, xanchor="left"),
        margin=dict(l=fig.layout.margin.l, r=fig.layout.margin.r,
                    b=fig.layout.margin.b, t=max(cur_t, want_top))
    )
    return fig

# ---- PATCH D: Tabbed detail panel ----
if mode == "êµ­ê°€ë³„ ì´ê³„":
    st.subheader("ìƒì„¸ íŒ¨ë„")
    if clicked_iso:
        sub = dfx[dfx["iso3"]==clicked_iso].copy()
        if not sub.empty:
            country_name = sub["country_ko"].iloc[0]
            st.markdown(f"### {country_name} â€” í”„ë¡œì íŠ¸ {sub['íŒŒì¼ëª…'].nunique()}ê±´")

            tab_overview, tab_cloud, tab_table = st.tabs(["ê°œìš”", "ì›Œë“œí´ë¼ìš°ë“œ / í‚¤ì›Œë“œ", "í…Œì´ë¸”"])

            with tab_overview:
                st.markdown("#### êµ­ê°€ ë¸Œë¦¬í”„")
                if isinstance(briefs_map, dict) and briefs_map:
                    iso = clicked_iso
                    brief_txt = briefs_map.get(iso) or briefs_map.get(sub["country_en"].iloc[0], None) or briefs_map.get(country_name, None)
                    st.write(brief_txt if brief_txt else "ë¸Œë¦¬í”„ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.info("ì¢Œì¸¡ì—ì„œ CountryBriefs.ipynbë¥¼ ì§€ì •í•˜ì„¸ìš”.")
                st.divider()

                st.markdown("#### í•µì‹¬ ì§€í‘œ")
                _suby = expand_years(sub)  # ê¸°ì¡´ì— ì •ì˜ëœ í•¨ìˆ˜
                sub_years = sorted(set(_suby["ì—°ë„"].dropna().astype(int).tolist()))
                cA, cB, cC = st.columns(3)
                with cA: st.metric("ì—°ë„ ë²”ìœ„", f"{min(sub_years) if sub_years else '-'}â€“{max(sub_years) if sub_years else '-'}")
                with cB: st.metric("ICT ìœ í˜• ê³ ìœ ", f"{sub['ICT ìœ í˜•'].astype(str).str.strip().nunique():,}")
                with cC: st.metric("ëŒ€ìƒê¸°ê´€ ìˆ˜", f"{sub['ëŒ€ìƒê¸°ê´€'].nunique():,}")

            with tab_cloud:
                st.markdown("#### ì›Œë“œí´ë¼ìš°ë“œ (í•´ì‹œíƒœê·¸ + ìš”ì•½/ë‚´ìš©)")
                # 0) í† í° ìˆ˜ì§‘ (í•´ì‹œíƒœê·¸ + ìš”ì•½/ë‚´ìš©)
                tokens: list[str] = []
                if "Hashtag_str" in sub.columns and sub["Hashtag_str"].notna().any():
                    for txt in sub["Hashtag_str"].dropna().astype(str):
                        tokens += [z.strip() for z in re.split(r"[;,]", txt) if z.strip()]
                elif "Hashtag" in sub.columns and sub["Hashtag"].notna().any():
                    for txt in sub["Hashtag"].dropna().astype(str):
                        tokens += [z.strip() for z in re.split(r"[;,]", txt) if z.strip()]

                pool_cols = [c for c in ["ìš”ì•½", "ì£¼ìš” ë‚´ìš©"] if c in sub.columns]
                if pool_cols:
                    for txt in sub[pool_cols].fillna("").astype(str).agg(" ".join, axis=1).tolist():
                        for w in re.split(r"[^0-9A-Za-zê°€-í£]+", txt):
                            w = w.strip()
                            if len(w) >= 2:
                                tokens.append(w)

                # 1) ì •ì œ
                tokens = [
                    w for w in tokens
                    if w and w.lower() not in STOP_LOW and not re.fullmatch(r"\d+(\.\d+)?", w)
                ]
                freq = Counter(tokens)
                top_freqs = dict(freq.most_common(220))
                top20 = freq.most_common(10)

                # 2) 2ì—´ ë°°ì¹˜ (ì›Œë“œí´ë¼ìš°ë“œ : ë§‰ëŒ€ = 6 : 7)
                lc, rc = st.columns([6, 7], gap="large")

                # Left) ì›Œë“œí´ë¼ìš°ë“œ â€” â€œì ë‹¹íˆ í¼ + ì„ ëª…â€, ì»¬ëŸ¼ í­ì— ë§ì¶° ìë™ ë§ì¶¤
                with lc:
                    st.markdown("**ì›Œë“œí´ë¼ìš°ë“œ**")
                    if top_freqs:
                        font_path = find_korean_font()
                        # ë°ì€/ì–´ë‘ìš´ í…Œë§ˆ ìë™ ë°°ê²½
                        bg = "white" if ui.get("plotly_template", "plotly_white") == "plotly_white" else ui.get("card", "#0f1115")
                        # (ì™¼ìª½) ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±

                        

                        png_bytes = render_wordcloud_png(top_freqs, bg_color=VIZ_BG["wc"], alpha=0.5)
                        if png_bytes:
                            st.image(png_bytes, use_container_width=True, output_format="PNG")  # â˜… ì•ˆì „
                        else:
                            if not WC_FONT_PATH:
                                st.error("ì›Œë“œí´ë¼ìš°ë“œìš© í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë¦¬í¬ì— assets/fonts/NanumGothic.ttf ì¶”ê°€ ë˜ëŠ” packages.txtì— fonts-nanum)")
                            else:
                                st.info("í‘œì‹œí•  ë‹¨ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

                        

                        # ìƒìœ„ í‚¤ì›Œë“œ ì¹©(ë¹ ë¥¸ ìŠ¤ìº”ìš©, 12ê°œ)
                        chips = " ".join([f'<span class="ksp-chip">{k}</span>' for k, _ in freq.most_common(12)])
                        st.markdown(chips, unsafe_allow_html=True)
                    else:
                        st.info("í‘œì‹œí•  ë‹¨ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

                # Right) Top-20 ê°€ë¡œë§‰ëŒ€ â€” ë¼ë²¨ ì˜ë¦¼ ë°©ì§€ + ê°’ ì™¸ë¶€í‘œì‹œ
                with rc:
                    st.markdown("**ìƒìœ„ í‚¤ì›Œë“œ Top-10**")
                    if top20:
                        bar_df = pd.DataFrame(top20, columns=["í‚¤ì›Œë“œ", "ë¹ˆë„"])
                        fig_bar = px.bar(
                            bar_df.sort_values("ë¹ˆë„"),
                            x="ë¹ˆë„", y="í‚¤ì›Œë“œ", orientation="h", text="ë¹ˆë„"
                        )
                        fig_bar = style_fig(fig_bar, "Top-10 í‚¤ì›Œë“œ", legend="none", top_margin=64,
                        bg_color=VIZ_BG["bar_topk"], bg_alpha=0.5)
                        fig_bar.update_traces(textposition="outside", cliponaxis=False)
                        fig_bar.update_xaxes(title_text="ë¹ˆë„")
                        fig_bar.update_yaxes(title_text=None)
                        st.plotly_chart(style_fig(fig_bar, "Top-10 í‚¤ì›Œë“œ", legend="none", top_margin=64),
                                        use_container_width=True, config={"displayModeBar": False})
                    else:
                        st.info("í‘œì‹œí•  í‚¤ì›Œë“œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

            with tab_table:
                st.markdown("#### í”„ë¡œì íŠ¸ ëª©ë¡")
                cols_pref = ["íŒŒì¼ëª…","ì§€ì›ê¸°ê´€","ì‚¬ì—… ê¸°ê°„","ì£¼ì œë¶„ë¥˜(ëŒ€)","ICT ìœ í˜•","ì£¼ìš” ë‚´ìš©","ê¸°ëŒ€ íš¨ê³¼","Hashtag_str","ëŒ€ìƒê¸°ê´€","ëŒ€ìƒêµ­"]
                cols_use  = pick_existing_columns(sub, cols_pref, fallback_max=10)
                st.caption(f"í‘œì‹œ ì»¬ëŸ¼: {', '.join(cols_use)}")
                st.dataframe(sub[cols_use].drop_duplicates().reset_index(drop=True), use_container_width=True)

    else:
        st.info("ìƒë‹¨ ì§€ë„ì—ì„œ êµ­ê°€ë¥¼ í´ë¦­í•˜ë©´ ìƒì„¸ê°€ ì—´ë¦½ë‹ˆë‹¤.")

# ===================== â‘¡ ICT ìœ í˜• ë‹¨ì¼í´ë˜ìŠ¤ (ì§€ë„ë¥¼ êµ­ê°€ í•˜ì´ë¼ì´íŠ¸ë¡œë§Œ ì‚¬ìš©, ìƒì„¸ëŠ” 'í´ë˜ìŠ¤ ì „ì²´' ê¸°ì¤€) =====================
elif mode == "ICT ìœ í˜• ë‹¨ì¼í´ë˜ìŠ¤":
    st.subheader("ICT ìœ í˜• ë‹¨ì¼í´ë˜ìŠ¤ í”„ë¡œì íŠ¸ ìˆ˜")

    # 1) í´ë˜ìŠ¤ ì„ íƒ
    wb_classes = [c for c in sorted(df["ICT ìœ í˜•"].astype(str).str.strip().dropna().unique()) if c and c != "nan"]
    if not wb_classes:
        st.info("ICT ìœ í˜• ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    sel = st.selectbox("ICT ìœ í˜• ì„ íƒ", wb_classes, index=0, key="wb_class_select_main")

    # 2) ì§€ë„(ê°œìš”): ì´ Classê°€ ìˆ˜í–‰ëœ 'êµ­ê°€ í•˜ì´ë¼ì´íŠ¸'ë§Œ, í´ë¦­ì€ ì§‘ê³„ì— ì˜í–¥ X
    sub_wb_geo = dfx[dfx["ICT ìœ í˜•"].astype(str).str.strip() == sel]  # ì§€ë„ìš©(êµ­ê°€ í™•ì¥ë³¸ ì‚¬ìš©)
    agg_geo = sub_wb_geo.groupby(["iso3", "country_ko"], as_index=False).agg(n=("íŒŒì¼ëª…", "nunique"))
    value_map = {r.iso3: int(r.n) for _, r in agg_geo.iterrows()}
    gj = augment_geojson_values(world_geojson, key_on_info, value_map, "ksp_wb_cnt")

    base = make_base_map()
    ch = folium.Choropleth(
        geo_data=gj, data=agg_geo, columns=["iso3","n"],
        key_on=key_on_info[0],
        fill_color="PuBuGn", fill_opacity=0.90,
        line_opacity=0.5, line_color="#888",
        nan_fill_color="#fbfbfb", legend_name=f"{sel} ê±´ìˆ˜", highlight=True
    ); ch.add_to(base)
    ch.geojson.add_child(folium.features.GeoJsonTooltip(
        fields=["ISO3", "name" if "name" in gj["features"][0]["properties"] else "ISO3", "ksp_wb_cnt"],
        aliases=["ISO3","êµ­ê°€","ê±´ìˆ˜"], sticky=False
    ))
    ch.geojson.add_child(folium.features.GeoJsonPopup(fields=["ISO3"], aliases=["ISO3"]))
    # í´ë¦­ì€ ë³´ì¡° ì •ë³´ë¡œë§Œ ì‚¬ìš©(ì„ íƒêµ­ê°€ í‘œì‹œì—ë§Œ ì“°ê³ , ë³¸ë¬¸ ì§‘ê³„ì—ëŠ” ì˜í–¥ X)
    ret = st_folium(base, height=520, use_container_width=True)
    clicked_iso = extract_iso_from_stfolium(ret)

    # 3) ìƒì„¸ íŒ¨ë„ â€” â˜… í•µì‹¬: 'í´ë˜ìŠ¤ ì „ì²´' ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„/ì‹œê°í™” â˜…
    st.subheader("ìƒì„¸ íŒ¨ë„ â€” ICT ìœ í˜•")

    # ë³¸ë¬¸ ì§‘ê³„ìš©ì€ 'êµ­ê°€ í™•ì¥ ì—†ëŠ” ì›ë³¸ df'ì—ì„œ í•„í„° (ë™ì¼ ë³´ê³ ì„œê°€ ë‹¤êµ­ê°€ì— ì¤‘ë³µ ì§‘ê³„ë˜ëŠ” ë¬¸ì œ ë°©ì§€)
    sub_wb = df[df["ICT ìœ í˜•"].astype(str).str.strip() == sel].copy()

    # ìƒë‹¨ íƒ€ì´í‹€ + ë©”íŠ¸ë¦­
    n_docs = sub_wb["íŒŒì¼ëª…"].nunique()
    part_countries = sub_wb_geo["iso3"].nunique()  # ì°¸ì—¬êµ­ê°€ ìˆ˜(ì§€ë„ìš© í™•ì¥ dfë¡œ ê³„ì‚°)
    st.markdown(f"### {sel} â€” ì „ì²´ í”„ë¡œì íŠ¸ {n_docs:,}ê±´ Â· ì°¸ì—¬êµ­ê°€ {part_countries:,}ê°œêµ­")

    tab_overview, tab_brief, tab_cloud, tab_extract, tab_table = st.tabs(
    ["ê°œìš”", f"{sel} ì¢…í•©ìš”ì•½", "ì›Œë“œí´ë¼ìš°ë“œ / í‚¤ì›Œë“œ", "í‚¤ì›Œë“œ ë¬¸ì¥ ë°œì·Œ", "í…Œì´ë¸”"]
    )


    # ---- (1) ê°œìš”: ì—°ë„ ë²”ìœ„, ëŒ€ìƒê¸°ê´€ ìˆ˜, ì°¸ì—¬êµ­ê°€ ìƒìœ„ ë³´ê¸°(ì„ íƒ êµ­ê°€ ë³´ì¡° í‘œê¸°) ----
    with tab_overview:
        _suby = expand_years(sub_wb)
        sub_years = sorted(set(_suby["ì—°ë„"].dropna().astype(int).tolist()))
        cA, cB, cC = st.columns(3)
        with cA:
            st.metric("ì—°ë„ ë²”ìœ„", f"{min(sub_years) if sub_years else '-'}â€“{max(sub_years) if sub_years else '-'}")
        with cB:
            st.metric("í”„í† ì íŠ¸ ìˆ˜", f"{n_docs:,}")
        with cC:
            st.metric("ëŒ€ìƒê¸°ê´€ ìˆ˜", f"{sub_wb['ëŒ€ìƒê¸°ê´€'].nunique():,}")

        # ì°¸ì—¬êµ­ê°€ Top-10 (í”„ë¡œì íŠ¸ ìˆ˜ ê¸°ì¤€)
        st.markdown("#### ì°¸ì—¬êµ­ê°€ (í”„ë¡œì íŠ¸ ìˆ˜ Top 10)")
        top_c = (sub_wb_geo.groupby(["country_ko"], as_index=False)
                          .agg(ê±´ìˆ˜=("íŒŒì¼ëª…","nunique"))
                          .sort_values("ê±´ìˆ˜", ascending=False).head(10))
        if clicked_iso:
            # í´ë¦­í•œ êµ­ê°€ê°€ ìˆìœ¼ë©´ ì¹©ìœ¼ë¡œ ë³´ì¡° í‘œê¸°
            iso_name = sub_wb_geo.loc[sub_wb_geo["iso3"]==clicked_iso, "country_ko"]
            if len(iso_name):
                st.caption(f"ì§€ë„ë¡œ ì„ íƒëœ êµ­ê°€: **{iso_name.iloc[0]}** (ì§‘ê³„ì—ëŠ” ì˜í–¥ ì—†ìŒ)")

        st.dataframe(top_c.reset_index(drop=True), use_container_width=True)

    # ---- (2) ì¢…í•©ìš”ì•½: WB_ClassBriefs.ipynbì—ì„œ sel í‚¤ë¡œ ë¡œë“œ ----
    with tab_brief:
        if isinstance(wb_briefs_map, dict) and wb_briefs_map:
            # í‚¤ ë§¤ì¹­(ëŒ€ì†Œë¬¸ì/ê³µë°± ë¬´ì‹œ)
            key_fold = next((k for k in wb_briefs_map.keys()
                             if str(k).strip().lower() == str(sel).strip().lower()), None)
            brief_txt = wb_briefs_map.get(sel) or (wb_briefs_map.get(key_fold) if key_fold else None)
            st.write(brief_txt if brief_txt else f"'{sel}' ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤. WB_ClassBriefs.ipynbì— ì¶”ê°€í•˜ì„¸ìš”.")
        else:
            st.info("ì¢Œì¸¡ì—ì„œ WB_ClassBriefs.ipynbë¥¼ ì§€ì •í•˜ì„¸ìš”.")

    # ---- (3) ì›Œë“œí´ë¼ìš°ë“œ/í‚¤ì›Œë“œ: í´ë˜ìŠ¤ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ìƒì„± (êµ­ê°€ ë¬´ê´€) ----
    with tab_cloud:
        tokens: list[str] = []
        if "Hashtag_str" in sub_wb.columns and sub_wb["Hashtag_str"].notna().any():
            for txt in sub_wb["Hashtag_str"].dropna().astype(str):
                tokens += [z.strip() for z in re.split(r"[;,]", txt) if z.strip()]
        elif "Hashtag" in sub_wb.columns and sub_wb["Hashtag"].notna().any():
            for txt in sub_wb["Hashtag"].dropna().astype(str):
                tokens += [z.strip() for z in re.split(r"[;,]", txt) if z.strip()]

        pool_cols = [c for c in ["ìš”ì•½", "ì£¼ìš” ë‚´ìš©"] if c in sub_wb.columns]
        if pool_cols:
            for txt in sub_wb[pool_cols].fillna("").astype(str).agg(" ".join, axis=1).tolist():
                for w in re.split(r"[^0-9A-Za-zê°€-í£]+", txt):
                    w = w.strip()
                    if len(w) >= 2:
                        tokens.append(w)

        tokens = [w for w in tokens if w and w.lower() not in STOP_LOW and not re.fullmatch(r"\d+(\.\d+)?", w)]
        freq   = Counter(tokens)
        top20  = freq.most_common(10)
        top_freqs = dict(freq.most_common(220))

        lc, rc = st.columns([6, 7], gap="large")
        with lc:
            st.markdown("**ì›Œë“œí´ë¼ìš°ë“œ**")
            if top_freqs:
                png_bytes = render_wordcloud_png(top_freqs, bg_color=VIZ_BG["wc"], alpha=0.5)
                if png_bytes:
                    st.image(png_bytes, use_container_width=True, output_format="PNG")  # â˜… ì•ˆì „
                else:
                    if not WC_FONT_PATH:
                        st.error("ì›Œë“œí´ë¼ìš°ë“œìš© í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë¦¬í¬ì— assets/fonts/NanumGothic.ttf ì¶”ê°€ ë˜ëŠ” packages.txtì— fonts-nanum)")
                    else:
                        st.info("í‘œì‹œí•  ë‹¨ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                chips = " ".join([f'<span class="ksp-chip">{k}</span>' for k, _ in freq.most_common(12)])
                st.markdown(chips, unsafe_allow_html=True)
            else:
                st.info("í‘œì‹œí•  ë‹¨ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        with rc:
            st.markdown("**ìƒìœ„ í‚¤ì›Œë“œ Top-10**")
            if top20:
                bar_df = pd.DataFrame(top20, columns=["í‚¤ì›Œë“œ","ë¹ˆë„"])
                fig_bar = px.bar(bar_df.sort_values("ë¹ˆë„"), x="ë¹ˆë„", y="í‚¤ì›Œë“œ",
                                 orientation="h", text="ë¹ˆë„")
                fig_bar = style_fig(fig_bar, f"Top-10 í‚¤ì›Œë“œ ({sel})",
                                    legend="none", top_margin=64,
                                    bg_color=VIZ_BG["bar_topk"], bg_alpha=0.5)
                fig_bar.update_traces(textposition="outside", cliponaxis=False)
                fig_bar.update_xaxes(title_text="ë¹ˆë„"); fig_bar.update_yaxes(title_text=None)
                st.plotly_chart(fig_bar, use_container_width=True, config={"displayModeBar": False})
            else:
                st.info("í‘œì‹œí•  í‚¤ì›Œë“œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        with tab_extract:
            st.markdown("#### ëŒ€í‘œ í‚¤ì›Œë“œ ë¬¸ì¥ ë°œì·Œ (ì„ë² ë”© ê¸°ë°˜ Â· TF-IDF)")

            
            
                    
            # (1) í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ìë™ ì„ íƒ (full_text > ì£¼ìš” ë‚´ìš© > ìš”ì•½)
            # (1) í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ìë™ ì„ íƒ
            pref_cols = ["full_text", "ì£¼ìš” ë‚´ìš©", "ìš”ì•½"]
            text_cols = [c for c in pref_cols if c in sub_wb.columns]
            if not text_cols:
                st.info("ë¬¸ì¥ ë°œì·Œì— ì‚¬ìš©í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. (full_text/ì£¼ìš” ë‚´ìš©/ìš”ì•½ ì¤‘ í•˜ë‚˜ í•„ìš”)")
                st.stop()
            
            # â–¼ ì´ ì¤„ì„ "í‚¤ì›Œë“œ ì„ íƒ/ë Œë”" ë¸”ë¡ì˜ ë§¨ ìœ„ì— ì¶”ê°€
            RUN_TAG = f"extract_once::{sel}::{','.join(text_cols)}"
            
            
            # --- ì—¬ê¸°ë¶€í„° ë‹¹ì‹ ì˜ ê¸°ì¡´ ì½”ë“œ ---
            # (2) ë¬¸ì„œ ì¤€ë¹„
            docs_class = _prep_docs(sub_wb, text_cols)
            docs_neg   = _prep_docs(df[df["ICT ìœ í˜•"].astype(str).str.strip() != sel], text_cols)
        
            # (3) ëŒ€ë¹„í˜• TF-IDF í‚¤ì›Œë“œ
            candidates = contrastive_keywords_tfidf(
                docs_class=docs_class,
                docs_neg=docs_neg,
                top_n=80,
                ngram_bonus=(0.10, 0.20)
            )
        
            # (4) ì„ íƒ ìˆ˜ këŠ” í•˜ë“œ í´ë¨í”„
            k_req = int(st.session_state.get("topk_auto", 8))
            k     = max(1, min(k_req, 8))
            diversity = float(st.session_state.get("diversity", 0.65))
            per_kw    = int(st.session_state.get("per_kw", 2))
            seed      = int(st.session_state.get("seed", 42))
        
            kw_selected = mmr_select_text(candidates, k=k, lambda_div=diversity)
            kw_selected = kw_selected[:k]  # í˜¹ì‹œë¼ë„ ì´í›„ì— ëˆ„ê°€ ë” ë¶™ì´ë©´ ì˜ë¼ì„œ ë³´ì¥
            st.caption(f"[debug] k={k} / candidates={len(candidates)} / selected={len(kw_selected)}")
        
       
        
            # (5) ë¬¸ì¥ ìƒ˜í”Œë§/í‘œì‹œ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
            if not kw_selected:
                st.info("ì„ íƒëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.markdown("<style>.ksp-quote{background:var(--card);border:1px solid var(--border);padding:10px;border-radius:10px;margin:6px 0}</style>", unsafe_allow_html=True)
                cols = st.columns(2, gap="large") if len(kw_selected) >= 6 else [st.container()]
                for i, kw in enumerate(kw_selected):
                    target_col = cols[i % len(cols)]
                    with target_col:
                        st.markdown(f"**ğŸ” {kw}**")
                        sents = sample_sentences_for_keyword(sub_wb, kw, text_cols, per_kw=int(per_kw), seed=int(seed))
                        if not sents:
                            st.caption("Â· ì¼ì¹˜ ë¬¸ì¥ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            for fn, html_sent in sents:
                                meta = f"<div style='font-size:12px;color:#6b7280'>{fn}</div>" if fn else ""
                                st.markdown(f"<div class='ksp-quote'>{html_sent}{meta}</div>", unsafe_allow_html=True)
            




    # ---- (4) í…Œì´ë¸”: í´ë˜ìŠ¤ ì „ì²´ ë³´ê³ ì„œ ëª©ë¡ ----
    with tab_table:
        st.markdown("#### í”„ë¡œì íŠ¸ ëª©ë¡ (í´ë˜ìŠ¤ ì „ì²´)")
        cols_pref = ["íŒŒì¼ëª…","ì§€ì›ê¸°ê´€","ì‚¬ì—… ê¸°ê°„","ì£¼ì œë¶„ë¥˜(ëŒ€)","ICT ìœ í˜•","ëŒ€ìƒêµ­","ëŒ€ìƒê¸°ê´€","ì£¼ìš” ë‚´ìš©","ê¸°ëŒ€ íš¨ê³¼","Hashtag_str"]
        cols_use  = pick_existing_columns(sub_wb, cols_pref, fallback_max=12)
        st.caption(f"í‘œì‹œ ì»¬ëŸ¼: {', '.join(cols_use)}")
        st.dataframe(sub_wb[cols_use].drop_duplicates().reset_index(drop=True), use_container_width=True)





# --------------------- ì „ì²´ ë¶„í¬ ëŒ€ì‹œë³´ë“œ ---------------------
st.markdown("---")
st.subheader("ì „ì²´ ë¶„í¬ ëŒ€ì‹œë³´ë“œ")

# ì£¼ì œ ë„ë„›
subj_counts = df["ì£¼ì œë¶„ë¥˜(ëŒ€)"].fillna("ë¯¸ë¶„ë¥˜").astype(str).str.strip().replace({"nan":"ë¯¸ë¶„ë¥˜"}).value_counts().reindex(SUBJ_ORDER, fill_value=0).reset_index()
subj_counts.columns = ["ì£¼ì œë¶„ë¥˜(ëŒ€)","count"]
fig1 = px.pie(subj_counts, names="ì£¼ì œë¶„ë¥˜(ëŒ€)", values="count", hole=0.55,
              category_orders={"ì£¼ì œë¶„ë¥˜(ëŒ€)": SUBJ_ORDER},
              color="ì£¼ì œë¶„ë¥˜(ëŒ€)", color_discrete_map=COLOR_SUBJ)
fig1 = style_fig(fig1, "ì£¼ì œë¶„ë¥˜(ëŒ€) ë¶„í¬", legend="right", top_margin=120,
                 bg_color=VIZ_BG["donut_subj"], bg_alpha=0.5)

# ICT ë„ë„›
wb_counts = (df["ICT ìœ í˜•"].astype(str).str.strip().replace({"nan":"ë¯¸ë¶„ë¥˜"}).fillna("ë¯¸ë¶„ë¥˜").value_counts()
             .reindex(WB_ORDER, fill_value=0).reset_index())
wb_counts.columns = ["ICT ìœ í˜•","count"]
fig2 = px.pie(wb_counts, names="ICT ìœ í˜•", values="count", hole=0.55,
              category_orders={"ICT ìœ í˜•": WB_ORDER},
              color="ICT ìœ í˜•", color_discrete_map=COLOR_WB)
fig2 = style_fig(fig2, "ICT ìœ í˜• ë¶„í¬", legend="right", top_margin=120,
                 bg_color=VIZ_BG["donut_wb"], bg_alpha=0.5)


c0, c00 = st.columns([1,1], gap="large")
with c0: st.plotly_chart(fig1, use_container_width=True)
with c00: st.plotly_chart(fig2, use_container_width=True)

# (3) ì£¼ì œÃ—WB 100% ëˆ„ì  ë§‰ëŒ€
cross = (df.assign(WB=df["ICT ìœ í˜•"].astype(str).str.strip().replace({"nan":"ë¯¸ë¶„ë¥˜"}).fillna("ë¯¸ë¶„ë¥˜"))
           .groupby(["ì£¼ì œë¶„ë¥˜(ëŒ€)","WB"], as_index=False).size())

pivot = cross.pivot(index="ì£¼ì œë¶„ë¥˜(ëŒ€)", columns="WB", values="size").fillna(0)
pivot_pct = (pivot
    .div(pivot.sum(axis=1).replace(0, np.nan), axis=0)
    .fillna(0)
    .reset_index()
    .melt(id_vars="ì£¼ì œë¶„ë¥˜(ëŒ€)", var_name="WB", value_name="pct"))

fig3 = px.bar(
    pivot_pct,
    x="ì£¼ì œë¶„ë¥˜(ëŒ€)", y="pct",
    color="WB", barmode="stack",
    category_orders={"WB": WB_ORDER, "ì£¼ì œë¶„ë¥˜(ëŒ€)": SUBJ_ORDER},
    color_discrete_map=COLOR_WB,
)
fig3.update_yaxes(range=[0,1], tickformat=".0%")
fig3.update_layout(bargap=0.68, bargroupgap=0.08)
fig3 = style_fig(fig3, "ì£¼ì œë¶„ë¥˜(ëŒ€)ë³„ ICT ìœ í˜• ë¹„ì¤‘ (100%)",
                 legend="right", top_margin=120,
                 bg_color=VIZ_BG["stack_100"], bg_alpha=0.5)
st.plotly_chart(fig3, use_container_width=True)


# ---------- (4)(5) ì—°ë„ë³„ ë¹„ì¤‘ â€” ì„ íƒí˜• ì‹œê°í™” (íˆíŠ¸ë§µ ì œê±°) ----------
dfy_valid = dfy.dropna(subset=["ì—°ë„"]).copy()

def time_share(df_in: pd.DataFrame, group_col: str) -> pd.DataFrame:
    """
    - ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
    - 'ì—°ë„'ì™€ group_colì´ DataFrameë¡œ ë“¤ì–´ì˜¤ë©´ ì²« ì—´ë§Œ ì‚¬ìš©
    """
    df1 = df_in.loc[:, ~df_in.columns.duplicated()].copy()

    y = df1["ì—°ë„"]
    if isinstance(y, pd.DataFrame):
        y = y.iloc[:, 0]

    gcol = df1[group_col]
    if isinstance(gcol, pd.DataFrame):
        gcol = gcol.iloc[:, 0]

    tmp = pd.DataFrame({"ì—°ë„": y, group_col: gcol})
    g = tmp.groupby(["ì—°ë„", group_col], as_index=False).size()
    totals = g.groupby("ì—°ë„")["size"].transform("sum")
    g["pct"] = g["size"] / totals
    return g


def draw_year_chart(g, group_col, title_prefix):
    if g.empty:
        fig = px.line()
        return style_fig(fig, f"{title_prefix} (ì—°ë„ ì¶”ì¶œ ë¶ˆê°€)")

    is_subj = (group_col == "ì£¼ì œë¶„ë¥˜(ëŒ€)")
    c_orders = {"ì—°ë„": sorted(g["ì—°ë„"].unique())}
    if is_subj:
        c_orders[group_col] = SUBJ_ORDER
        color_map = COLOR_SUBJ
    else:
        # group_col == "WB" (ICT ìœ í˜•)
        c_orders[group_col] = WB_ORDER
        color_map = COLOR_WB

    if year_mode == "ìˆœìœ„ Bump":
        ranks = g.copy()
        ranks["rank"] = ranks.groupby("ì—°ë„")["pct"].rank(ascending=False, method="dense")
        fig = px.line(
            ranks, x="ì—°ë„", y="rank", color=group_col, markers=True,
            category_orders=c_orders, color_discrete_map=color_map
        )
        fig.update_traces(line=dict(width=3), marker=dict(size=8))
        fig.update_yaxes(autorange="reversed", dtick=1, title="ìˆœìœ„(1=ìµœìƒ)")
        return style_fig(fig, f"{title_prefix} â€” ìˆœìœ„ Bump", legend="top", top_margin=120)
    else:
        # 100% ëˆ„ì  ë§‰ëŒ€ ëŒ€ì‹  ë¼ì¸ ë¹„ì¤‘ ê·¸ë˜í”„ë¥¼ ì“°ê¸°ë¡œ í•œ í˜„ì¬ ì½”ë“œì— ë§ì¶¤
        fig = px.line(
            g, x="ì—°ë„", y="pct", color=group_col, markers=True,
            labels={"pct":"ë¹„ì¤‘"},
            category_orders=c_orders, color_discrete_map=color_map
        )
        fig.update_yaxes(range=[0,1], tickformat=".0%")
        return style_fig(fig, f"{title_prefix} â€” ë¹„ì¤‘ Bump", legend="top", top_margin=120)


if not dfy_valid.empty:
    g_subj = time_share(dfy_valid, "ì£¼ì œë¶„ë¥˜(ëŒ€)")
    g_wb   = time_share(dfy_valid.assign(WB=dfy_valid["ICT ìœ í˜•"].astype(str).str.strip().replace({"nan":"ë¯¸ë¶„ë¥˜"}).fillna("ë¯¸ë¶„ë¥˜")), "WB")
else:
    g_subj = pd.DataFrame(columns=["ì—°ë„","ì£¼ì œë¶„ë¥˜(ëŒ€)","size","pct"])
    g_wb   = pd.DataFrame(columns=["ì—°ë„","WB","size","pct"])

fig4 = draw_year_chart(g_subj, "ì£¼ì œë¶„ë¥˜(ëŒ€)", "ì—°ë„ë³„ ì£¼ì œë¶„ë¥˜(ëŒ€) ë¹„ì¤‘")
fig5 = draw_year_chart(g_wb, "WB", "ì—°ë„ë³„ ICT ìœ í˜• ë¹„ì¤‘")
c1, c2 = st.columns([1,1], gap="large")
with c1: st.plotly_chart(fig4, use_container_width=True)
with c2: st.plotly_chart(fig5, use_container_width=True)

# =====================================================================
# ì¶”ê°€ ì‹œê°í™” â‘ : ëŒ€í‘œ í‚¤ì›Œë“œ ìƒëŒ€ íŠ¸ë Œë“œ(ìƒìŠ¹ì„¸/í•˜ë½ì„¸)  â€” Plotly
# =====================================================================
st.markdown("---")
st.subheader("AI ì¶”ì¶œ í‚¤ì›Œë“œ ìƒëŒ€ íŠ¸ë Œë“œ (ìƒìŠ¹/í•˜ë½)")

# 'ëŒ€í‘œ í‚¤ì›Œë“œ ìƒëŒ€ íŠ¸ë Œë“œ'ì— ì¶”ê°€ ì ìš©í•  ë¶ˆìš©ì–´
BASE_STOP = { 
    "ê²½ì œ","ì‚¬íšŒ","ì‚¬íšŒì •ì±…","ì •ì±…","ë°ì´í„°","ë””ì§€í„¸","ì„œë¹„ìŠ¤","ì‹œì¥","ìš´ì˜","í˜„í™©","ì „ëµ","ë°©ì•ˆ","ë„ì…",
    "ê°œì„ ","êµ¬ì¶•","ì²´ê³„","ê¸°ë°˜","ì¤‘ì¥ê¸°","ìµœì¢…ë³´ê³ ","ì¤‘ê°„ë³´ê³ ","ë¶„ì„","ì§€ì›","ì •ë¶€","ê³µê³µ","êµ­ê°€","ì°¨ì„¸ëŒ€",
    "í‰ê°€","í”„ë¡œì íŠ¸","ë¡œë“œë§µ","ë¹„ì „","í™œìš©","ê°•í™”","í™•ëŒ€","ì˜ˆì •","ì—°êµ¬","ì‚¬ë¡€","í˜„ì§€","ì •í•©ì„±","ìˆ˜ë¦½",
    "ë§ˆìŠ¤í„°í”Œëœ","ê°œí¸","ê³ ë„í™”","ê°œì •","ê°œë°œ","ì—…ê·¸ë ˆì´ë“œ","ì ìš©","ì‹œë²”","ì»¨ì„¤íŒ…","í˜‘ë ¥","ì •ë¹„","ë„ì‹œ",
    "ì¸í”„ë¼","í”Œë«í¼","í”Œë ›í¼","ì‹œìŠ¤í…œ","í¬í„¸","ì¡°ë‹¬","ë²•ì œ","ì œë„","ê°€ì´ë“œë¼ì¸","ê¸°íš","ì¶”ì§„","ì„±ê³¼",
    "í˜„ì•ˆ","ê³¼ì œ","ê¸°ìˆ ","ê³„íš","ìë£Œ","ë³´ê³ ","ìš”ì•½","ì¥ë‹¨ì ","í•œê³„","ê²½ë³´","ì•ˆì „","ë³´ì•ˆ","ì„±ì¥",
    "ì „ìì„¸ê¸ˆê³„ì‚°ì„œ","ë²•ì ","ì˜ë¬´í™”","ì˜ˆì‚°","ê°€ë­„","êµìœ¡","ê°œì¸ì •ë³´ë³´í˜¸","vat","ê±´ì„¤","vision","ì„¸ì •","ë‹¤ì¸µ","ë¯¼ê°„","ê·¼ê±°",
    "ì‚°ì—…","ì„¸ìˆ˜","ì„¸ë¬´ì¡°ì§","ì¬ì •","ì¸ì‚¬","ì¬ë¬´ë¶€","íˆ¬ì","í†µí•©","í›ˆë ¨","í™ë³´","ì¡°ì •","ë¬´ì—­","í™ìˆ˜","í´ë¼ìš°ë“œ","ë°ì´í„°ì„¼í„°",
    "ì „ìì •ë¶€","ì¶”ì •","ì†ŒìŠ¤","ì½˜í…ì¸ ", "ì¡°ì„¸", "ì˜ë£Œ", "êµí†µ", "ip", "Ip", "ì¸ì¦", "í˜ê¸°ë¬¼", "ë‚©ì„¸ì", "ì˜ì•½í’ˆ", "ìƒì‚°ì„±",
    "ì „ì", "ê°ì‚¬", "ê³µë¬´ì›ì˜", "ë“±ë¡", "ì§‘í–‰", "ì‚¬ì´ë²„", "ì¡°ì„¸í–‰ì •", "ë†’ì—¬", "ì›ê²©", "ì‚¬ìš©ì", "ì½œì„¼í„°", "ê¸°ê´€ë³„", "ì—ë„ˆì§€", "ì „ìì¡°ë‹¬", "ê¸ˆìœµ", "ë‚©ì„¸", "ì •ë³´í™”",
    "ìˆìŒ", "ì§€ì†ê°€ëŠ¥í•œ",
    # ì¶•/ë¼ë²¨ ê´€ë ¨ ë¶ˆìš©ì–´ ì¶”ê°€
    "ì—°ë„","ë…„ë„","year","years",
    # ì˜ë¬¸ ìƒíˆ¬ì–´
    "and","or","of","in","to","for","the","with","on","by","from","eu",
    "data","digital","service","services","policy","strategy","plan","roadmap","project","program",
    "system","platform","portal","model","evaluation","improvement","implementation","phase","final","interim",
    "procurement"
}

BASE_STOP_LOW = {s.lower() for s in BASE_STOP}
 

# ---- ê³ ì • íŒŒë¼ë¯¸í„° (ìŠ¬ë¼ì´ë” ì œê±°)
TOP_K_PER_FIG = 25   # ìƒìŠ¹/í•˜ë½ ê°ê° í‘œê¸° í‚¤ì›Œë“œ ìˆ˜
ROLL = 5             # Jeffreys + ë¡¤ë§ ìœˆë„(ë…„)
ALPHA = 0.7
WINDOW_YEARS = 10
RECENT_YEARS = 5
MIN_DOCS_BASE, MIN_YEARS_BASE = 4, 3
RECENT_DOCS_MIN, RECENT_YEARS_MIN = 2, 2

HASHTAG_COL = "Hashtag" if "Hashtag" in df.columns else ("Hashtag_str" if "Hashtag_str" in df.columns else None)

def clean(s): return s.astype(str).str.replace(r"\s+"," ",regex=True).str.strip()




SYN = {"sme":"SME","pki":"PKI","ai":"AI","ict":"ICT","bigdata":"ë¹…ë°ì´í„°","big data":"ë¹…ë°ì´í„°",
       "e-gp":"ì „ìì¡°ë‹¬","egp":"ì „ìì¡°ë‹¬","e-procurement":"ì „ìì¡°ë‹¬","data center":"ë°ì´í„°ì„¼í„°","cloud":"í´ë¼ìš°ë“œ",
       "platform":"í”Œë«í¼","platfrom":"í”Œë«í¼","í”Œë ›í¼":"í”Œë«í¼", "ifmis":"IFMIS", "bim":"BIM"}

def norm_token(x: str) -> str:
    x = re.sub(r"[\"'â€™â€œâ€()\[\]{}<>]", "", x.strip()); xl = x.lower()
    return SYN.get(xl, x)

import ast

def _clean_token(x: str) -> str:
    # ë”°ì˜´í‘œ/ëŒ€ê´„í˜¸/ê´„í˜¸ë¥˜ ì œê±° + ê³µë°± ì •ë¦¬ + ë™ì˜ì–´ ë§¤í•‘
    x = re.sub(r"[\"'â€™â€œâ€()\[\]{}<>]", "", str(x).strip())
    x = re.sub(r"\s{2,}", " ", x)
    xl = x.lower()
    return SYN.get(xl, x)

def split_hashtags(s, stopset):
    """
    í•´ì‹œíƒœê·¸ ì…€ í•˜ë‚˜ë¥¼ -> í† í° ë¦¬ìŠ¤íŠ¸ë¡œ.
    - "['ì¡°ë‹¬','ì „ìì¡°ë‹¬']" ê°™ì€ ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´ì€ literal_evalë¡œ íŒŒì‹±
    - ì‹¤íŒ¨í•˜ë©´ ì¼ë°˜ êµ¬ë¶„ê¸°í˜¸(,;/ ê³µë°±2+)ë¡œ ë¶„í• 
    - êµ­ê°€/ìˆ«ì/ë¶ˆìš©ì–´/ì¡ë¬¸ì ì œê±°
    """
    if not isinstance(s, str) or not s.strip():
        return []

    items = []
    txt = s.strip()

    # 1) ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´ì´ë©´ ì•ˆì „ íŒŒì‹±
    if txt.startswith("[") and txt.endswith("]"):
        try:
            arr = ast.literal_eval(txt)
            if isinstance(arr, (list, tuple)):
                items = [str(z) for z in arr]
        except Exception:
            items = []  # íŒŒì‹± ì‹¤íŒ¨í•˜ë©´ ì•„ë˜ fallbackë¡œ ì´ì–´ê°

    # 2) fallback: ì¼ë°˜ ë¶„í• 
    if not items:
        items = re.split(r"[,\;/]| {2,}", txt)

    out = []
    for t in items:
        t = _clean_token(t)
        core = re.sub(r"\s+", "", t.lower())
        if not core or len(core) < 2:
            continue
        if re.fullmatch(r"[\W_]+", core) or re.fullmatch(r"\d+(\.\d+)?", core):
            continue
        if core in stopset:
            continue
        out.append(t)

    # ì¤‘ë³µ ì œê±°(ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
    seen = set()
    dedup = []
    for w in out:
        k = w.lower()
        if k not in seen:
            seen.add(k)
            dedup.append(w)
    return dedup


def jeffreys_rolling_ratio(num, den, k=ROLL, alpha=ALPHA):
    numr = (num + alpha).rolling(k, center=True, min_periods=1).sum()
    denr = (den + 2*alpha).rolling(k, center=True, min_periods=1).sum()
    return (numr/denr*100.0).fillna(0.0)

@st.cache_data(show_spinner=False)
def build_keyword_time(df_in: pd.DataFrame, stop_extra: set):
    df_local = df_in.loc[:, ~df_in.columns.duplicated()].copy()

    # ì—°ë„ ì†ŒìŠ¤: ì§€ì •/ìë™(ìš”ì•½Â·ë‚´ìš© í¬í•¨) â†’ ë¦¬ìŠ¤íŠ¸ë¡œ í™•ì¥
    ser_year = _year_text_series(df_local)
    years_list = ser_year.apply(years_from_span)
    all_years = sorted({y for ys in years_list for y in (ys or [])})
    if not all_years:
        return [], {}, pd.Series([], dtype=int), pd.DataFrame()

    # ë™ì  ë¶ˆìš©ì–´(ëŒ€ë¶„ë¥˜/í´ë˜ìŠ¤/êµ­ê°€ ë“±)
    dyn = set()
    for col in ["ì£¼ì œë¶„ë¥˜(ëŒ€)", "ICT ìœ í˜•", "ëŒ€ìƒêµ­", "ëŒ€ìƒê¸°ê´€", "ì§€ì›ê¸°ê´€"]:
        if col in df_local.columns:
            dyn |= {str(v).strip().lower() for v in df_local[col].dropna().unique()}
    stopset = {w.lower() for w in stop_extra} | dyn

    # í•´ì‹œíƒœê·¸ í† í° (ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´ í¬í•¨ ì•ˆì „ íŒŒì‹±)
    HASHTAG_COL = "Hashtag" if "Hashtag" in df_local.columns else ("Hashtag_str" if "Hashtag_str" in df_local.columns else None)
    if HASHTAG_COL:
        tokens_by_row = [split_hashtags(s, stopset) for s in df_local[HASHTAG_COL].fillna("").astype(str)]
    else:
        tokens_by_row = [[] for _ in range(len(df_local))]

    # ì—°ë„ë³„ ì´ ë¬¸ì„œ ìˆ˜
    docs_per_year = pd.Series(0, index=all_years, dtype=int)
    for ys in years_list:
        for y in (ys or []):
            docs_per_year[y] += 1

    # ì—°ë„ë³„ í‚¤ì›Œë“œ ë“±ì¥ ìˆ˜(ë¬¸ì„œ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°)
    kw_doc = {y: Counter() for y in all_years}
    for toks, ys in zip(tokens_by_row, years_list):
        if not ys or not toks:
            continue
        for y in ys:
            kw_doc[y].update(set(toks))

    return all_years, kw_doc, docs_per_year, df_local



all_years, kw_doc, docs_per_year, _ = build_keyword_time(df, STOP | BASE_STOP)


def ensure_topk(pool_tokens, need_k, docs_per_year, kw_doc, years):
    """
    í† í° ì„ ë³„: (1) ê¸°ë³¸ ì»·ì˜¤í”„ ì¶©ì¡± â†’ (2) ìµœê·¼ RECENT_YEARS ì»·ì˜¤í”„ ëŒ€ì²´ â†’ (3) ìµœê·¼ì„±/ë³€ë™ì„± ë­í¬ ë³´ì¶©
    í•­ìƒ need_k ê°œìˆ˜ë¥¼ ë°˜í™˜í•˜ë ¤ê³  ì‹œë„.
    """
    def cnt_years_for(k, yrs):
        cnt = sum(kw_doc[y][k] for y in yrs)
        yrs_hit = sum(kw_doc[y][k] > 0 for y in yrs)
        return cnt, yrs_hit

    # (1) ê¸°ë³¸ ì»·ì˜¤í”„
    base_ok = []
    for k in pool_tokens:
        c, yh = cnt_years_for(k, years)
        if c >= MIN_DOCS_BASE and yh >= MIN_YEARS_BASE:
            base_ok.append(k)

    # (2) ìµœê·¼ ì»·ì˜¤í”„ (ë¶€ì¡±í•˜ë©´ ëŒ€ì²´ í—ˆìš©)
    last = years[-min(RECENT_YEARS, len(years)):]
    recent_ok = []
    for k in pool_tokens:
        c, yh = cnt_years_for(k, last)
        if c >= RECENT_DOCS_MIN and yh >= RECENT_YEARS_MIN:
            recent_ok.append(k)

    # (3) ë­í¬ â€” ìµœê·¼ ì ì¤‘ìˆ˜, ìµœê·¼ ë“±ì¥ì—°ìˆ˜, ë³€ë™ì„±
    recent_hits  = Counter(); recent_years = Counter()
    for y in last:
        recent_hits.update(kw_doc[y])
        for k,c in kw_doc[y].items():
            if c>0: recent_years[k]+=1
    var_proxy = {k: np.var([kw_doc[y][k]>0 for y in years]) for k in set().union(*[kw_doc[y].keys() for y in years])}
    ranked = sorted(set().union(*[kw_doc[y].keys() for y in years]),
                    key=lambda k: (recent_hits[k], recent_years[k], var_proxy.get(k,0.0)),
                    reverse=True)

    # í•©ì¹˜ê¸° + need_k ì±„ìš¸ ë•Œê¹Œì§€ ë³´ì¶©
    out = list(dict.fromkeys(base_ok))
    if len(out) < need_k:
        out = list(dict.fromkeys(out + recent_ok))
    if len(out) < need_k:
        out = list(dict.fromkeys(out + ranked))
    return out[:need_k]


def build_share_lift(tokens, years, kw_doc, docs_per_year):
    share = pd.DataFrame({
        k: jeffreys_rolling_ratio(
            pd.Series({y: kw_doc[y][k] for y in years}, dtype=float),
            docs_per_year.astype(float))
        for k in tokens
    }, index=years)
    w = docs_per_year / docs_per_year.sum()
    base = (share.mul(w, axis=0)).sum(axis=0).replace(0, np.nan)
    lift = share.div(base, axis=1).replace([np.inf,-np.inf], np.nan).fillna(0.0)
    return share, lift


def cagr(series):
    s = np.asarray(series, float)
    s = np.where(s<=0, np.nan, s)
    s = pd.Series(s).dropna()
    if len(s)<2: return 0.0
    n = len(s)-1
    return ((s.iloc[-1]/s.iloc[0])**(1/n) - 1) * 100.0


# ---- Plotly ë¼ì¸ ì°¨íŠ¸ ìƒì„±

def plot_trend_plotly(keys, years_plot, lift_df, title):
    fig = go.Figure()
    for k in keys:
        ys = [lift_df.loc[y, k] for y in years_plot]
        fig.add_trace(go.Scatter(x=years_plot, y=ys, mode="lines+markers", name=k,
                                 line=dict(width=3), marker=dict(size=8), connectgaps=True))
    fig.add_hline(y=1.0, line_width=1.5, line_dash="dash", opacity=0.6)
    fig.update_xaxes(title_text="ì—°ë„")
    fig.update_yaxes(title_text="lift (ë°°)")
    return style_fig(fig, title, legend="right", top_margin=100)

# ---- Plotly: ë¼ì¸ ë ë¼ë²¨(ê²¹ì¹¨ ë°©ì§€) ìœ í‹¸
# --- REPLACE THIS FUNCTION ENTIRELY ---
def add_line_end_labels(fig, years_plot, df, keys,
                        min_gap=0.03, xpad_frac=0.16, right_margin=200):
    """
    years_plot: ì •ë ¬ëœ ì—°ë„ ë¦¬ìŠ¤íŠ¸
    df: [index=years, columns=keys] ë˜ëŠ” [index=keys, columns=years] ëª¨ë‘ ì²˜ë¦¬
    keys: ë¼ë²¨ë§í•  ì‹œë¦¬ì¦ˆ ì´ë¦„ë“¤
    """
    import numpy as _np

    if not keys:
        return fig

    # 1) ë°ì´í„° ë°©í–¥ ìë™ ë³´ì •: keysê°€ ì—´ì— ì—†ìœ¼ë©´ ì „ì¹˜
    df2 = df if (keys[0] in df.columns) else df.T

    # 2) ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” í‚¤ë§Œ ì‚¬ìš©
    keys = [k for k in keys if k in df2.columns]
    if not keys:
        return fig

    # 3) y ë²”ìœ„ ê³„ì‚°
    ymins = [_np.nanmin(df2.loc[years_plot, k].astype(float).values) for k in keys]
    ymaxs = [_np.nanmax(df2.loc[years_plot, k].astype(float).values) for k in keys]
    y_min, y_max = float(min(ymins)), float(max(ymaxs))
    yrng = (y_max - y_min) if y_max > y_min else 1.0

    # 4) ë§ˆì§€ë§‰ y ê°’ ì •ë ¬ â†’ ê°„ê²© ë²Œë ¤ì„œ ê²¹ì¹¨ ë°©ì§€
    y_last = _np.array([df2.loc[years_plot, k].iloc[-1] for k in keys], dtype=float)
    order = _np.argsort(-y_last)
    y_des = y_last[order].copy()
    top_cap, bottom_cap = y_max - 0.02*yrng, y_min + 0.02*yrng
    gap = max(min_gap, 0.6/max(1, len(keys))) * yrng

    y_pos = _np.empty_like(y_des)
    y_pos[0] = float(_np.clip(y_des[0], bottom_cap, top_cap))
    for i in range(1, len(y_des)):
        yi = float(_np.clip(y_des[i], bottom_cap, top_cap))
        if y_pos[i-1] - yi < gap:
            yi = y_pos[i-1] - gap
        if yi < bottom_cap:
            shift = bottom_cap - yi
            y_pos[:i] += shift
            yi = bottom_cap
            if _np.any(y_pos[:i] > top_cap):
                start = top_cap - gap * (i)
                y_pos[:i+1] = _np.linspace(start, top_cap, i+1)
                break
        y_pos[i] = yi

    inv = _np.empty_like(order); inv[order] = _np.arange(len(order))
    y_final = y_pos[inv]

    # 5) xì¶• íŒ¨ë”© + ì˜¤ë¥¸ìª½ ë§ˆì§„ í™•ì¥(ì˜ë¦¼ ë°©ì§€)
    x0, x1 = years_plot[0], years_plot[-1]
    xpad = (x1 - x0) * xpad_frac
    fig.update_xaxes(range=[x0, x1 + xpad])
    # margin.rë§Œ ì¦ì„¤(ê¸°ì¡´ l/t/bëŠ” ìœ ì§€)
    fig.update_layout(margin=dict(r=max(getattr(fig.layout.margin, "r", 0), right_margin)))
    x_label = x1 + xpad*0.55

    # 6) ì—°ê²°ì„  + ì£¼ì„
    for i, k in enumerate(keys):
        yk_end = float(y_last[i]); yf = float(y_final[i])
        fig.add_shape(type="line",
                      x0=x1, y0=yk_end, x1=x1 + xpad*0.45, y1=yf,
                      line=dict(color="rgba(128,128,128,0.85)", width=1))
        fig.add_annotation(x=x_label, y=yf, text=k, showarrow=False,
                           xanchor="left", yanchor="middle",
                           bgcolor="rgba(255,255,255,0.82)",
                           bordercolor="rgba(0,0,0,0)", borderpad=2,
                           font=dict(size=12, color=ui['text']))
    return fig



if all_years:
    # í’€ í›„ë³´
    all_tokens = sorted({k for y in all_years for k in kw_doc[y].keys()})
    need_k = max(TOP_K_PER_FIG*2, 16)
    pool_tokens = ensure_topk(all_tokens, need_k, docs_per_year, kw_doc, all_years)
    share_all, lift_all = build_share_lift(pool_tokens, all_years, kw_doc, docs_per_year)

    win_years = all_years[-min(WINDOW_YEARS, len(all_years)):]
    share_win  = share_all.loc[win_years]
    lift_win   = lift_all.loc[win_years]

    latest_share = share_win.iloc[-1]
    delta_share  = (share_win.iloc[-1] - share_win.iloc[0])  # p.p. ë³€í™”
    last_lift    = lift_win.iloc[-1]
    cagr_lift    = pd.Series({k: cagr(lift_win[k].values) for k in lift_win.columns})

    # ì ìˆ˜(ì •ë ¬ìš©) â€” 2-of-3 ê·œì¹™ê³¼ ì¡°í™”ë˜ë„ë¡ êµ¬ì„±
    rise_score = (last_lift - 1.0) + 0.7*(cagr_lift/100.0) + 0.5*(delta_share/100.0)
    fall_score = (1.0 - last_lift) + 0.7*((-cagr_lift)/100.0) + 0.5*((-delta_share)/100.0)

    # 2-of-3 ê·œì¹™ìœ¼ë¡œ ìƒ/í•˜ë½ í›„ë³´ ë¶„ë¦¬
    sig_up   = ((last_lift >= 1.0).astype(int) + (cagr_lift > 0).astype(int) + (delta_share > 0).astype(int))
    sig_down = ((last_lift < 1.0).astype(int)  + (cagr_lift < 0).astype(int) + (delta_share < 0).astype(int))

    rise_order = [k for k in rise_score.sort_values(ascending=False).index if sig_up[k]   >= 2]
    fall_order = [k for k in fall_score.sort_values(ascending=False).index if sig_down[k] >= 2]

    used=set(); rise_sel=[]; fall_sel=[]
    for k in rise_order:
        if k not in used: rise_sel.append(k); used.add(k)
    for k in fall_order:
        if k not in used: fall_sel.append(k); used.add(k)

    # ë¶€ì¡± ì‹œ ìµœê·¼ì„± ê¸°ì¤€ ë³´ì¶© (ì¤‘ë³µ ê¸ˆì§€)
    def backfill(sel, need, base_rank, predicate):
        if len(sel) >= need: return sel
        recent = win_years[-min(RECENT_YEARS, len(win_years)):]
        hits_recent  = Counter(); years_recent = Counter()
        for y in recent:
            hits_recent.update(kw_doc[y])
            for k,c in kw_doc[y].items():
                if c>0: years_recent[k]+=1
        var_proxy = {k: np.var([kw_doc[y][k]>0 for y in win_years]) for k in base_rank}
        rank = sorted(base_rank, key=lambda k: (hits_recent[k], years_recent[k], var_proxy.get(k,0.0)), reverse=True)
        for k in rank:
            if len(sel) >= need: break
            if k in used: continue
            if predicate(k): sel.append(k); used.add(k)
        if len(sel) < need:
            for k in base_rank:
                if len(sel) >= need: break
                if k in used: continue
                sel.append(k); used.add(k)
        return sel

    rise_sel = backfill(rise_sel, TOP_K_PER_FIG, list(rise_score.sort_values(ascending=False).index),
                        lambda k: (last_lift[k] >= 1.0) or (cagr_lift[k] > 0) or (delta_share[k] > 0))
    fall_sel = backfill(fall_sel, TOP_K_PER_FIG, list(fall_score.sort_values(ascending=False).index),
                        lambda k: (last_lift[k] < 1.0) or (cagr_lift[k] < 0) or (delta_share[k] < 0))

    rise_sel = rise_sel[:TOP_K_PER_FIG]
    fall_sel = fall_sel[:TOP_K_PER_FIG]

    # === ì—¬ê¸°ê¹Œì§€ rise_sel, fall_sel ëª©ë¡ì´ ë§Œë“¤ì–´ì§„ ìƒíƒœ ===

    # â‘  ì›í•˜ëŠ” ëª©í‘œ ê°œìˆ˜(ì–‘ìª½ ë™ì¼) â€” TOP_K_PER_FIG ì‚¬ìš©
    TARGET = min(TOP_K_PER_FIG, len(rise_sel), len(fall_sel))
    
    # â‘¡ ë¶€ì¡±í•˜ë©´ ë­í‚¹ìœ¼ë¡œ ë³´ì¶©
    #    (ìƒìŠ¹/í•˜ë½ ê°ê°ì˜ ì •ë ¬ ê¸°ì¤€ ì´ë¯¸ ê³„ì‚°ë˜ì–´ ìˆë‹¤ê³  ê°€ì •: rise_score, fall_score)
    rise_rank = list(rise_score.sort_values(ascending=False).index)
    fall_rank = list(fall_score.sort_values(ascending=False).index)
    used = set(rise_sel) | set(fall_sel)
    
    def fill_to(target, base_list, rank_pool):
        out = list(base_list)
        for k in rank_pool:
            if len(out) >= target: break
            if k in used: continue
            out.append(k); used.add(k)
        return out[:target]
    
    rise_sel = fill_to(TARGET, rise_sel, rise_rank)
    fall_sel = fill_to(TARGET, fall_sel, fall_rank)
    
    # â‘¢ í˜¹ì‹œ ë‘˜ ë‹¤ ë„ˆë¬´ ì ì„ ë•Œ(ë°ì´í„° í¬ë°•), ë‘ìª½ì˜ ì‹¤ì œ ê°€ëŠ¥í•œ ìµœì†Œì¹˜ë¡œ ì¬ì¡°ì •
    TARGET = min(len(rise_sel), len(fall_sel))
    rise_sel = rise_sel[:TARGET]
    fall_sel = fall_sel[:TARGET]


    years_plot = win_years[-min(RECENT_YEARS*2, len(win_years)):]  # ìµœê·¼ 10ë…„ ë‚´ì—ì„œ 10~?ë…„ ìŠ¬ë¼ì´ìŠ¤

    fig_up   = plot_trend_plotly(rise_sel, years_plot, lift_all, f"ìƒìŠ¹ì„¸ â€” ìµœê·¼ {len(years_plot)}ë…„")
    fig_up   = style_fig(fig_up, bg_color=VIZ_BG["trend_up"], bg_alpha=0.5)
    fig_up   = add_line_end_labels(fig_up, years_plot, lift_all, rise_sel)
    fig_down = plot_trend_plotly(fall_sel, years_plot, lift_all, f"í•˜ë½ì„¸ â€” ìµœê·¼ {len(years_plot)}ë…„")
    fig_down = style_fig(fig_down, bg_color=VIZ_BG["trend_down"], bg_alpha=0.5)
    fig_down = add_line_end_labels(fig_down, years_plot, lift_all, fall_sel)

    u, v = st.columns([1,1], gap="large")
    # íŠ¸ë Œë“œ ì°¨íŠ¸ëŠ” ë²”ë¡€ë¥¼ ìƒë‹¨ìœ¼ë¡œ ì´ë™ (ë¸”ë¡ ë‚´ë¶€ì— ìœ ì§€)
    with u:
        fig_up.update_layout(legend=dict(orientation="h", y=1.10, yanchor="bottom", x=0, xanchor="left"))
        fig_up = add_line_end_labels(fig_up, years_plot, lift_all, rise_sel)
        fig_up = force_legend_top_padding(fig_up, base_top=130)  # â˜… ì¶”ê°€(ë³´ìˆ˜ì )
        st.plotly_chart(fig_up, use_container_width=True, config={"displayModeBar": False})
    with v:
        fig_down.update_layout(legend=dict(orientation="h", y=1.10, yanchor="bottom", x=0, xanchor="left"))
        fig_down = add_line_end_labels(fig_down, years_plot, lift_all, fall_sel)
        fig_down = force_legend_top_padding(fig_down, base_top=130)  # â˜… ì¶”ê°€
        st.plotly_chart(fig_down, use_container_width=True, config={"displayModeBar": False})
else:
    st.info("ì‚¬ì—… ê¸°ê°„ì—ì„œ ì—°ë„ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ í‚¤ì›Œë“œ ìƒëŒ€ íŠ¸ë Œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

# ===================== í‚¤ì›Œë“œ íŠ¸ë Œë“œ â€” ì™„ì „ ìˆ˜ë™(ê²€ìƒ‰ ì—†ìŒ) =====================
st.markdown("---")
st.subheader("í‚¤ì›Œë“œ íŠ¸ë Œë“œ â€” ì§ì ‘ ì„ íƒ (ê²€ìƒ‰ ì—†ìŒ Â· êµ­ê°€/AI í‚¤ì›Œë“œ ì œì™¸)")
# ì‚¬ì´ë“œë°” ì–´ë”˜ê°€ì—:
if st.sidebar.button("ìºì‹œ ì´ˆê¸°í™”", use_container_width=True):
    # ìºì‹œ ë¹„ìš°ê¸°
    st.cache_data.clear()
    st.cache_resource.clear()
    # ë¦¬ëŸ° (ë²„ì „ í˜¸í™˜)
    try:
        st.rerun()                 # Streamlit >= 1.27+
    except Exception:
        try:
            st.experimental_rerun()  # êµ¬ë²„ì „ ë°±ì—…
        except Exception:
            pass  # ìµœí›„ì˜ ë³´ë£¨: ë¦¬ëŸ° ì‹¤íŒ¨í•´ë„ ì•±ì€ ê³„ì† ë™ì‘
            
with st.sidebar.expander("í™˜ê²½ ì ê²€", expanded=False):
    import sys, importlib.util
    st.write("Python:", sys.version)
    for m in ["streamlit_folium", "folium", "wordcloud", "plotly", "kiwipiepy", "sentence_transformers", "keybert"]:
        st.write(f"{m}: ", importlib.util.find_spec(m) is not None)



# ====================== ì‚¬ìš©ì ë¶ˆìš©ì–´ (ì½”ë“œì—ì„œ ì§ì ‘ í¸ì§‘) ======================
# ====================== ì‚¬ìš©ì ë¶ˆìš©ì–´ ======================
# ==== 0) ì‚¬ìš©ì ë¶ˆìš©ì–´ (ì—¬ê¸°ë§Œ ìˆ˜ì •) ====
STOP_CUSTOM = {"ë†’ì—¬", "ê¸°ê´€ë³„", "ì§€ì†ê°€ëŠ¥í•œ", "ê³µë¬´ì›ì˜", "ìˆìŒ", "ì‚¬ìš©ì", "ê²½ì œ", "ì¤‘ì†Œê¸°ì—…ì˜", "ì¡°ë‹¬", "ê³µê³µ", "ê°œí˜ì„", "ê¸°ì—…ë“¤ì˜", "ë¼ì˜¤ìŠ¤ì˜", "ë¶„ì„",
               "ì „ëµì„", "ì œë„ì˜", "ì²´ê³„ì ìœ¼ë¡œ", "í†µê³„", "í‘œì¤€í™”", "ê²ƒì´ë‹¤", "ë² íŠ¸ë‚¨ì˜", "ê³¼í…Œë§ë¼ì˜", "ë©”ì½©ê°•", "ì‹œìŠ¤í…œì„", "ì´ì§‘íŠ¸ì˜", "í†µí•©", "í•„ë¦¬í•€ì˜", 
               "ì‚°ì—…", "í˜ì‹ ", "ê°€ë‚˜ì˜", "ì „í™˜", "ì§‘í–‰", "íŒŒë¼ê³¼ì´ì˜", "ê²€ìƒ‰", "ê·œì œ", "ê¸°ìˆ ", "ìƒíƒœê³„ë¥¼", "ì²˜ë¦¬", "í˜‘ë ¥", "ë“±ë¡", "ë‚©ì„¸ì"}
STOP_SET = {w.strip().upper() for w in STOP_CUSTOM if w.strip()}

# ==== 1) í—¬í¼ ====
import re
from collections import Counter

def _norm_token(x: str) -> str:
    x = re.sub(r'[\"\'â€™â€œâ€()\[\]{}<>]', "", str(x).strip())
    return x.upper()  # ëŒ€ë¬¸ì ê¸°ì¤€ìœ¼ë¡œ í†µì¼

def _is_numericish(s: str) -> bool:
    return bool(re.fullmatch(r"\d+(\.\d+)?", s))

# ==== 2) í•´ì‹œíƒœê·¸ ë¹ˆë„ ìˆ˜ì§‘ ====
HASHTAG_COL = "Hashtag" if "Hashtag" in df.columns else ("Hashtag_str" if "Hashtag_str" in df.columns else None)

def collect_hashtag_freq(df_in) -> Counter:
    freq = Counter()
    if not HASHTAG_COL or HASHTAG_COL not in df_in.columns or df_in[HASHTAG_COL].isna().all():
        return freq
    for raw in df_in[HASHTAG_COL].dropna().astype(str):
        for t in re.split(r"[,\;/]| {2,}", raw):
            t = _norm_token(t)
            if not t or len(t) < 2:
                continue
            if t in STOP_SET or _is_numericish(t):
                continue
            freq[t] += 1
    return freq

# ==== 3) ì œì™¸ì„¸íŠ¸ (êµ­ê°€/AI í‚¤ì›Œë“œ) ====
COUNTRY_WORDS = set()
if "COUNTRY_MAP" in globals():
    for k, (iso, en, ko) in COUNTRY_MAP.items():
        COUNTRY_WORDS |= {str(k).upper(), str(iso).upper(), str(en).upper(), str(ko).upper()}

AI_SET = set()
if "rise_sel" in globals() and rise_sel is not None:
    AI_SET |= {str(t).upper() for t in rise_sel}
if "fall_sel" in globals() and fall_sel is not None:
    AI_SET |= {str(t).upper() for t in fall_sel}

def is_excluded(tok: str) -> bool:
    t = tok.upper().strip()
    return (t in COUNTRY_WORDS) or (t in AI_SET) or (t in STOP_SET) or _is_numericish(t)

# ==== 4) ì—¬ê¸°ì„œ freq_allì„ 'ë¨¼ì €' ë§Œë“  ë‹¤ìŒ í›„ë³´ ìƒì„± ====
freq_all = collect_hashtag_freq(df)          # â† ë°˜ë“œì‹œ ë¨¼ì €!
candidates_all = [(k, c) for k, c in freq_all.items() if not is_excluded(k)]

# í›„ë³´ê°€ ë„ˆë¬´ ì ìœ¼ë©´(ì˜ˆ: ë¶ˆìš©ì–´ê°€ ë§ì„ ë•Œ) ì™„í™”
if len(candidates_all) < 25 and HASHTAG_COL:
    tmp = Counter()
    for raw in df[HASHTAG_COL].dropna().astype(str):
        for t in re.split(r"[,\;/]| {2,}", raw):
            t = _norm_token(t)
            if t and (t not in COUNTRY_WORDS) and (t not in AI_SET) and (t not in STOP_SET) and (not _is_numericish(t)):
                tmp[t] += 1
    for k, v in tmp.items():
        freq_all[k] = max(freq_all.get(k, 0), v)
    candidates_all = [(k, c) for k, c in freq_all.items()
                      if (k not in COUNTRY_WORDS) and (k not in AI_SET) and (k not in STOP_SET) and (not _is_numericish(k))]

# ì •ë ¬/ë¼ë²¨
candidates_all = sorted(candidates_all, key=lambda x: (-x[1], x[0]))[:300]
cand_labels = [k for k, _ in candidates_all]


# --- 3) ì²´í¬ë°•ìŠ¤ ê·¸ë¦¬ë“œ UI(ê²€ìƒ‰ ì—†ìŒ) ---
def checkbox_multi(label: str, options: list[str], max_select: int = 30, cols: int = 4) -> list[str]:
    st.caption("ì›í•˜ëŠ” í‚¤ì›Œë“œë¥¼ ì²´í¬í•˜ì„¸ìš”. (2â€“30ê°œ)")
    picks = []
    grids = [options[i::cols] for i in range(cols)]
    cols_obj = st.columns(cols, gap="large")
    for col, opts in zip(cols_obj, grids):
        with col:
            for o in opts:
                key = f"kw_pick_{label}_{o}"
                if st.checkbox(o, key=key, value=False):
                    picks.append(o)
    if len(picks) > max_select:
        st.warning(f"{max_select}ê°œê¹Œì§€ë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ìš”. í˜„ì¬ {len(picks)}ê°œ ì„ íƒë¨ â†’ ì•ì—ì„œ {max_select}ê°œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        picks = picks[:max_select]
    return picks

chosen = checkbox_multi("kw", cand_labels, max_select=30, cols=4)

if len(chosen) < 2:
    st.info("í‚¤ì›Œë“œë¥¼ ìµœì†Œ 2ê°œ ì´ìƒ ì„ íƒí•˜ë©´ ì•„ë˜ì— ì¶”ì„¸ ê·¸ë˜í”„ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
    st.stop()

# --- 4) ì—°ë„ ì§‘ê³„(!!! ì—¬ê¸° í•µì‹¬: 'ì‚¬ì—… ê¸°ê°„' ì§ì ‘ ì°¸ì¡° ê¸ˆì§€) ---
years_series = _year_text_series(df)                 # <-- ì´ê±¸ë¡œ ì—°ë„ ì†ŒìŠ¤ ìë™/ì§€ì •
years_list   = years_series.apply(years_from_span)
all_years    = sorted({y for ys in years_list for y in (ys or [])})
if not all_years:
    st.warning("ì—°ë„ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ì„œ ì¶”ì„¸ë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ì–´ìš”.")
    st.stop()

docs_per_year = pd.Series(0, index=all_years, dtype=int)
for ys in years_list:
    for y in (ys or []): docs_per_year[y] += 1

# í‚¤ì›Œë“œ ë“±ì¥ìˆ˜(ì—°ë„ë³„, 'ì„ íƒëœ ê²ƒ'ë§Œ ê³„ì‚°)
kw_doc = {y: Counter() for y in all_years}
if HASHTAG_COL:
    for (_, row), ys in zip(df.iterrows(), years_list):
        if not ys: 
            continue
        toks = []
        for t in re.split(r"[,\;/]| {2,}", str(row.get(HASHTAG_COL, ""))):
            t = _norm_token(t.strip())
            if t and t in chosen:
                toks.append(t)
        if not toks:
            continue
        for y in ys:
            kw_doc[y].update(set(toks))

# share & lift
share = pd.DataFrame({
    k: jeffreys_rolling_ratio(
        pd.Series({y: kw_doc[y][k] for y in all_years}, dtype=float),
        docs_per_year.astype(float))
    for k in chosen
}, index=all_years).fillna(0.0)

w = docs_per_year / docs_per_year.sum()
base = (share.mul(w, axis=0)).sum(axis=0).replace(0, np.nan)
lift = share.div(base, axis=1).replace([np.inf, -np.inf], np.nan).fillna(0.0)

# ìµœê·¼ 10ë…„ë§Œ
years_plot = all_years[-min(10, len(all_years)):]
fig = go.Figure()
for k in chosen:
    ys = [float(lift.loc[y, k]) if (y in lift.index and k in lift.columns) else np.nan for y in years_plot]
    if np.all(np.isnan(ys)):   # ì™„ì „ ë¯¸ë“±ì¥ ë°©ì§€
        continue
    fig.add_trace(go.Scatter(x=years_plot, y=ys, mode="lines+markers", name=k,
                             line=dict(width=3), marker=dict(size=8), connectgaps=True))
fig.add_hline(y=1.0, line_width=1.5, line_dash="dash", opacity=0.6)
fig.update_xaxes(title_text="ì—°ë„")
fig.update_yaxes(title_text="lift (ë°°)")
fig = style_fig(fig, "ì„ íƒ í‚¤ì›Œë“œ ì¶”ì„¸ â€” ìµœê·¼ 10ë…„", legend="top", top_margin=130,
                bg_color=VIZ_BG["trend_up"], bg_alpha=0.35)
fig = add_line_end_labels(fig, years_plot, lift, [k for k in chosen if k in lift.columns])
fig = force_legend_top_padding(fig, base_top=130)
st.plotly_chart(fig, use_container_width=True, config={"displayModeBar": False})




# --------------------- ì„¤ì¹˜ / ì‹¤í–‰ ---------------------
with st.expander("ì„¤ì¹˜ / ì‹¤í–‰"):
    st.code("pip install streamlit folium streamlit-folium pandas wordcloud plotly matplotlib", language="bash")
    st.code("streamlit run S_KSP_clickpro_v4_plotly_patch_FIXED.py", language="bash")






















































































































































































































































































