# S_KSP_clickpro_v4_plotly_patch_FIXED.py
# ===============================================
# KSP Explorer â€” Leaflet + Plotly (Pro v4 â€¢ Plotly patch, FIXED)
# - ì§€ë„: â‘  êµ­ê°€ë³„ ì´ê³„(í´ë¦­) â‘¡ ICT ìœ í˜• ë‹¨ì¼í´ë˜ìŠ¤(í´ë¦­)
# - ìƒì„¸: ì›Œë“œí´ë¼ìš°ë“œ(í•­ìƒ: í•´ì‹œíƒœê·¸+ìš”ì•½/ë‚´ìš©) + ìƒìœ„ í‚¤ì›Œë“œ ê°€ë¡œë§‰ëŒ€(ë¼ë²¨ ì˜ë¦¼ ë°©ì§€)
# - ì „ì—­ ëŒ€ì‹œë³´ë“œ: ë„ë„› 2ê°œ + ì£¼ì œÃ—WB 100% ëˆ„ì  ë§‰ëŒ€
# - ì—°ë„ ì‹œê°í™”: ìˆœìœ„ Bump / 100% ëˆ„ì  ë§‰ëŒ€ (í† ê¸€)
# - ì¶”ê°€ ì‹œê°í™”: ëŒ€í‘œ í‚¤ì›Œë“œ ìƒëŒ€ íŠ¸ë Œë“œ(ìƒ/í•˜, Plotly) + ëŒ€í‘œ 'ì£¼ì œ(í‚¤ì›Œë“œ)' ìƒëŒ€ íŠ¸ë Œë“œ(ìƒ/í•˜, Plotly)
# - ë¶ˆí•„ìš”í•œ ìŠ¬ë¼ì´ë”/ì˜µì…˜ ì œê±°: ì›Œë“œí´ë¼ìš°ë“œ ì†ŒìŠ¤ ê³ ì •, Top-K ì¡°ì ˆ/Jeffreys+ë¡¤ë§ ìœˆë„ ì¡°ì ˆ ì œê±°
# - FIX: with/else ë“¤ì—¬ì“°ê¸° ì •ë¦¬, ë¸”ë¡ ì‚¬ì´ì— ì½”ë“œ ì‚½ì…ìœ¼ë¡œ ì¸í•œ SyntaxError í•´ê²°
# ===============================================
import os, io, re, json, urllib.request, hashlib, pathlib, copy
from typing import List, Dict, Tuple
from collections import Counter, defaultdict, OrderedDict
import urllib.request
from pathlib import Path
import numpy as np
import pandas as pd
from PIL import Image
from PIL import ImageFont
import streamlit as st
import folium
from streamlit_folium import st_folium
from wordcloud import WordCloud
import plotly.express as px
import plotly.graph_objects as go
from matplotlib import font_manager, rcParams

#######################################################
# --------------------- Changyeon ---------------------
# import pdfplumber
# import zipfile
# import streamlit.components.v1 as components
# import json
# from sklearn.feature_extraction.text import TfidfVectorizer
# import tempfile
#######################################################

# --------------------- í˜ì´ì§€/í…Œë§ˆ ---------------------
st.set_page_config(page_title="KSP Explorer (Pro v4)", layout="wide", page_icon="ğŸŒ")

@st.cache_resource
def resolve_korean_font() -> str | None:
    # 1) ë¦¬í¬ì— ë™ë´‰ëœ í°íŠ¸ ìš°ì„ 
    candidates = [
        Path(__file__).parent / "assets/fonts/NanumGothic.ttf",
        Path(__file__).parent / "assets/fonts/NotoSansKR-Regular.otf",
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf",
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
        r"C:\Windows\Fonts\malgun.ttf",
        "/System/Library/Fonts/AppleGothic.ttf",
    ]
    for p in candidates:
        p = str(p)
        if os.path.exists(p):
            try:
                ImageFont.truetype(p, 20)
                return p
            except Exception:
                pass

    # 2) ìµœí›„: ì‹œìŠ¤í…œ í°íŠ¸ ë””ë ‰í† ë¦¬ ì „ì²´ ìŠ¤ìº”(ìºì‹œë¨)
    roots = ["/usr/share/fonts", "/Library/Fonts", "/System/Library/Fonts", r"C:\Windows\Fonts"]
    keys  = ["nanum","noto","apple","malgun","gulim","batang","sourcehan","cjk"]
    for root in roots:
        if not os.path.isdir(root):
            continue
        for dirpath, _, filenames in os.walk(root):
            for fn in filenames:
                if fn.lower().endswith((".ttf",".otf",".ttc")) and any(k in fn.lower() for k in keys):
                    cand = os.path.join(dirpath, fn)
                    try:
                        ImageFont.truetype(cand, 20)
                        return cand
                    except Exception:
                        continue
    return None


WC_FONT_PATH = resolve_korean_font()
#GLOBAL_FONT_FAMILY = "Noto Sans KR, NanumGothic, Malgun Gothic, AppleGothic, Arial Unicode MS, sans-serif"

st.sidebar.header("í™˜ê²½ ì„¤ì •")
theme_name = st.sidebar.selectbox(
    "í…Œë§ˆ", ["Nord", "Emerald", "Sandstone", "Slate"], index=0
)

THEME_PRESETS = {
    "Obsidian":  {"bg":"#0f1115","text":"#e6e8eb","panel":"#151923","card":"#0f141c","border":"#202634","accent":"#4f9cf0","plotly_template":"plotly_dark"},
    "Midnight":  {"bg":"#0b1220","text":"#e8eefc","panel":"#101a2c","card":"#0d1526","border":"#1c2a45","accent":"#8ac6ff","plotly_template":"plotly_dark"},
    "Nord":      {"bg":"#ECEFF4","text":"#2E3440","panel":"#E5E9F0","card":"#FFFFFF","border":"#D8DEE9","accent":"#5E81AC","plotly_template":"plotly_white"},
    "Emerald":   {"bg":"#f3fbf7","text":"#123026","panel":"#e8f6ee","card":"#ffffff","border":"#cfe9dc","accent":"#2bb673","plotly_template":"plotly_white"},
    "Sandstone": {"bg":"#faf7f2","text":"#2c251b","panel":"#f1ece3","card":"#ffffff","border":"#e3d9c6","accent":"#d49a6a","plotly_template":"plotly_white"},
    "Slate":     {"bg":"#f6f7fb","text":"#111827","panel":"#eef1f6","card":"#ffffff","border":"#e5e7eb","accent":"#3b82f6","plotly_template":"plotly_white"},
}
ui = THEME_PRESETS[theme_name]



st.markdown(f"""
<style>
:root {{
  --bg:{ui['bg']}; --text:{ui['text']};
  --panel:{ui['panel']}; --card:{ui['card']}; --border:{ui['border']}; --accent:{ui['accent']};
}}
html, body, .block-container {{ background:var(--bg) !important; color:var(--text) !important; }}
section[data-testid="stSidebar"] {{ background:var(--panel) !important; }}
div[data-testid="stHeader"] {{ background:var(--bg) !important; }}
.stMarkdown, p, h1,h2,h3,h4,h5,h6 {{ color:var(--text) !important; }}
.ksp-card {{ background:var(--card); border:1px solid var(--border); border-radius:14px; padding:14px; }}
.ksp-chip {{ display:inline-block; background:var(--panel); border:1px solid var(--border);
            border-radius:14px; padding:4px 10px; margin:4px; }}
a {{ color: var(--accent); }}
</style>
""", unsafe_allow_html=True)

# ---- Force Korean-capable fonts in the browser (Plotly/HTML) ----
#st.markdown(f"""
#<style>
#* {{ font-family: {GLOBAL_FONT_FAMILY} !important; }}
#</style>
#""", unsafe_allow_html=True)

# ---- Plotly font stack (safe getter) ----
FONT_STACK_DEFAULT = "Noto Sans KR, NanumGothic, Malgun Gothic, AppleGothic, Arial Unicode MS, Arial, sans-serif"

def _plotly_font_family():
    # ì„¸ì…˜/ê¸€ë¡œë²Œ/ë””í´íŠ¸ ìˆœìœ¼ë¡œ ê°€ì ¸ì˜´
    return (
        st.session_state.get("plotly_font_family")
        or globals().get("GLOBAL_FONT_FAMILY")
        or FONT_STACK_DEFAULT
    )



st.title("KSP Explorer ğŸŒ â€” Pro v4")



# --------------------- ë¶ˆìš©ì–´ ---------------------
STOP = {
    "ë°","ë“±","ê´€ë ¨","ìˆ˜ë¦½","ë°©ì•ˆ","ê°œì„ ","ì „ëµ","ì§€ì›","ì •ì±…","ì‚¬ì—…","í”„ë¡œì íŠ¸","ì œë„", "í•œêµ­ê³¼", "ë©•ì‹œì½”ì˜", "í–¥ìƒì„", "í”„ë¡œì íŠ¸ëŠ”", "í˜„ëŒ€í™”ë¥¼", "í—¬í”„ë°ìŠ¤í¬ì™€", "ê¸°ëŠ¥", "ìƒìŠ¹", "íƒ‘ì¬", "ê³¼ì œ", "100ìœ„ì—ì„œ", "ëª¨ë‹ˆí„°ë§ì„", "ê³µìœ ", "ì§€ì ", "ë†’ì€", "28ì¤‘", "ë¯¸í¡", "9ëŒ€í•­ëª©",
    "êµ¬ì¶•","ë„ì…","ê°œìš”","í˜„í™©","ìœ„í•œ","í™œìš©","ë¶„ì„","ì œê³µ","ê°œë°œ","ê¸°ë°˜","ë””ì§€í„¸","data", "ê·¼ê±°", "ê²½í—˜", "67ì†Œ", "9ëŒ€", "3ê·¸ë£¹", "ì œë„ì ", "ë³€í™”", "ê¸°ê´€", "ì¡°ì‚¬", "ë¶€ë¬¸", "í™•ëŒ€", "ê¸°ì—…", "í˜ì‹ ì„", "í™œìš©í•œ", "ë“±ì´ë‹¤", "íš¨ê³¼ëŠ”", "ê²½í—˜ì„", "ë°©ì•ˆì„",
    "ë°ì´í„°","system","ì •ë¶€","ksp","koica","kdi","idb","ebrd","wb","adb","êµ­ê°€","í•œêµ­", "ì œë„ì™€", "ì‹œìŠ¤í…œì„", "ê²€ìƒ‰", "ì „ë¬¸ê°€", "ì—…ì²´", "ì‚¬ì—…ì€", "ì œë„ê°œì„ ì„", "ë¡œë“œë§µê³¼", "ì„¤ë¬¸ì„", "ë””ì§€í„¸ì •ë¶€ì˜", "ì²´ê³„ë¥¼", "ìˆœìœ„", "ìˆœìœ„ê°€", "80ìœ„ë¡œ", "í•„ìš”ì„±", "í‘œì¤€", "ê¸°ê´€ì˜", "28ì¤‘í•­ëª©",
    "ì—°êµ¬","ë³´ê³ ","ìµœì¢…","ì¤‘ê°„","ì„±ê³¼","í–¥ìƒ","ì œê³ ","ë„ì›€","ì°¨ì„¸ëŒ€","ë¡œë“œë§µ","ìš´ì˜","ì„œë¹„ìŠ¤", "ë°”íƒ•ìœ¼ë¡œ", "ë§ë ˆì´ì‹œì•„ì˜", "ì§„ë‹¨í•˜ê³ ", "ì •ë¶€ì™€", "KSPëŠ”", "ì§€ì›í–ˆë‹¤", "í•œêµ­ì˜", "ì‹œìŠ¤í…œ", "íšŒì˜", "ë…¼ì˜", "ì°¸ì—¬", "ì‹œìŠ¤í…œê³¼", "KSPì—ì„œ", "ì°¸ê³ í•˜ì—¬", "êµ¬ì¶•ì„", "ë°©ë¬¸", "í’€í…ìŠ¤íŠ¸",
    "ë¼ì˜¤ìŠ¤","2030","ntca","to be","í—ê°€ë¦¬","be","ì‚¬ë¡€","ëª¨ë¸","ì‚°ì—…", "ë¹„êµ", "ê°•ì¡°", "ìµœì¢…ë³´ê³ ", "ìœ„í•´", "ì—°ìˆ˜", "ë¹„ì „", "ê°œìµœ", "í˜‘ì˜", "ì œ2", "êµ¬ì¶•", "ê¶Œê³ ", "ë¬¸ì œ", "ì˜¨ë‚˜ë¼", "ì¤‘ì•™", "ë„ì…ê³¼", "í•˜ë¶€", "ì„±ì¥", "ë“±ì„", 'í’ˆì§ˆ', "ì—°êµ¬ê°œë°œê³¼", "ê±°ë²„ë„ŒìŠ¤ë¥¼", "ì„¤ë¦½",
    "ê²©ì°¨","í•´ì†Œ","ì—­ëŸ‰","ê°•í™”","ì‹¤í–‰ê³„íš","ì—°ê¸ˆ","vision","ì‹¤ìš©ì‹ ì•ˆ", "í‰ê°€", "ì œì‹œ", "í†µí•´", "ì„¤ì¹˜", "ì œì‹œí–ˆë‹¤", "ê¶Œê³ í•˜ë©°", "ë¶„ì„í•˜ê³ ", "ì‹œìŠ¤í…œì˜", "ê°œì„ ì•ˆì„", "í–ˆë‹¤", "í†µí•´", "ëª©í‘œë¡œ", "í–¥ìƒê³¼", "ì œì•ˆ", "ê´€ë¦¬", "í†µí•©", "í˜‘ë ¥", "ì œì•ˆí•˜ì˜€ë‹¤", "ì²´ê³„", "ë¹„êµí•˜ì—¬", "ì„¤ëª…ê°€ëŠ¥",
    "ê°€ì§€", "ì¥ê¸°", "íˆ¬ëª…ì„±ê³¼", "ê¸°ëŒ€í–ˆë‹¤", "ì§„í–‰ë˜ì—ˆìœ¼ë©°", "ì „í™˜ê³¼", "ë§ˆë ¨í–ˆë‹¤", "ëª¨ë¸ë¡œ", "í˜‘ì˜ì™€", "ë¹„êµì™€", "ê°œë°œì„", "ì´í›„", "ë§ˆë ¨", "ë¶€ì¡±ì„", "ìˆ˜ë¦½ì„", "ì‚¬ë¡€ë¥¼", "ì •ë¶€ì™€", "í”„ë ˆì„ì›Œí¬", "í†µí•œ", "ê¸°ë°˜ìœ¼ë¡œ", "ì¹´í•˜", "ë°œì „", "ì œì•ˆí–ˆë‹¤", "ì œì‹œí•˜ê³ ", "ì •ì±…ì„", "í™”ìƒíšŒì˜ë¥¼",
    "ì ìš©", "í¬í„¸", "ìˆ˜ë¦½í•˜ì˜€ë‹¤", "ê²ƒì´ë‹¤", "ë„ì…ì„", "ì •ë¶€ì˜", "ì„±ìˆ™ë„", "ê³¼ì •", "ê³„íšì„", "í¬ìˆ˜", "ë”°ë¥¸", "ê³„íšê³¼", "TV", "ìƒˆë¡œìš´", "ì¤‘ì†Œê¸°ì—…ì˜", "ìë£Œ", "ì¶”ì •", "ì•ˆì •", "ì„ ì •", "ê²½ìŸë ¥", "ì „ëµì„", "í˜„í™©ì„", "ê°œì„ ì„", "í™œì„±í™”ë¥¼", "To", "ê²€í† ", "ì‹¤í–‰ê³„íšê³¼", "ë‹¨ê³„ë³„",
    "ê°•í™”ì™€", "í˜„í–‰", "ìƒíƒœê³„ë¥¼", "í˜„í™©ê³¼", "ê°€ì¹˜í‰ê°€", "ì „í™˜ì—", "í™œë™", "êµ­ë¯¼", "í¬í•¨", "ê¸°ë³¸ê³„íš", "ì ‘ê·¼ì„±", "ì „í™˜", "í• ë‹¹", "ì–‘êµ­", "íš¨ê³¼", "ì¶”ì§„", "í˜‘ë ¥ê³¼", "ì´ìš©ê³„íš", "ë‹´ì€", "ë¡œë“œë§µì„", "ì œì‹œí•˜ì˜€ë‹¤", "WASH", "ê¸°ìˆ ", "ê¸°ëŒ€íš¨ê³¼", "ê°€ë‚˜ì˜", "ì†”ë£¨ì…˜ì„", "ìˆ˜í–‰í•˜ì˜€ë‹¤", "ë¶„ì•¼",
    "67ì†Œí•­ëª©ìœ¼ë¡œ", "ë¶„ì•¼", "ì§„ë‹¨", "ì¤‘ë³µì¡°ì‚¬ì™€", "ì‚¬ìš©", "í•­ëª©", "í¬ìš©ì ", "ê·œì œ", "ê¸°ì—…ë“¤ì˜", "í˜ì‹ ", "ë¼ì˜¤ìŠ¤ì˜", "ë©”ì½©ê°•", "ë² íŠ¸ë‚¨ì˜", "í†µê³„", "ì²˜ë¦¬", "ì œë„ì˜", "ì œì•ˆí•©ë‹ˆë‹¤", "ìœ ì—­ì˜", "ì´ë¥¼", "ê°•í™”ë¥¼", "ìˆìŠµë‹ˆë‹¤", "íê¸°ë¬¼", "í•„ë¦¬í•€ì˜", "tuneps", "í‘œì¤€í™”", "ì¬ì •ê°ì‚¬ê°ë…ì²­", "ì´ì§‘íŠ¸ì˜",
    "ê°œí˜ì„", "ì²´ê³„ì ìœ¼ë¡œ", "dur", "íŒŒë¼ê³¼ì´ì˜", "ê³¼í…Œë§ë¼ì˜", "ì œì•ˆí•œë‹¤", "ë³´ê³ ì„œëŠ”", "ê°•í™”í•˜ê³ ", "êµ­ê°€ë“¤ì˜", "íšŒì›êµ­", "ë„ì¶œí–ˆìŠµë‹ˆë‹¤", "ë¶„ì„í•˜ì—¬", "íŒ€ì€", "ì œì•ˆí•œë‹¤", "ê²ƒì„", "ì œì‹œí•œë‹¤", "êµ­ê°€ë“¤ì˜", "ìš°ì¦ˆë² í‚¤ìŠ¤íƒ„ì˜", "ê²€í† í•˜ì˜€ìŠµë‹ˆë‹¤", "ë‚®ì€", "ê·¸ë£¹", "ê²½ìŸë ¥ì„", "ê°•ì¡°í•œë‹¤", "ì¤‘ìš”ì„±ì„",
    "í•µì‹¬", "ìˆ˜ë™", "ì§€ì—°", "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ë³´ê³ ì„œì…ë‹ˆë‹¤", "ê²ªê³ ", "ì¸í•´", "í˜„ì¬", "ë‹¤ë‹ˆëŠ”", "ë‹¤ë£¬ë‹¤", "ì¤‘ì‹¬ìœ¼ë¡œ", "ê°€ëŠ¥í•œ", "í•œë‹¤", "ìœ„ì¹˜", "ë¶€ë¬¸ì˜", "ê°€ì¥", "ì˜¨ë‘ë¼ìŠ¤ì˜", "ìš´ì˜ì„", "ì„¼í„°", "íŠ¹íˆ", "ì°¸ì—¬ë¥¼", "ë“±ë¡", "ì´ˆì ì„", "ì§€ì›ì„", "ì œì‹œí–ˆìŠµë‹ˆë‹¤", "í–‰ì •ì˜",
    "ì ‘ê·¼ì„±ì„", "ë°œìƒí•˜ëŠ”", "ìˆ˜ë¦½í•©ë‹ˆë‹¤", "ì œì‹œí•©ë‹ˆë‹¤", "ì„±ê³µì ì¸", "íš¨ìœ¨ì ì¸", "ì „í™˜ì„", "ëŒ€ì‘", "ìë¬¸ì„", "í•©ë‹ˆë‹¤", "ê¸°ìˆ ì„", "ì„œë¹„ìŠ¤ì—", "ë“±ì˜", "ì£¼ìš”", "ë¶„ì ˆëœ", "ì‹œìŠ¤í…œì€", "ê¸°ëŠ¥ì´", "ì„¸ë¥´ë¹„ì•„ì˜", "ë°©ê¸€ë¼ë°ì‹œì˜", "ê°•í™”í•˜ê¸°", "ì²´ê³„ì ì¸", "ë¬¸ì œë¥¼", "ì§€ì›í•©ë‹ˆë‹¤", "ë†’ì´ëŠ”", "ê¸°ë³¸", "ë‹¨ì§€ì˜", "ì‚°ì—…ì˜",
    "ë¯¸í¡í•œ", "ì‹œìŠ¤í…œì´", "ë¹„ë¡¯í•œ", "ë‹¤ê°í™”ì™€", "íƒ€ì§€í‚¤ìŠ¤íƒ„ì€", "íƒ€ì§€í‚¤ìŠ¤íƒ„ì˜", "ì •ë³´", "ì´ì—", "ë”°ë¼", "ì‹¤ì •ì…ë‹ˆë‹¤", "ë°ì´í„°ì˜", "ë°ì´í„°ë¥¼", "ê³µìœ í•˜ê³ ", "ê²ƒì…ë‹ˆë‹¤", "ê¶ê·¹ì ìœ¼ë¡œ", "ê¸°ì—¬í• ", "ì •í™•ì„±ì„", "ìë™í™”í•˜ì—¬", "ìˆ˜ë¦½ì˜", "ê³µìœ ë¥¼", "ìœµí•©í•˜ì—¬", "ë¶€ë¬¸ì„", "ì§€ì†", "ë‹¬ì„±í•˜ë„ë¡", "ì„±ì¥ì„", "ë•ëŠ”", "ì‚°ì—…ê³¼",
    "ê²½ì œì—ì„œ", "ê²½ì œë¡œ", "ì „í™˜í•˜ê³ ì", "ê·¸ëŸ¬ë‚˜" ,"ë¶€ë¬¸ì€", "ë¶€ì¡±ìœ¼ë¡œ", "ì ì¬ë ¥ì„", "ì¶©ë¶„íˆ", "í™œìš©í•˜ì§€", "ëª»í•˜ê³ ", "í˜¸ì£¼ëŠ”", "í˜¸ì£¼ì˜", "ë¶„ì•¼ì—ì„œ", "ì¸ë„ë„¤ì‹œì•„ëŠ”", "ë¬¸ì œì ì„", "íš¨ìœ¨ì„±ì„"
}
STOP_LOW = {w.lower() for w in STOP}
#######################################################
# --------------------- Changyeon ---------------------
# st.sidebar.header("1. LLM ì…ë ¥ìš© ZIP í´ë” ìƒì„±")
# def extract_smooth_text_from_pdf(pdf_path: str) -> str:
#     """
#     PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì´ì–´ë¶™ì„
#     """
#     full_text = ""
#     with pdfplumber.open(pdf_path) as pdf:
#         for page in pdf.pages:
#             page_text = page.extract_text() or ""
#             page_text = page_text.strip()
#             if not page_text:
#                 continue

#             if re.search(r'[.?!\'"]$', page_text.strip()):
#                 full_text += page_text + "\n"
#             else:
#                 full_text += page_text + " "
#     return full_text.strip()

# # ì‚¬ì´ë“œë°”ì—ì„œ ZIP íŒŒì¼ ì—…ë¡œë“œ
# uploaded_zip = st.sidebar.file_uploader("ğŸ“‚ PDF í´ë”(ZIP) ì—…ë¡œë“œ", type="zip")

# if uploaded_zip is not None:
#     results = []
#     txt_files = []

#     # ì—…ë¡œë“œí•œ ZIPì„ ì„ì‹œ í´ë”ì— í’€ê¸°
#     with tempfile.TemporaryDirectory() as tmpdir:
#         with zipfile.ZipFile(uploaded_zip, "r") as zip_ref:
#             zip_ref.extractall(tmpdir)

#         # ë³€í™˜ëœ TXTë“¤ì„ ë‹´ì„ ZIP ë²„í¼
#         zip_buffer = io.BytesIO()
#         with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as out_zip:
#             for filename in os.listdir(tmpdir):
#                 if not filename.lower().endswith(".pdf"):
#                     continue

#                 pdf_path = os.path.join(tmpdir, filename)
#                 txt_filename = os.path.splitext(filename)[0] + ".txt"

#                 try:
#                     text = extract_smooth_text_from_pdf(pdf_path)
#                     if len(text) < 100:
#                         results.append(f"âš ï¸ í…ìŠ¤íŠ¸ ë¶€ì¡± â†’ ê±´ë„ˆëœ€: {filename}")
#                         continue

#                     # ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¥¼ ZIPì— ì§ì ‘ ì €ì¥
#                     out_zip.writestr(txt_filename, text)
#                     txt_files.append(txt_filename)
#                     results.append(f"âœ“ ì €ì¥ ì™„ë£Œ: {filename}")
#                 except Exception as e:
#                     results.append(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {filename} â†’ {e}")

#         # ZIP ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
#         zip_buffer.seek(0)
#         st.write("### ì²˜ë¦¬ ê²°ê³¼")
#         st.text("\n".join(results))

#         if txt_files:
#             st.download_button(
#                 label="ğŸ“¥ ë³€í™˜ëœ TXT ZIP ë‹¤ìš´ë¡œë“œ",
#                 data=zip_buffer,
#                 file_name="PDF2TXT_Result.zip",
#                 mime="application/zip"
#             )
# else:
#     st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ZIP íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

# st.sidebar.header("2. LLM ì…ë ¥ìš© í”„ë¡¬í”„íŠ¸ ë³µì‚¬")
# text = """
# ë„¤ ì—­í• ì€ Tabulation machineì´ì•¼.
# zip í´ë”ì˜ ì••ì¶•ì„ í•´ì œí•œ ë’¤ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì„œ tableì„ ë§Œë“¤ ê±°ì•¼.
# tableì˜ ì—´ì€ ['íŒŒì¼ëª…', 'ëŒ€ìƒêµ­', 'ëŒ€ìƒê¸°ê´€', 'ì£¼ìš” ë¶„ì•¼', 'ì—°ë„, 'ì§€ì›ê¸°ê´€', 'ì£¼ìš” ë‚´ìš©', 'ê¸°ëŒ€ íš¨ê³¼', 'ìš”ì•½', 'WB_Class']ë¡œ êµ¬ì„±í•´.
# íŒŒì¼ëª…ì€ zip í´ë” ë‚´ í™•ì¥ì ë° ì˜ë¬¸, êµ­ë¬¸ í‘œê¸°ë¥¼ ì œì™¸í•œ íŒŒì¼ëª…ì„ ì…ë ¥í•´.
# ëŒ€ìƒêµ­, ëŒ€ìƒê¸°ê´€, ì£¼ìš” ë¶„ì•¼, ì§€ì›ê¸°ê´€ì€ íŒŒì¼ ë‚´ìš©ìœ¼ë¡œë¶€í„° ì¶”ì¶œí•´. ì´ë“¤ì€ í•œêµ­ì–´ label í˜•íƒœë¡œ ì…ë ¥í•´.
# ì—°ë„ì€ ì—°ë„ì™€ ëŒ€ì‹œë¥¼ ì‚¬ìš©í•´ì„œ ë‚˜íƒ€ë‚´.
# ì£¼ìš” ë‚´ìš©, ê¸°ëŒ€ íš¨ê³¼, ìš”ì•½ì€ ê°ê° 5ë¬¸ì¥ ì´ìƒ, 10ë¬¸ì¥ ì´í•˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ì…ë ¥í•´.
# ì£¼ìš” ë‚´ìš©ì€ í˜„í™©ê³¼ ì´ìŠˆ, ë¬¸ì œì , ì œì•ˆ ë° ì œì–¸ì„ ìœ„ì£¼ë¡œ ì‘ì„±í•´.
# ê¸°ëŒ€íš¨ê³¼ëŠ” ì •ì„±ì  ë° ì •ëŸ‰ì  ì„±ê³¼, ì „ë§, ê¸°ëŒ€íš¨ê³¼ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•´.
# ìš”ì•½ì€ ë„¤ íŒë‹¨ í•˜ì— ë‹¤ë£° ë§Œí•œ ë¶€ë¶„ì„ ì¢…í•©ì ìœ¼ë¡œ ì‘ì„±í•´.
# ICT ìœ í˜•ëŠ” https://data360.worldbank.org/en/digitalì˜ Topicì„ labelë¡œ ì‚¬ìš©í•  ê±°ì•¼.
# 'Connectivity', 'Data Infrastructure', 'Cybersecurity', 'Digital Industry and Jobs', 'Digital Services' ì¤‘ì— ì„ íƒí•´.
# ë³´ê³ ì„œ ì„±ê²©ì— ë”°ë¼ ì•„ë˜ ì‚¬ì „ì„ ì°¸ê³ í•˜ì—¬ labelì„ í• ë‹¹í•´.
# {Connectivity: [Telecom Networks, Telecom Subscriptions, Digital Adoption, Telecom Markets and Competition, Affordability, Telecom Regulation],
# Data Infrastructure: [Data Centers, Internet Exchange Points (IXPs)],
# Cybersecurity: [ITU Global Cybersecurity Index (GCI)],
# Digital Industry and Jobs: [ICT Industry , Digital Skills],
# Digital Services: [Digital Public Infrastructure - DPI, E-Government]}
# """
# st.sidebar.code(text, language="text")
# st.sidebar.link_button("ğŸŒ LLM ì ‘ì†", "https://chatgpt.com/c")

# st.sidebar.header("3. Hashtag ì¶”ì¶œ ë° ë¬¸ì¥ ê²°í•©")
# def make_tags(row):  # 1
#     tags = []
#     text = f"{row['íŒŒì¼ëª…']} {row['ëŒ€ìƒê¸°ê´€']} {row['ì§€ì›ê¸°ê´€']}"
    
#     if 'KDI' in text:
#         tags += ['ê²½ì œ', 'ì‚¬íšŒì •ì±…']
#     if 'í•œêµ­ìˆ˜ì¶œì…ì€í–‰' in text:
#         tags += ['ê±´ì„¤', 'ì¸í”„ë¼']
#     if 'KOTRA' in text:
#         tags += ['ì‚°ì—…', 'ë¬´ì—­', 'íˆ¬ì']
        
#     return list(dict.fromkeys(tags))

# def del_word(row, column, word):  # 2
#     text = f"{row[column]}"
#     if pd.isna(text):
#         return None
    
#     parts = [p.strip() for p in str(text).split(',')]
#     result_parts = []

#     for p in parts:
#         if not p:
#             continue
#         if word not in p:
#             result_parts.append(p)
#         else:
#             match = re.search(r'\(([^)]*)\)', p)
#             if match:
#                 result_parts.append(match.group(1).strip())

#     return ', '.join(result_parts) if result_parts else None

# def top_tfidf_terms(row_tfidf, terms, k=3):
#     sorted_indices = row_tfidf.toarray().ravel().argsort()[::-1]
#     top_idxs = sorted_indices[:k]
#     return [terms[i] for i in top_idxs if row_tfidf[0, i] > 0]

# uploaded_file = st.sidebar.file_uploader("ğŸ“‚ ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ", type=["xlsx"])

# if uploaded_file:
#     df_c = pd.read_excel(uploaded_file)

#     # ê¸°ì¡´ ì—´ì€ ê·¸ëŒ€ë¡œ ë‘ê³  ìƒˆë¡œìš´ ì—´ ì¶”ê°€
#     df_c['Hashtag'] = df_c.apply(make_tags, axis=1)
#     df_c['ì§€ì›ê¸°ê´€'] = df_c.apply(lambda r: del_word(r, 'ì§€ì›ê¸°ê´€', 'KSP'), axis=1)
#     df_c[['ì§€ì›ê¸°ê´€']] = df_c[['ì§€ì›ê¸°ê´€']].fillna('-')

#     target_cols = ['ëŒ€ìƒê¸°ê´€', 'ì§€ì›ê¸°ê´€']
#     df_c[target_cols] = df_c[target_cols].fillna('')
#     for col in target_cols:
#         df_c[col] = df_c[col].str.replace(r'\s*ë“±$', '', regex=True)

#     df_c['full_text'] = (
#         df_c[['ì£¼ìš” ë‚´ìš©','ê¸°ëŒ€ íš¨ê³¼','ìš”ì•½']]
#         .fillna('')
#         .agg(' '.join, axis=1)
#     )

#     # TFâ€“IDF
#     korean_stopwords = [
#         'ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì„','ë¥¼','ì—','ì™€','ê³¼','ë„','ìœ¼ë¡œ','ì—ì„œ',
#         'í•˜ë‹¤','í•œë‹¤','ìˆë‹¤','ì—†ë‹¤','ì¢‹ë‹¤','ê°™ë‹¤','ë˜ë‹¤','ìˆ˜','ì„','ê¸°','ë“±']
#     tfidf = TfidfVectorizer(max_df=0.8, min_df=2,
#                             stop_words=korean_stopwords,
#                             ngram_range=(1,1), max_features=2000)
#     X_tfidf = tfidf.fit_transform(df_c['full_text'])
#     terms = tfidf.get_feature_names_out()

#     # ê¸°ì¡´ Hashtag + TF-IDF í‚¤ì›Œë“œ í•©ì¹˜ê¸°
#     new_tags = []
#     for i, vec in enumerate(X_tfidf):
#         kws = top_tfidf_terms(vec, terms, k=2)
#         existing = df_c.at[i, 'Hashtag']
#         if isinstance(existing, str):
#             exist_list = [t.strip() for t in existing.split(',') if t.strip()]
#         elif isinstance(existing, list):
#             exist_list = existing.copy()
#         else:
#             exist_list = []
#         for w in kws:
#             if w not in exist_list:
#                 exist_list.append(w)
#         new_tags.append(exist_list)

#     df_c['Hashtag'] = new_tags
#     df_c['Hashtag_str'] = df_c['Hashtag'].apply(lambda lst: ', '.join(lst) if lst else None)

#     # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
#     st.subheader("ğŸ” Hashtag ì¶”ì¶œ ê²°ê³¼ (ìƒìœ„ 10í–‰)")
#     st.dataframe(df_c.head(10))

#     # ë‹¤ìš´ë¡œë“œ
#     output = io.BytesIO()
#     with pd.ExcelWriter(output, engine="openpyxl") as writer:
#         df_c.to_excel(writer, index=False, sheet_name="Result")
#     output.seek(0)

#     st.download_button(
#         "ğŸ“¥ ê²°ê³¼ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
#         data=output,
#         file_name="Hashtag_Result.xlsx",
#         mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
#     )

# else:
#     st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
#######################################################

# --------------------- ë°ì´í„° ì…ë ¥ ---------------------
st.sidebar.header("ë°ì´í„° ì…ë ¥")

# ê¸°ë³¸ê°’(ìˆì–´ë„ ë˜ê³  ì—†ì–´ë„ ë¨ â€” ìë™ íƒì§€ ì‹œ ë¬´ì‹œë¨)
DEFAULT_DATA_PATH = r"df1_20250901_145328.xlsx"

# ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€ ë””ë ‰í† ë¦¬(ë…¸íŠ¸ë¶/REPL ëŒ€ë¹„ fallback)
DATA_DIR = Path(__file__).resolve().parent if "__file__" in globals() else Path.cwd()
SEARCH_DIRS = [DATA_DIR, DATA_DIR / "data", DATA_DIR / "assets"]




@st.cache_data(show_spinner=False)
def load_from_path(path: str) -> pd.DataFrame:
    ext = os.path.splitext(path)[1].lower()
    if ext in [".xlsx", ".xls"]: return pd.read_excel(path)
    if ext == ".csv": return pd.read_csv(path, encoding_errors="ignore")
    return pd.read_excel(path)

def load_from_uploader(f) -> pd.DataFrame:
    name = f.name.lower()
    if name.endswith((".xlsx", ".xls")): return pd.read_excel(f)
    if name.endswith(".csv"): return pd.read_csv(f, encoding_errors="ignore")
    return pd.read_excel(f)

def load_from_csv_text(txt: str) -> pd.DataFrame:
    return pd.read_csv(io.StringIO(txt), encoding_errors="ignore")

@st.cache_data(show_spinner=False)
def discover_data_files(dirs: list[Path]) -> list[Path]:
    """ê°™ì€ í´ë”(+ ê´€ìš© ì„œë¸Œí´ë”)ì—ì„œ ì—‘ì…€/CSV í›„ë³´ íƒìƒ‰ & ìŠ¤ì½”ì–´ë§"""
    cands: list[Path] = []
    for base in dirs:
        if not base.exists(): continue
        for pat in ("*.xlsx", "*.xls", "*.csv"):
            cands.extend(sorted(base.glob(pat)))
    # ìŠ¤ì½”ì–´: íŒŒì¼ëª… íŒíŠ¸(ê°€ì¤‘) + ìµœì‹  ìˆ˜ì •ì‹œê°„
    def score(p: Path) -> tuple:
        name = p.name.lower()
        s = 0
        # í”„ë¡œì íŠ¸ì—ì„œ ìì£¼ ì“°ëŠ” íŒ¨í„´ ê°€ì¤‘ì¹˜
        if "df1" in name: s += 8
        if "ksp" in name: s += 6
        if "state_of_the_table" in name: s += 5
        if "export" in name or "table" in name: s += 2
        if name.startswith("~$") or name.endswith(".tmp"): s -= 100
        # ìµœì‹  íŒŒì¼ ìš°ì„ 
        return (-s, -p.stat().st_mtime)
    # ê°€ì¥ ë†’ì€ ì ìˆ˜(ìŒìˆ˜ ì •ë ¬ ë³´ì • ìœ„í•´ -s)ë¶€í„° ì˜¤ë¦„ì°¨ìˆœ â†’ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê±´ ì ìˆ˜ í° ìˆœì´ë¯€ë¡œ ë‹¤ì‹œ ì •ë ¬ ê¸°ì¤€ ì£¼ì˜
    # ìœ„ scoreì—ì„œ -s, -mtimeì„ ì¤¬ìœ¼ë‹ˆ "ì˜¤ë¦„ì°¨ìˆœ"ìœ¼ë¡œ ì •ë ¬í•˜ë©´ ì‹¤ì§ˆì ìœ¼ë¡œ ì ìˆ˜â†“ â†’ ì›í•˜ëŠ” ê±´ ë°˜ëŒ€.
    # ê°„ë‹¨íˆ ë³„ë„ keyë¡œ ë‹¤ì‹œ ì •ë ¬:
    cands = sorted(cands, key=lambda p: (
        # ê°™ì€ ë¡œì§ì„ ì–‘ìˆ˜ë¡œ ì¬ì‘ì„±
        -(8 if "df1" in p.name.lower() else 0)
        -(6 if "ksp" in p.name.lower() else 0)
        -(5 if "state_of_the_table" in p.name.lower() else 0)
        -(2 if ("export" in p.name.lower() or "table" in p.name.lower()) else 0),
        -p.stat().st_mtime
    ))
    # ì¤‘ë³µ ì œê±°(ë™ì¼ ê²½ë¡œ ëŒ€ë¹„ ì•ˆì „)
    seen = set(); out = []
    for p in cands:
        if p.resolve() not in seen:
            out.append(p); seen.add(p.resolve())
    return out

# â”€â”€ UI: ì†ŒìŠ¤ ì„ íƒ(ìë™ì´ ê¸°ë³¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
src_mode = st.sidebar.radio(
    "ì†ŒìŠ¤ ì„ íƒ",
    ["ìë™(ê°™ì€ í´ë”)", "íŒŒì¼ ì—…ë¡œë“œ", "CSV ë¶™ì—¬ë„£ê¸°", "íŒŒì¼ ê²½ë¡œ"],
    index=0
)

# ìºì‹œ ë¦¬ë¡œë“œ
if st.sidebar.button("ë¡œë“œ/ìƒˆë¡œê³ ì¹¨", use_container_width=True):
    st.cache_data.clear()

df = None
auto_files = discover_data_files(SEARCH_DIRS)

if src_mode == "ìë™(ê°™ì€ í´ë”)":
    if auto_files:
        # í›„ë³´ê°€ ì—¬ëŸ¬ ê°œë©´ ì„ íƒ ë°•ìŠ¤ ì œê³µ(ê¸°ë³¸: ìµœìš°ì„  í›„ë³´)
        labels = [f"{p.name}  â€”  {p.parent.name}/  (ìˆ˜ì •: {pd.to_datetime(p.stat().st_mtime, unit='s'):%Y-%m-%d %H:%M})"
                  for p in auto_files]
        sel_idx = 0
        if len(auto_files) > 1:
            sel_idx = st.sidebar.selectbox("ìë™ íƒì§€ëœ íŒŒì¼", list(range(len(auto_files))),
                                           index=0, format_func=lambda i: labels[i])
        st.sidebar.caption(f"ê²½ë¡œ: `{auto_files[sel_idx]}`")
        df = load_from_path(str(auto_files[sel_idx]))
    else:
        st.sidebar.info("ê°™ì€ í´ë”(ë˜ëŠ” ./data, ./assets)ì—ì„œ ì í•©í•œ ë°ì´í„° íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì†ŒìŠ¤ ë°©ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")

elif src_mode == "íŒŒì¼ ì—…ë¡œë“œ":
    up = st.sidebar.file_uploader("ì—‘ì…€(.xlsx/.xls) ë˜ëŠ” CSV ì—…ë¡œë“œ", type=["xlsx", "xls", "csv"])
    if up is not None:
        df = load_from_uploader(up)

elif src_mode == "CSV ë¶™ì—¬ë„£ê¸°":
    pasted = st.sidebar.text_area("CSV ì›ë¬¸ ë¶™ì—¬ë„£ê¸°(í—¤ë” í¬í•¨)", height=160)
    if pasted.strip():
        df = load_from_csv_text(pasted)

else:  # íŒŒì¼ ê²½ë¡œ
    # ìë™ í›„ë³´ê°€ ìˆìœ¼ë©´ ê¸°ë³¸ê°’ì„ ê·¸ ì¤‘ ì²« ë²ˆì§¸ë¡œ ë…¸ì¶œ(ì—†ìœ¼ë©´ DEFAULT ì‚¬ìš©)
    default_path = str(auto_files[0]) if auto_files else DEFAULT_DATA_PATH
    data_path = st.sidebar.text_input("ì—‘ì…€/CSV ê²½ë¡œ", default_path)
    if os.path.exists(data_path):
        df = load_from_path(data_path)
        st.sidebar.caption(f"ê²½ë¡œ: `{Path(data_path).resolve()}`")

# ë°ì´í„° ì—†ìœ¼ë©´ ì¤‘ë‹¨
if df is None or df.empty:
    st.stop()

# í•„ìˆ˜ ì»¬ëŸ¼ ì§„ë‹¨
REQ = ["íŒŒì¼ëª…","ëŒ€ìƒêµ­","ëŒ€ìƒê¸°ê´€","ì£¼ìš” ë¶„ì•¼","ì§€ì›ê¸°ê´€","ì—°ë„","ì£¼ìš” ë‚´ìš©","ê¸°ëŒ€ íš¨ê³¼",
       "ìš”ì•½","ICT ìœ í˜•","ì£¼ì œë¶„ë¥˜(ëŒ€)","Hashtag","Hashtag_str","full_text"]
missing = [c for c in REQ if c not in df.columns]
if missing:
    st.warning(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")

with st.expander("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° / ì§„ë‹¨", expanded=False):
    st.write(f"í–‰ ìˆ˜: {len(df):,}  |  ê³ ìœ  ëŒ€ìƒêµ­: {df['ëŒ€ìƒêµ­'].nunique()}  |  ê³ ìœ  ICT ìœ í˜•: {df['ICT ìœ í˜•'].nunique()}")
    st.dataframe(df.head(25), use_container_width=True)
# --------------------- ë°ì´í„° ì…ë ¥ (ë) ---------------------


def _font_path_safe():
    return GLOBAL_FONT_PATH or find_korean_font()  # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ None



# ========================= êµ­ê°€ ë¸Œë¦¬í”„(ìš”ì•½) ì…ë ¥ =========================
st.sidebar.header("êµ­ê°€ ë¸Œë¦¬í”„(ìš”ì•½)")

@st.cache_data(show_spinner=False)
def load_country_briefs_from_ipynb_bytes(b: bytes) -> dict:
    """ipynb ì•ˆì˜ code cellì—ì„œ 'briefs = {...}' ë”•ì…”ë„ˆë¦¬ë¥¼ ì°¾ì•„ ë°˜í™˜"""
    import json
    nb = json.loads(b.decode("utf-8"))
    for cell in nb.get("cells", []):
        if cell.get("cell_type") == "code":
            code = "".join(cell.get("source", []))
            if "briefs" in code:
                ns = {}
                try:
                    exec(code, {}, ns)
                except Exception:
                    ns = {}
                briefs = ns.get("briefs", {})
                if isinstance(briefs, dict):
                    return briefs
    return {}

@st.cache_data(show_spinner=False)
def load_country_briefs_auto(app_dir: Path) -> tuple[dict, str | None]:
    """
    ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”(ë˜ëŠ” ê´€ìš© ì„œë¸Œí´ë”)ì—ì„œ CountryBriefs.ipynb ìë™ íƒìƒ‰
    ë°˜í™˜: (briefs_map, ì‚¬ìš©í•œ ê²½ë¡œ ë˜ëŠ” None)
    """
    candidates = [
        app_dir / "CountryBriefs.ipynb",
        app_dir / "assets" / "CountryBriefs.ipynb",
        app_dir / "data" / "CountryBriefs.ipynb",
    ]
    for p in candidates:
        if p.exists():
            try:
                return load_country_briefs_from_ipynb_bytes(p.read_bytes()), str(p)
            except Exception:
                pass
    return {}, None

# í˜„ì¬ ì•± ë””ë ‰í„°ë¦¬(ìŠ¤íŠ¸ë¦¼ë¦¿ì—ì„œ __file__ì´ ì •ìƒì ìœ¼ë¡œ ë“¤ì–´ì˜¨ë‹¤. ë…¸íŠ¸ë¶/REPL ëŒ€ë¹„ fallback ë™ì‘)
APP_DIR = Path(__file__).resolve().parent if "__file__" in globals() else Path.cwd()

brief_mode = st.sidebar.radio("ì†ŒìŠ¤", ["ìë™(ê°™ì€ í´ë”)", "íŒŒì¼ ì—…ë¡œë“œ", "ë¹„í™œì„±í™”"], index=0, horizontal=True)
if st.sidebar.button("ë¸Œë¦¬í”„ ë¦¬ë¡œë“œ", use_container_width=True):
    st.cache_data.clear()

briefs_map: dict = {}
brief_path_used: str | None = None

if brief_mode == "ìë™(ê°™ì€ í´ë”)":
    briefs_map, brief_path_used = load_country_briefs_auto(APP_DIR)
    if brief_path_used:
        st.sidebar.caption(f"ê²½ë¡œ: `{brief_path_used}`")
    else:
        st.sidebar.info("ê°™ì€ í´ë”ì—ì„œ `CountryBriefs.ipynb`ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
elif brief_mode == "íŒŒì¼ ì—…ë¡œë“œ":
    upb = st.sidebar.file_uploader("CountryBriefs.ipynb ì—…ë¡œë“œ", type=["ipynb"])
    if upb is not None:
        briefs_map = load_country_briefs_from_ipynb_bytes(upb.read())
# ë¹„í™œì„±í™”ë©´ briefs_map == {}


# ========================= ICT ìœ í˜• ë¸Œë¦¬í”„(ìš”ì•½) ì…ë ¥ =========================
st.sidebar.header("ICT ìœ í˜• ë¸Œë¦¬í”„(ìš”ì•½)")

@st.cache_data(show_spinner=False)
def load_wb_briefs_from_ipynb_bytes(b: bytes) -> dict:
    """ipynb ì•ˆì˜ code cellì—ì„œ 'wb_briefs' (ë˜ëŠ” 'briefs', 'class_briefs') ë”•ì…”ë„ˆë¦¬ ì°¾ì•„ ë°˜í™˜"""
    import json
    nb = json.loads(b.decode("utf-8"))
    for cell in nb.get("cells", []):
        if cell.get("cell_type") != "code":
            continue
        code = "".join(cell.get("source", []))
        ns = {}
        try:
            exec(code, {}, ns)
        except Exception:
            continue

        # ìš°ì„ ìˆœìœ„: wb_briefs > briefs > class_briefs > ê·¸ ì™¸ 'dict' í›„ë³´
        for key in ["wb_briefs", "briefs", "class_briefs"]:
            obj = ns.get(key)
            if isinstance(obj, dict):
                return obj

        # í˜¹ì‹œ ëª¨ë¥¼ ê¸°íƒ€ ë”•ì…”ë„ˆë¦¬ë„ ìŠ¤ìº”
        for k, v in ns.items():
            if isinstance(v, dict) and k.lower().endswith("briefs"):
                return v
    return {}


@st.cache_data(show_spinner=False)
def load_wb_briefs_auto(app_dir: Path) -> tuple[dict, str | None]:
    """
    ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”/ìì£¼ ì“°ëŠ” ì„œë¸Œí´ë”ì—ì„œ WB_ClassBriefs ë…¸íŠ¸ë¶ ìë™ íƒìƒ‰
    ë°˜í™˜: (wb_briefs_map, ì‚¬ìš©í•œ ê²½ë¡œ ë˜ëŠ” None)
    """
    candidates = []
    for base in [app_dir, app_dir / "assets", app_dir / "data"]:
        for pat in [
            "WB_ClassBriefs.ipynb", "WBClassBriefs.ipynb",
            "wb_class_briefs.ipynb", "wbclass_briefs.ipynb",
            "*WB*Class*Brief*.ipynb",
        ]:
            candidates += list(base.glob(pat))

    for p in candidates:
        try:
            return load_wb_briefs_from_ipynb_bytes(p.read_bytes()), str(p)
        except Exception:
            continue
    return {}, None


wb_brief_mode = st.sidebar.radio("ì†ŒìŠ¤ (ICT ìœ í˜•)", ["ìë™(ê°™ì€ í´ë”)", "íŒŒì¼ ì—…ë¡œë“œ", "ë¹„í™œì„±í™”"],
                                 index=0, horizontal=True)

# 'ë¸Œë¦¬í”„ ë¦¬ë¡œë“œ' ë²„íŠ¼ì€ ìœ„ì—ì„œ st.cache_data.clear()ë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ì—¬ê¸°ì—ë„ ì ìš©ë¨
wb_briefs_map: dict = {}
wb_brief_path_used: str | None = None

if wb_brief_mode == "ìë™(ê°™ì€ í´ë”)":
    wb_briefs_map, wb_brief_path_used = load_wb_briefs_auto(APP_DIR)
    if wb_brief_path_used:
        st.sidebar.caption(f"WB ë¸Œë¦¬í”„ ê²½ë¡œ: `{wb_brief_path_used}`")
    else:
        st.sidebar.info("ê°™ì€ í´ë”ì—ì„œ `WB_ClassBriefs.ipynb`ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
elif wb_brief_mode == "íŒŒì¼ ì—…ë¡œë“œ":
    up_wb = st.sidebar.file_uploader("WB_ClassBriefs.ipynb ì—…ë¡œë“œ", type=["ipynb"])
    if up_wb is not None:
        wb_briefs_map = load_wb_briefs_from_ipynb_bytes(up_wb.read())
# ë¹„í™œì„±í™”ë©´ wb_briefs_map == {}


# --------------------- êµ­ê°€ ë§¤í•‘ ---------------------
COUNTRY_MAP = {
    # ğŸŒ ì•„ì‹œì•„
    "ëŒ€í•œë¯¼êµ­": ("KOR","Korea, Republic of","ëŒ€í•œë¯¼êµ­"), "í•œêµ­": ("KOR","Korea, Republic of","ëŒ€í•œë¯¼êµ­"),
    "ë¶í•œ": ("PRK","Korea, Democratic People's Republic of","ë¶í•œ"),
    "ì¼ë³¸": ("JPN","Japan","ì¼ë³¸"), "ì¤‘êµ­": ("CHN","China","ì¤‘êµ­"), "ëª½ê³¨": ("MNG","Mongolia","ëª½ê³¨"),
    "ë² íŠ¸ë‚¨": ("VNM","Vietnam","ë² íŠ¸ë‚¨"), "ë¼ì˜¤ìŠ¤": ("LAO","Laos","ë¼ì˜¤ìŠ¤"), "ìº„ë³´ë””ì•„": ("KHM","Cambodia","ìº„ë³´ë””ì•„"),
    "íƒœêµ­": ("THA","Thailand","íƒœêµ­"), "ë¯¸ì–€ë§ˆ": ("MMR","Myanmar","ë¯¸ì–€ë§ˆ"),
    "ë§ë ˆì´ì‹œì•„": ("MYS","Malaysia","ë§ë ˆì´ì‹œì•„"), "ì‹±ê°€í¬ë¥´": ("SGP","Singapore","ì‹±ê°€í¬ë¥´"),
    "ì¸ë„ë„¤ì‹œì•„": ("IDN","Indonesia","ì¸ë„ë„¤ì‹œì•„"), "í•„ë¦¬í•€": ("PHL","Philippines","í•„ë¦¬í•€"),
    "ë¸Œë£¨ë‚˜ì´": ("BRN","Brunei Darussalam","ë¸Œë£¨ë‚˜ì´"), "ë™í‹°ëª¨ë¥´": ("TLS","Timor-Leste","ë™í‹°ëª¨ë¥´"),
    "ì¸ë„": ("IND","India","ì¸ë„"), "íŒŒí‚¤ìŠ¤íƒ„": ("PAK","Pakistan","íŒŒí‚¤ìŠ¤íƒ„"), "ë„¤íŒ”": ("NPL","Nepal","ë„¤íŒ”"),
    "ë¶€íƒ„": ("BTN","Bhutan","ë¶€íƒ„"), "ìŠ¤ë¦¬ë‘ì¹´": ("LKA","Sri Lanka","ìŠ¤ë¦¬ë‘ì¹´"), "ëª°ë””ë¸Œ": ("MDV","Maldives","ëª°ë””ë¸Œ"),
    "ì¹´ìíìŠ¤íƒ„": ("KAZ","Kazakhstan","ì¹´ìíìŠ¤íƒ„"), "ìš°ì¦ˆë² í‚¤ìŠ¤íƒ„": ("UZB","Uzbekistan","ìš°ì¦ˆë² í‚¤ìŠ¤íƒ„"),
    "í‚¤ë¥´ê¸°ìŠ¤ìŠ¤íƒ„": ("KGZ","Kyrgyzstan","í‚¤ë¥´ê¸°ìŠ¤ìŠ¤íƒ„"), "íƒ€ì§€í‚¤ìŠ¤íƒ„": ("TJK","Tajikistan","íƒ€ì§€í‚¤ìŠ¤íƒ„"),
    "íˆ¬ë¥´í¬ë©”ë‹ˆìŠ¤íƒ„": ("TKM","Turkmenistan","íˆ¬ë¥´í¬ë©”ë‹ˆìŠ¤íƒ„"), "ì•„í”„ê°€ë‹ˆìŠ¤íƒ„": ("AFG","Afghanistan","ì•„í”„ê°€ë‹ˆìŠ¤íƒ„"),
    "ì´ë€": ("IRN","Iran","ì´ë€"), "ì´ë¼í¬": ("IRQ","Iraq","ì´ë¼í¬"), "ì‹œë¦¬ì•„": ("SYR","Syrian Arab Republic","ì‹œë¦¬ì•„"),
    "ë ˆë°”ë…¼": ("LBN","Lebanon","ë ˆë°”ë…¼"), "ì´ìŠ¤ë¼ì—˜": ("ISR","Israel","ì´ìŠ¤ë¼ì—˜"), "íŒ”ë ˆìŠ¤íƒ€ì¸": ("PSE","Palestine","íŒ”ë ˆìŠ¤íƒ€ì¸"),
    "ìš”ë¥´ë‹¨": ("JOR","Jordan","ìš”ë¥´ë‹¨"), "ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„": ("SAU","Saudi Arabia","ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„"),
    "ì˜ˆë©˜": ("YEM","Yemen","ì˜ˆë©˜"), "ì˜¤ë§Œ": ("OMN","Oman","ì˜¤ë§Œ"), "ì•„ëì—ë¯¸ë¦¬íŠ¸": ("ARE","United Arab Emirates","ì•„ëì—ë¯¸ë¦¬íŠ¸"),
    "ì¹´íƒ€ë¥´": ("QAT","Qatar","ì¹´íƒ€ë¥´"), "ë°”ë ˆì¸": ("BHR","Bahrain","ë°”ë ˆì¸"), "ì¿ ì›¨ì´íŠ¸": ("KWT","Kuwait","ì¿ ì›¨ì´íŠ¸"),

    # ğŸŒ ìœ ëŸ½
    "ì˜êµ­": ("GBR","United Kingdom","ì˜êµ­"), "ì•„ì¼ëœë“œ": ("IRL","Ireland","ì•„ì¼ëœë“œ"), "í”„ë‘ìŠ¤": ("FRA","France","í”„ë‘ìŠ¤"),
    "ë…ì¼": ("DEU","Germany","ë…ì¼"), "ì´íƒˆë¦¬ì•„": ("ITA","Italy","ì´íƒˆë¦¬ì•„"), "ìŠ¤í˜ì¸": ("ESP","Spain","ìŠ¤í˜ì¸"),
    "í¬ë¥´íˆ¬ê°ˆ": ("PRT","Portugal","í¬ë¥´íˆ¬ê°ˆ"), "ë„¤ëœë€ë“œ": ("NLD","Netherlands","ë„¤ëœë€ë“œ"),
    "ë²¨ê¸°ì—": ("BEL","Belgium","ë²¨ê¸°ì—"), "ë£©ì…ˆë¶€ë¥´í¬": ("LUX","Luxembourg","ë£©ì…ˆë¶€ë¥´í¬"),
    "ìŠ¤ìœ„ìŠ¤": ("CHE","Switzerland","ìŠ¤ìœ„ìŠ¤"), "ì˜¤ìŠ¤íŠ¸ë¦¬ì•„": ("AUT","Austria","ì˜¤ìŠ¤íŠ¸ë¦¬ì•„"),
    "ë´ë§ˆí¬": ("DNK","Denmark","ë´ë§ˆí¬"), "ë…¸ë¥´ì›¨ì´": ("NOR","Norway","ë…¸ë¥´ì›¨ì´"), "ìŠ¤ì›¨ë´": ("SWE","Sweden","ìŠ¤ì›¨ë´"),
    "í•€ë€ë“œ": ("FIN","Finland","í•€ë€ë“œ"), "ì•„ì´ìŠ¬ë€ë“œ": ("ISL","Iceland","ì•„ì´ìŠ¬ë€ë“œ"),
    "ì²´ì½”": ("CZE","Czechia","ì²´ì½”"), "í´ë€ë“œ": ("POL","Poland","í´ë€ë“œ"), "í—ê°€ë¦¬": ("HUN","Hungary","í—ê°€ë¦¬"),
    "ìŠ¬ë¡œë°”í‚¤ì•„": ("SVK","Slovakia","ìŠ¬ë¡œë°”í‚¤ì•„"), "ìŠ¬ë¡œë² ë‹ˆì•„": ("SVN","Slovenia","ìŠ¬ë¡œë² ë‹ˆì•„"),
    "í¬ë¡œì•„í‹°ì•„": ("HRV","Croatia","í¬ë¡œì•„í‹°ì•„"), "ì„¸ë¥´ë¹„ì•„": ("SRB","Serbia","ì„¸ë¥´ë¹„ì•„"),
    "ëª¬í…Œë„¤ê·¸ë¡œ": ("MNE","Montenegro","ëª¬í…Œë„¤ê·¸ë¡œ"), "ë³´ìŠ¤ë‹ˆì•„í—¤ë¥´ì²´ê³ ë¹„ë‚˜": ("BIH","Bosnia and Herzegovina","ë³´ìŠ¤ë‹ˆì•„í—¤ë¥´ì²´ê³ ë¹„ë‚˜"),
    "ë¶ë§ˆì¼€ë„ë‹ˆì•„": ("MKD","North Macedonia","ë¶ë§ˆì¼€ë„ë‹ˆì•„"), "ì•Œë°”ë‹ˆì•„": ("ALB","Albania","ì•Œë°”ë‹ˆì•„"),
    "ê·¸ë¦¬ìŠ¤": ("GRC","Greece","ê·¸ë¦¬ìŠ¤"), "í„°í‚¤": ("TUR","TÃ¼rkiye","í„°í‚¤"),
    "ë£¨ë§ˆë‹ˆì•„": ("ROU","Romania","ë£¨ë§ˆë‹ˆì•„"), "ë¶ˆê°€ë¦¬ì•„": ("BGR","Bulgaria","ë¶ˆê°€ë¦¬ì•„"),
    "ëª°ë„ë°”": ("MDA","Moldova","ëª°ë„ë°”"), "ìš°í¬ë¼ì´ë‚˜": ("UKR","Ukraine","ìš°í¬ë¼ì´ë‚˜"), "ë²¨ë¼ë£¨ìŠ¤": ("BLR","Belarus","ë²¨ë¼ë£¨ìŠ¤"),
    "ë¦¬íˆ¬ì•„ë‹ˆì•„": ("LTU","Lithuania","ë¦¬íˆ¬ì•„ë‹ˆì•„"), "ë¼íŠ¸ë¹„ì•„": ("LVA","Latvia","ë¼íŠ¸ë¹„ì•„"), "ì—ìŠ¤í† ë‹ˆì•„": ("EST","Estonia","ì—ìŠ¤í† ë‹ˆì•„"),
    "ì¡°ì§€ì•„": ("GEO","Georgia","ì¡°ì§€ì•„"), "ì•„ë¥´ë©”ë‹ˆì•„": ("ARM","Armenia","ì•„ë¥´ë©”ë‹ˆì•„"), "ì•„ì œë¥´ë°”ì´ì”": ("AZE","Azerbaijan","ì•„ì œë¥´ë°”ì´ì”"),
    "ëŸ¬ì‹œì•„": ("RUS","Russian Federation","ëŸ¬ì‹œì•„"),

    # ğŸŒ ì•„í”„ë¦¬ì¹´
    "ì´ì§‘íŠ¸": ("EGY","Egypt","ì´ì§‘íŠ¸"), "ë¦¬ë¹„ì•„": ("LBY","Libya","ë¦¬ë¹„ì•„"), "ì•Œì œë¦¬": ("DZA","Algeria","ì•Œì œë¦¬"),
    "ëª¨ë¡œì½”": ("MAR","Morocco","ëª¨ë¡œì½”"), "íŠ€ë‹ˆì§€": ("TUN","Tunisia","íŠ€ë‹ˆì§€"), "ìˆ˜ë‹¨": ("SDN","Sudan","ìˆ˜ë‹¨"),
    "ë‚¨ìˆ˜ë‹¨": ("SSD","South Sudan","ë‚¨ìˆ˜ë‹¨"), "ì—í‹°ì˜¤í”¼ì•„": ("ETH","Ethiopia","ì—í‹°ì˜¤í”¼ì•„"),
    "ì—ë¦¬íŠ¸ë ˆì•„": ("ERI","Eritrea","ì—ë¦¬íŠ¸ë ˆì•„"), "ì§€ë¶€í‹°": ("DJI","Djibouti","ì§€ë¶€í‹°"),
    "ì†Œë§ë¦¬ì•„": ("SOM","Somalia","ì†Œë§ë¦¬ì•„"), "ì¼€ëƒ": ("KEN","Kenya","ì¼€ëƒ"), "íƒ„ìë‹ˆì•„": ("TZA","Tanzania","íƒ„ìë‹ˆì•„"),
    "ìš°ê°„ë‹¤": ("UGA","Uganda","ìš°ê°„ë‹¤"), "ë¥´ì™„ë‹¤": ("RWA","Rwanda","ë¥´ì™„ë‹¤"), "ë¶€ë£¬ë””": ("BDI","Burundi","ë¶€ë£¬ë””"),
    "ì½©ê³ ë¯¼ì£¼ê³µí™”êµ­": ("COD","Democratic Republic of the Congo","ì½©ê³ ë¯¼ì£¼ê³µí™”êµ­"),
    "ì½©ê³ ê³µí™”êµ­": ("COG","Republic of the Congo","ì½©ê³ ê³µí™”êµ­"),
    "ì•™ê³¨ë¼": ("AGO","Angola","ì•™ê³¨ë¼"), "ì ë¹„ì•„": ("ZMB","Zambia","ì ë¹„ì•„"), "ì§ë°”ë¸Œì›¨": ("ZWE","Zimbabwe","ì§ë°”ë¸Œì›¨"),
    "ë§ë¼ìœ„": ("MWI","Malawi","ë§ë¼ìœ„"), "ëª¨ì ë¹„í¬": ("MOZ","Mozambique","ëª¨ì ë¹„í¬"), "ë§ˆë‹¤ê°€ìŠ¤ì¹´ë¥´": ("MDG","Madagascar","ë§ˆë‹¤ê°€ìŠ¤ì¹´ë¥´"),
    "ë‚¨ì•„í”„ë¦¬ì¹´ê³µí™”êµ­": ("ZAF","South Africa","ë‚¨ì•„í”„ë¦¬ì¹´ê³µí™”êµ­"), "ë³´ì¸ ì™€ë‚˜": ("BWA","Botswana","ë³´ì¸ ì™€ë‚˜"),
    "ë‚˜ë¯¸ë¹„ì•„": ("NAM","Namibia","ë‚˜ë¯¸ë¹„ì•„"), "ë ˆì†Œí† ": ("LSO","Lesotho","ë ˆì†Œí† "), "ì—ìŠ¤ì™€í‹°ë‹ˆ": ("SWZ","Eswatini","ì—ìŠ¤ì™€í‹°ë‹ˆ"),
    "ê°€ë‚˜": ("GHA","Ghana","ê°€ë‚˜"), "ì½”íŠ¸ë””ë¶€ì•„ë¥´": ("CIV","CÃ´te d'Ivoire","ì½”íŠ¸ë””ë¶€ì•„ë¥´"), "ë‚˜ì´ì§€ë¦¬ì•„": ("NGA","Nigeria","ë‚˜ì´ì§€ë¦¬ì•„"),
    "ì„¸ë„¤ê°ˆ": ("SEN","Senegal","ì„¸ë„¤ê°ˆ"), "ë§ë¦¬": ("MLI","Mali","ë§ë¦¬"), "ë‹ˆì œë¥´": ("NER","Niger","ë‹ˆì œë¥´"),
    "ì°¨ë“œ": ("TCD","Chad","ì°¨ë“œ"), "ì¹´ë©”ë£¬": ("CMR","Cameroon","ì¹´ë©”ë£¬"), "ê°€ë´‰": ("GAB","Gabon","ê°€ë´‰"),
    "ì ë„ê¸°ë‹ˆ": ("GNQ","Equatorial Guinea","ì ë„ê¸°ë‹ˆ"),

    # ğŸŒ ì•„ë©”ë¦¬ì¹´
    "ë¯¸êµ­": ("USA","United States of America","ë¯¸êµ­"), "ìºë‚˜ë‹¤": ("CAN","Canada","ìºë‚˜ë‹¤"),
    "ë©•ì‹œì½”": ("MEX","Mexico","ë©•ì‹œì½”"), "ë¸Œë¼ì§ˆ": ("BRA","Brazil","ë¸Œë¼ì§ˆ"), "ì•„ë¥´í—¨í‹°ë‚˜": ("ARG","Argentina","ì•„ë¥´í—¨í‹°ë‚˜"),
    "ì¹ ë ˆ": ("CHL","Chile","ì¹ ë ˆ"), "í˜ë£¨": ("PER","Peru","í˜ë£¨"), "ì½œë¡¬ë¹„ì•„": ("COL","Colombia","ì½œë¡¬ë¹„ì•„"),
    "ì—ì½°ë„ë¥´": ("ECU","Ecuador","ì—ì½°ë„ë¥´"), "ìš°ë£¨ê³¼ì´": ("URY","Uruguay","ìš°ë£¨ê³¼ì´"), "íŒŒë¼ê³¼ì´": ("PRY","Paraguay","íŒŒë¼ê³¼ì´"),
    "ë³¼ë¦¬ë¹„ì•„": ("BOL","Bolivia","ë³¼ë¦¬ë¹„ì•„"), "ë² ë„¤ìˆ˜ì—˜ë¼": ("VEN","Venezuela","ë² ë„¤ìˆ˜ì—˜ë¼"),
    "ì¿ ë°”": ("CUB","Cuba","ì¿ ë°”"), "ë„ë¯¸ë‹ˆì¹´ê³µí™”êµ­": ("DOM","Dominican Republic","ë„ë¯¸ë‹ˆì¹´ê³µí™”êµ­"),
    "ìë©”ì´ì¹´": ("JAM","Jamaica","ìë©”ì´ì¹´"), "ì•„ì´í‹°": ("HTI","Haiti","ì•„ì´í‹°"),
    "ì½”ìŠ¤íƒ€ë¦¬ì¹´": ("CRI","Costa Rica","ì½”ìŠ¤íƒ€ë¦¬ì¹´"), "íŒŒë‚˜ë§ˆ": ("PAN","Panama","íŒŒë‚˜ë§ˆ"),
    "ì˜¨ë‘ë¼ìŠ¤": ("HND","Honduras","ì˜¨ë‘ë¼ìŠ¤"), "ì—˜ì‚´ë°”ë„ë¥´": ("SLV","El Salvador","ì—˜ì‚´ë°”ë„ë¥´"),
    "ë‹ˆì¹´ë¼ê³¼": ("NIC","Nicaragua","ë‹ˆì¹´ë¼ê³¼"), "ê³¼í…Œë§ë¼": ("GTM","Guatemala","ê³¼í…Œë§ë¼"),

    # ğŸŒŠ ì˜¤ì„¸ì•„ë‹ˆì•„
    "í˜¸ì£¼": ("AUS","Australia","í˜¸ì£¼"), "ë‰´ì§ˆëœë“œ": ("NZL","New Zealand","ë‰´ì§ˆëœë“œ"),
    "íŒŒí‘¸ì•„ë‰´ê¸°ë‹ˆ": ("PNG","Papua New Guinea","íŒŒí‘¸ì•„ë‰´ê¸°ë‹ˆ"), "í”¼ì§€": ("FJI","Fiji","í”¼ì§€"),
    "ì‚¬ëª¨ì•„": ("WSM","Samoa","ì‚¬ëª¨ì•„"), "í†µê°€": ("TON","Tonga","í†µê°€"), "ë°”ëˆ„ì•„íˆ¬": ("VUT","Vanuatu","ë°”ëˆ„ì•„íˆ¬"),
}

REGION_RULES = {
    "ë©”ì½©ê°•ìœ„ì›íšŒ": [
        ("KHM","Cambodia","ìº„ë³´ë””ì•„"),
        ("LAO","Laos","ë¼ì˜¤ìŠ¤"),
        ("THA","Thailand","íƒœêµ­"),
        ("VNM","Vietnam","ë² íŠ¸ë‚¨"),
    ],
    "í˜¸ì£¼Â·í•œêµ­": [
        ("AUS","Australia","í˜¸ì£¼"),
        ("KOR","Korea, Republic of","ëŒ€í•œë¯¼êµ­"),
    ],
    "ì¤‘ë‚¨ë¯¸ ì§€ì—­": [
        ("ARG","Argentina","ì•„ë¥´í—¨í‹°ë‚˜"),("BRA","Brazil","ë¸Œë¼ì§ˆ"),("CHL","Chile","ì¹ ë ˆ"),
        ("URY","Uruguay","ìš°ë£¨ê³¼ì´"),("PRY","Paraguay","íŒŒë¼ê³¼ì´"),("BOL","Bolivia","ë³¼ë¦¬ë¹„ì•„"),
        ("PER","Peru","í˜ë£¨"),("ECU","Ecuador","ì—ì½°ë„ë¥´"),("COL","Colombia","ì½œë¡¬ë¹„ì•„"),
        ("VEN","Venezuela","ë² ë„¤ìˆ˜ì—˜ë¼"),("GUY","Guyana","ê°€ì´ì•„ë‚˜"),("SUR","Suriname","ìˆ˜ë¦¬ë‚¨"),
        ("MEX","Mexico","ë©•ì‹œì½”"),("GTM","Guatemala","ê³¼í…Œë§ë¼"),("BLZ","Belize","ë²¨ë¦¬ì¦ˆ"),
        ("HND","Honduras","ì˜¨ë‘ë¼ìŠ¤"),("SLV","El Salvador","ì—˜ì‚´ë°”ë„ë¥´"),("NIC","Nicaragua","ë‹ˆì¹´ë¼ê³¼"),
        ("CRI","Costa Rica","ì½”ìŠ¤íƒ€ë¦¬ì¹´"),("PAN","Panama","íŒŒë‚˜ë§ˆ"),
        ("CUB","Cuba","ì¿ ë°”"),("DOM","Dominican Republic","ë„ë¯¸ë‹ˆì¹´ê³µí™”êµ­"),("HTI","Haiti","ì•„ì´í‹°"),
        ("JAM","Jamaica","ìë©”ì´ì¹´"),("BRB","Barbados","ë°”ë² ì´ë„ìŠ¤"),("BHS","Bahamas","ë°”í•˜ë§ˆ"),
        ("TTO","Trinidad and Tobago","íŠ¸ë¦¬ë‹ˆë‹¤ë“œí† ë°”ê³ "),("LCA","Saint Lucia","ì„¸ì¸íŠ¸ë£¨ì‹œì•„"),
        ("VCT","Saint Vincent and the Grenadines","ì„¸ì¸íŠ¸ë¹ˆì„¼íŠ¸ê·¸ë ˆë‚˜ë”˜"),
        ("KNA","Saint Kitts and Nevis","ì„¸ì¸íŠ¸í‚¤ì¸ ë„¤ë¹„ìŠ¤"),
        ("GRD","Grenada","ê·¸ë ˆë‚˜ë‹¤"),("DMA","Dominica","ë„ë¯¸ë‹ˆì¹´ì—°ë°©"),
        ("ATG","Antigua and Barbuda","ì•¤í‹°ê°€ë°”ë¶€ë‹¤"),("PRI","Puerto Rico","í‘¸ì—ë¥´í† ë¦¬ì½”"),
        ("VIR","Virgin Islands (U.S.)","ë¯¸êµ­ë ¹ ë²„ì§„ ì•„ì¼ëœë“œ"),
        ("CYM","Cayman Islands","ì¼€ì´ë§¨ ì œë„"),("TCA","Turks and Caicos Islands","í„°í¬ìŠ¤ ì¼€ì´ì»¤ìŠ¤ ì œë„"),
        ("ABW","Aruba","ì•„ë£¨ë°”"),("CUW","CuraÃ§ao","í€´ë¼ì†Œ"),("SXM","Sint Maarten","ì‹ íŠ¸ë§ˆë¥´í„´"),
        ("MAF","Saint Martin (French part)","ìƒë§ˆë¥´íƒ±"),
    ],
}

def split_countries(x: str):
    if pd.isna(x): return []
    return [tok for tok in re.split(r"[Â·/,;|&]+|\s*,\s*|\s*&\s*", str(x).strip()) if tok]

def map_country_token(token: str):
    tkn = token.strip()
    if tkn in COUNTRY_MAP: return [COUNTRY_MAP[tkn]]
    if tkn in REGION_RULES: return REGION_RULES[tkn]
    out = []
    for s in re.split(r"[Â·/,;|&]+", tkn):
        s = s.strip()
        if s in COUNTRY_MAP: out.append(COUNTRY_MAP[s])
    return out

def expand_by_country(df_in: pd.DataFrame) -> pd.DataFrame:
    rows = []
    for _, row in df_in.iterrows():
        tokens = split_countries(row.get("ëŒ€ìƒêµ­",""))
        mapped = []
        for tk in tokens: mapped.extend(map_country_token(tk))
        if not mapped: continue
        for iso3, en, ko in mapped:
            d = row.to_dict()
            d.update({"iso3": iso3, "country_en": en, "country_ko": ko})
            rows.append(d)
    return pd.DataFrame(rows)

dfx = expand_by_country(df)

# --------------------- ì„¸ê³„ ê²½ê³„ + key_on ìë™ ---------------------
@st.cache_data(show_spinner=False)
def get_world_geojson_auto() -> Dict:
    url = "https://raw.githubusercontent.com/python-visualization/folium/master/examples/data/world-countries.json"
    cache_dir = pathlib.Path(".ksp_cache"); cache_dir.mkdir(exist_ok=True)
    local = cache_dir / ("world-countries." + hashlib.md5(url.encode()).hexdigest() + ".json")
    if not local.exists():
        with urllib.request.urlopen(url, timeout=20) as resp: local.write_bytes(resp.read())
    return json.loads(local.read_text(encoding="utf-8"))

world_geojson = get_world_geojson_auto()

def resolve_geojson_key_on(gj: dict):
    feat0 = gj["features"][0]; props = feat0.get("properties", {})
    for c in ["iso_a3","ISO_A3","adm0_a3","ADM0_A3","wb_a3","WB_A3","ISO3","id"]:
        if c in props: return f"feature.properties.{c}", c, True
    if "id" in feat0: return "feature.id", "id", False
    raise ValueError("GeoJSONì—ì„œ ISO3 í‚¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

key_on_info = resolve_geojson_key_on(world_geojson)

def augment_geojson_values(gj: dict, key_on_info, value_map: dict, value_prop: str):
    key_on_str, iso_key, in_props = key_on_info
    new_gj = copy.deepcopy(gj)
    for feat in new_gj["features"]:
        props = feat.setdefault("properties", {})
        iso = props.get(iso_key) if in_props else feat.get("id")
        props["ISO3"] = iso
        props[value_prop] = value_map.get(str(iso), 0)
    return new_gj

# --------------------- ì§€ë„/í´ë¦­ ìœ í‹¸ ---------------------
def make_base_map(center=[15,10], zoom=3):
    m = folium.Map(
        location=center, zoom_start=zoom, tiles=None,
        control_scale=True, prefer_canvas=True,
        world_copy_jump=False, max_bounds=True, max_bounds_viscosity=1.0,
        min_zoom=2
    )
    folium.TileLayer(tiles="CartoDB Positron", name="Base", control=False, no_wrap=True).add_to(m)
    return m

def extract_iso_from_stfolium(ret: dict):
    if not ret: return None
    iso_keys = ["ISO3","id","iso_a3","ISO_A3","adm0_a3","ADM0_A3","wb_a3","WB_A3"]
    obj = ret.get("last_object_clicked")
    if isinstance(obj, dict):
        props = obj.get("properties") or {}
        for k in iso_keys:
            if props.get(k): return props.get(k)
    lad = ret.get("last_active_drawing") or ret.get("last_active_drawing_geojson")
    if isinstance(lad, dict):
        props = lad.get("properties") or lad.get("feature", {}).get("properties") or {}
        for k in iso_keys:
            if props.get(k): return props.get(k)
    s = ret.get("last_object_clicked_popup")
    if isinstance(s, str):
        m = re.search(r"([A-Z]{3})", s)
        if m: return m.group(1)
    return None

# --------------------- ì—°ë„ íŒŒì„œ ---------------------
@st.cache_data(show_spinner=False)
def expand_years(df_in: pd.DataFrame) -> pd.DataFrame:
    def years_from_text(txt: str):
        if pd.isna(txt): return []
        raw = str(txt)
        yrs = [int(y) for y in re.findall(r"(?:19|20)\d{2}", raw)]
        yrs = [y for y in yrs if 1990 <= y <= 2035]
        if not yrs: return []
        if len(yrs) == 1: return yrs
        a, b = min(yrs), max(yrs)
        if b - a > 30: b = a
        return list(range(a, b+1))
    dfy = df_in.copy()
    dfy["ì—°ë„ëª©ë¡"] = dfy["ì—°ë„"].apply(years_from_text)
    dfy = dfy.explode("ì—°ë„ëª©ë¡")
    dfy = dfy.rename(columns={"ì—°ë„ëª©ë¡":"ì—°ë„"})
    dfy["ì—°ë„"] = pd.to_numeric(dfy["ì—°ë„"], errors="coerce").astype("Int64")
    return dfy

dfy = expand_years(df)     # í‚¤ì›Œë“œ/ì£¼ì œ ìƒëŒ€ íŠ¸ë Œë“œëŠ” 'êµ­ê°€ ì¤‘ë³µ ì—†ëŠ”' ì›ë³¸ df ê¸°ì¤€

# --------------------- ë³´ê¸° ëª¨ë“œ ---------------------
st.sidebar.header("ë³´ê¸° ëª¨ë“œ")
mode = st.sidebar.radio("ì§€ë„ ìœ í˜•", ["êµ­ê°€ë³„ ì´ê³„", "ICT ìœ í˜• ë‹¨ì¼í´ë˜ìŠ¤"], index=0)

# ì—°ë„ ì‹œê°í™” ì˜µì…˜ (íˆíŠ¸ë§µ ì œê±°)
st.sidebar.header("ì—°ë„ ì‹œê°í™” ë°©ì‹")
YEAR_OPTIONS = ["100% ëˆ„ì  ë§‰ëŒ€", "ìˆœìœ„ Bump"]
year_mode = st.sidebar.selectbox("í‘œí˜„ ë°©ì‹", YEAR_OPTIONS, index=0, key="year_mode")


clicked_iso = None

# ===================== â‘  êµ­ê°€ë³„ ì´ê³„ (í´ë¦­) =====================
if mode == "êµ­ê°€ë³„ ì´ê³„":
    st.subheader("êµ­ê°€ë³„ ì´ í”„ë¡œì íŠ¸ ìˆ˜")
    agg_country = dfx.groupby(["iso3","country_ko"], as_index=False).agg(n_docs=("íŒŒì¼ëª…","nunique"))
    base = make_base_map()
    value_map = {r.iso3: int(r.n_docs) for _, r in agg_country.iterrows()}
    gj = augment_geojson_values(world_geojson, key_on_info, value_map, "ksp_docs")

    ch = folium.Choropleth(
        geo_data=gj, data=agg_country, columns=["iso3","n_docs"],
        key_on=key_on_info[0],
        fill_color="YlGnBu", fill_opacity=0.88,
        line_opacity=0.55, line_color="#7f7f7f",
        legend_name="ë³´ê³ ì„œ ìˆ˜", nan_fill_color="#f0f0f0",
        highlight=True
    ); ch.add_to(base)
    ch.geojson.add_child(folium.features.GeoJsonTooltip(
        fields=["ISO3", "name" if "name" in gj["features"][0]["properties"] else "ISO3", "ksp_docs"],
        aliases=["ISO3", "êµ­ê°€", "ë³´ê³ ì„œ ìˆ˜"], sticky=False
    ))
    ch.geojson.add_child(folium.features.GeoJsonPopup(fields=["ISO3"], aliases=["ISO3"]))
    ret = st_folium(base, height=560, use_container_width=True)
    clicked_iso = extract_iso_from_stfolium(ret)



# --------------------- ìƒì„¸ íŒ¨ë„ ---------------------
def find_korean_font() -> str | None:
    candidates = [
        r"C:\\Windows\\Fonts\\malgun.ttf", r"C:\\Windows\\Fonts\\Malgun.ttf",
        r"C:\\Windows\\Fonts\\NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/System/Library/Fonts/AppleGothic.ttf",
    ]
    for p in candidates:
        if os.path.exists(p): return p
    return None

# ======== Color utils ========
def _hex_to_rgb(hexstr: str) -> tuple[int,int,int]:
    s = hexstr.strip().lstrip("#")
    if len(s) == 3: s = "".join(c*2 for c in s)
    return int(s[0:2],16), int(s[2:4],16), int(s[4:6],16)

def rgba_str(color: str, alpha: float=0.5) -> str:
    if not color: return "rgba(0,0,0,0)"
    if color.startswith("rgb"):  # already rgba/rgb string
        return color if "rgba" in color else color.replace("rgb(", "rgba(").replace(")", f", {alpha})")
    r,g,b = _hex_to_rgb(color); return f"rgba({r},{g},{b},{alpha})"


# ê³µí†µ Plotly ìŠ¤íƒ€ì¼ (í°íŠ¸ í¬ê²Œ)
# ê³µí†µ Plotly ìŠ¤íƒ€ì¼ (í°íŠ¸ í¬ê²Œ) â€” ë°°ê²½ íˆ¬ëª… + ë²”ë¡€ ì¤„ë°”ê¿ˆ ì‹œ ìƒë‹¨ì—¬ë°± ìë™ ë³´ì •
# ---- PATCH B: Upgrade style_fig (consistent, professional charts) ----
# ì•ˆì „í•œ style_fig: titleì´ Noneì´ë©´ ê¸°ì¡´ ì œëª©ì„ ë³´ì¡´
def style_fig(fig, title=None, height=None, legend="top", top_margin=96,
              auto_legend_space=True, bg_color=None, bg_alpha=0.5):
    # legend presets
    if legend == "top":
        legend_cfg = dict(orientation="h", y=1.12, yanchor="bottom", x=0, xanchor="left", bgcolor="rgba(0,0,0,0)")
        m = dict(l=16, r=24, b=72, t=top_margin)
    elif legend == "bottom":
        legend_cfg = dict(orientation="h", y=-0.22, yanchor="top", x=0, xanchor="left", bgcolor="rgba(0,0,0,0)")
        m = dict(l=16, r=24, b=120, t=72)
    elif legend == "right":
        legend_cfg = dict(orientation="v", y=0.5, yanchor="middle", x=1.02, xanchor="left", bgcolor="rgba(0,0,0,0)")
        m = dict(l=16, r=160, b=72, t=top_margin)
    else:
        legend_cfg, m = dict(), dict(l=16, r=24, b=72, t=top_margin)

    # ë°°ê²½ rgba
    def _hex_to_rgb(s):
        s = s.strip().lstrip("#")
        if len(s) == 3: s = "".join(c*2 for c in s)
        return int(s[0:2],16), int(s[2:4],16), int(s[4:6],16)
    def rgba_str(color, a=0.5):
        if not color: return "rgba(0,0,0,0)"
        if color.startswith("rgb"):  # rgb/rgba ë¬¸ìì—´
            return color if "rgba" in color else color.replace("rgb(", "rgba(").replace(")", f", {a})")
        r,g,b = _hex_to_rgb(color); return f"rgba({r},{g},{b},{a})"
    bg_rgba = rgba_str(bg_color, bg_alpha) if bg_color else "rgba(0,0,0,0)"

    # layout kwargsë¥¼ ë™ì ìœ¼ë¡œ êµ¬ì„± (titleì€ Noneì´ë©´ ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
    layout_kwargs = dict(
        template=ui["plotly_template"],
        paper_bgcolor=bg_rgba,
        plot_bgcolor=bg_rgba,
        font=dict(family=_plotly_font_family(), color=ui["text"], size=16),
        height=height,
        margin=m,
        legend=legend_cfg if legend != "none" else None,
        hovermode="x unified",
        hoverlabel=dict(font=dict(size=13, family=_plotly_font_family()),    # â˜… ì—¬ê¸°
                        bgcolor="rgba(255,255,255,0.92)", bordercolor="rgba(0,0,0,0.1)"),
        modebar=dict(bgcolor="rgba(0,0,0,0)", color="#808B98", activecolor=ui["accent"]),
    )

    if title is not None:  # â† ì œëª©ì„ ìƒˆë¡œ ì¤„ ë•Œë§Œ ì„¸íŒ…
        layout_kwargs["title"] = dict(
            text=title, font=dict(size=22, family="Inter, Noto Sans KR"),
            x=0.0, xanchor="left", y=0.98, yanchor="top"
        )

    fig.update_layout(**layout_kwargs)

    # ì¶• ìŠ¤íƒ€ì¼
    fig.update_xaxes(title_font=dict(size=16), tickfont=dict(size=13),
                     showline=True, linewidth=1, linecolor="rgba(0,0,0,0.25)",
                     gridcolor="rgba(127,127,127,0.16)", zeroline=False)
    fig.update_yaxes(title_font=dict(size=16), tickfont=dict(size=13),
                     showline=True, linewidth=1, linecolor="rgba(0,0,0,0.25)",
                     gridcolor="rgba(127,127,127,0.16)", zeroline=True, zerolinewidth=1,
                     zerolinecolor="rgba(127,127,127,0.20)")

    # ë²”ë¡€ ì¤„ë°”ê¿ˆ ì—¬ìœ 
    if auto_legend_space and getattr(fig.layout, "legend", None) and getattr(fig.layout.legend, "orientation", "") == "h":
        import numpy as _np
        n_items = sum(1 for tr in fig.data if getattr(tr, "showlegend", True) and getattr(tr, "name", None))
        rows_est = int(_np.ceil((n_items or 1)/8))
        if rows_est > 1:
            extra = 28 * (rows_est - 1)
            fig.update_layout(margin=dict(l=fig.layout.margin.l, r=fig.layout.margin.r,
                                          b=fig.layout.margin.b, t=max(fig.layout.margin.t or 0, top_margin + extra)))
    return fig



VIZ_BG = {
    "map_total":     "#E8F0FE",   # êµ­ê°€ë³„ ì´ê³„ ì§€ë„ ì¹´ë“œ
    "map_wb":        "#F1ECE3",   # ICT ìœ í˜• ë‹¨ì¼ ì§€ë„ ì¹´ë“œ
    "donut_subj":    "#F6F7FB",   # ì£¼ì œ ë„ë„›
    "donut_wb":      "#E8F6EE",   # WB ë„ë„›
    "stack_100":     "#FFF7ED",   # 100% ëˆ„ì  ë§‰ëŒ€(ì£¼ì œÃ—WB)
    "year_subj":     "#FDF2F8",   # ì—°ë„ë³„ ì£¼ì œ ì‹œê°í™”
    "year_wb":       "#EEF2FF",   # ì—°ë„ë³„ WB ì‹œê°í™”
    "trend_up":      "#E2F2FF",   # í‚¤ì›Œë“œ ìƒìŠ¹ì„¸
    "trend_down":    "#FFE4E6",   # í‚¤ì›Œë“œ í•˜ë½ì„¸
    "theme_up":      "#E5F9F0",   # í…Œë§ˆ ìƒìŠ¹ì„¸
    "theme_down":    "#FFF1F2",   # í…Œë§ˆ í•˜ë½ì„¸
    "wc":            "#EEF1F6",   # ì›Œë“œí´ë¼ìš°ë“œ
    "bar_topk":      "#FAF7F2",   # Top-20 ê°€ë¡œë§‰ëŒ€
}



def render_wordcloud_png(freqs: dict, bg_color: str, alpha: float=0.5,
                         width: int=820, height: int=460, scale: int=2) -> bytes | None:
    """ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ë¥¼ PNG ë°”ì´íŠ¸ë¡œ ë°˜í™˜ (st.image ì•ˆì „ í‘œì‹œìš©)."""
    if not freqs or not WC_FONT_PATH:
        return None

    wc = WordCloud(
        width=width, height=height, scale=scale,
        mode="RGBA", background_color=None,
        max_words=220, prefer_horizontal=0.95,
        max_font_size=108, min_font_size=10,
        font_path=WC_FONT_PATH, random_state=42
    ).generate_from_frequencies(freqs)

    wc_img = wc.to_image().convert("RGBA")  # PIL.Image
    r, g, b = _hex_to_rgb(bg_color)
    base    = Image.new("RGBA", wc_img.size, (r, g, b, int(255*alpha)))
    mixed   = Image.alpha_composite(base, wc_img)

    buf = io.BytesIO()
    mixed.save(buf, format="PNG")  # â˜… í¬ë§· í™•ì •
    return buf.getvalue()


def auto_expand_top_margin_for_wrapped_legend(fig, base_top=100, items_per_row=8, extra_per_row=28):
    """legendë¥¼ ì´í›„ì— top/horizontalë¡œ ë³€ê²½í•œ ê²½ìš° ìƒë‹¨ì—¬ë°±ì„ ìë™ ì¦ë¶„."""
    import math
    leg = getattr(fig.layout, "legend", None)
    if not leg or getattr(leg, "orientation", "") != "h":
        return fig
    n_items = sum(1 for tr in fig.data if getattr(tr, "showlegend", True) and getattr(tr, "name", None))
    rows_est = math.ceil(n_items / max(1, items_per_row)) if n_items else 1
    if rows_est > 1:
        extra = extra_per_row * (rows_est - 1)
        cur = getattr(fig.layout.margin, "t", base_top) or base_top
        fig.update_layout(margin=dict(l=fig.layout.margin.l, r=fig.layout.margin.r,
                                      b=fig.layout.margin.b, t=max(cur, base_top + extra)))
    return fig

def force_legend_top_padding(fig, base_top=120,
                             items_per_row_hard=6,   # í•œ ì¤„ì— 6ê°œë§Œ ìˆ˜ìš©í•œë‹¤ê³  ê°€ì •(ë³´ìˆ˜ì )
                             char_unit=10.0,         # ë¼ë²¨ ê¸¸ì´ ë³´ì •(10ì â‰ˆ 1 ìœ ë‹›)
                             extra_per_row=40,       # ì¤„ ëŠ˜ ë•Œë§ˆë‹¤ ì¶”ê°€í•  ì—¬ë°±(px)
                             y_step=0.05):           # ì¤„ ëŠ˜ ë•Œë§ˆë‹¤ legend yë¥¼ ì–¼ë§ˆë‚˜ ë” ì˜¬ë¦´ì§€
    """
    - ì‹¤ì œ í™”ë©´í­ì„ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ 'í•­ëª© ìˆ˜ + ë¼ë²¨ ê¸¸ì´'ë¡œ ì¤„ ìˆ˜ë¥¼ ê³¼ëŒ€ì¶”ì •í•´ì„œ ì•ˆì „ ì—¬ë°±ì„ í™•ë³´.
    - items_per_row_hard=6 ìœ¼ë¡œ ë‚®ì¶° ë‘ ì¤„ íŒë‹¨ì„ ì‰½ê²Œ ë§Œë“¦(â‡’ í•­ìƒ ë„‰ë„‰í•œ top margin).
    """
    import math

    # ë²”ë¡€ í•­ëª© ìˆ˜ì§‘
    names = [getattr(tr, "name", None) for tr in fig.data if getattr(tr, "showlegend", True)]
    names = [n for n in names if n]
    if not names:
        # ê·¸ë˜ë„ ìµœì†Œ base_topì€ ë³´ì¥
        cur_t = getattr(fig.layout.margin, "t", 0) or 0
        if cur_t < base_top:
            fig.update_layout(margin=dict(l=fig.layout.margin.l, r=fig.layout.margin.r,
                                          b=fig.layout.margin.b, t=base_top))
        return fig

    # ìœ ë‹› ê³„ì‚°(í•­ëª© 1 + ë¼ë²¨ ê¸¸ì´ì— ë¹„ë¡€í•œ ë³´ì •)
    total_units = sum(1.0 + (len(str(n)) / char_unit) for n in names)

    # ë³´ìˆ˜ì  ì¤„ìˆ˜ ì¶”ì •
    rows_est = int(math.ceil(total_units / max(1e-6, items_per_row_hard)))
    rows_est = max(rows_est, 1)

    # ì›í•˜ëŠ” top margin & legend y
    want_top = base_top + (rows_est - 1) * extra_per_row
    y_target = 1.10 + (rows_est - 1) * y_step

    cur_t = getattr(fig.layout.margin, "t", 0) or 0
    fig.update_layout(
        legend=dict(orientation="h", y=y_target, yanchor="bottom", x=0, xanchor="left"),
        margin=dict(l=fig.layout.margin.l, r=fig.layout.margin.r,
                    b=fig.layout.margin.b, t=max(cur_t, want_top))
    )
    return fig

# ---- PATCH D: Tabbed detail panel ----
if mode == "êµ­ê°€ë³„ ì´ê³„":
    st.subheader("ìƒì„¸ íŒ¨ë„")
    if clicked_iso:
        sub = dfx[dfx["iso3"]==clicked_iso].copy()
        if not sub.empty:
            country_name = sub["country_ko"].iloc[0]
            st.markdown(f"### {country_name} â€” í”„ë¡œì íŠ¸ {sub['íŒŒì¼ëª…'].nunique()}ê±´")

            tab_overview, tab_cloud, tab_table = st.tabs(["ê°œìš”", "ì›Œë“œí´ë¼ìš°ë“œ / í‚¤ì›Œë“œ", "í…Œì´ë¸”"])

            with tab_overview:
                st.markdown("#### êµ­ê°€ ë¸Œë¦¬í”„")
                if isinstance(briefs_map, dict) and briefs_map:
                    iso = clicked_iso
                    brief_txt = briefs_map.get(iso) or briefs_map.get(sub["country_en"].iloc[0], None) or briefs_map.get(country_name, None)
                    st.write(brief_txt if brief_txt else "ë¸Œë¦¬í”„ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.info("ì¢Œì¸¡ì—ì„œ CountryBriefs.ipynbë¥¼ ì§€ì •í•˜ì„¸ìš”.")
                st.divider()

                st.markdown("#### í•µì‹¬ ì§€í‘œ")
                _suby = expand_years(sub)  # ê¸°ì¡´ì— ì •ì˜ëœ í•¨ìˆ˜
                sub_years = sorted(set(_suby["ì—°ë„"].dropna().astype(int).tolist()))
                cA, cB, cC = st.columns(3)
                with cA: st.metric("ì—°ë„ ë²”ìœ„", f"{min(sub_years) if sub_years else '-'}â€“{max(sub_years) if sub_years else '-'}")
                with cB: st.metric("ICT ìœ í˜• ê³ ìœ ", f"{sub['ICT ìœ í˜•'].astype(str).str.strip().nunique():,}")
                with cC: st.metric("ëŒ€ìƒê¸°ê´€ ìˆ˜", f"{sub['ëŒ€ìƒê¸°ê´€'].nunique():,}")

            with tab_cloud:
                st.markdown("#### ì›Œë“œí´ë¼ìš°ë“œ (í•´ì‹œíƒœê·¸ + ìš”ì•½/ë‚´ìš©)")
                # 0) í† í° ìˆ˜ì§‘ (í•´ì‹œíƒœê·¸ + ìš”ì•½/ë‚´ìš©)
                tokens: list[str] = []
                if "Hashtag_str" in sub.columns and sub["Hashtag_str"].notna().any():
                    for txt in sub["Hashtag_str"].dropna().astype(str):
                        tokens += [z.strip() for z in re.split(r"[;,]", txt) if z.strip()]
                elif "Hashtag" in sub.columns and sub["Hashtag"].notna().any():
                    for txt in sub["Hashtag"].dropna().astype(str):
                        tokens += [z.strip() for z in re.split(r"[;,]", txt) if z.strip()]

                pool_cols = [c for c in ["ìš”ì•½", "ì£¼ìš” ë‚´ìš©"] if c in sub.columns]
                if pool_cols:
                    for txt in sub[pool_cols].fillna("").astype(str).agg(" ".join, axis=1).tolist():
                        for w in re.split(r"[^0-9A-Za-zê°€-í£]+", txt):
                            w = w.strip()
                            if len(w) >= 2:
                                tokens.append(w)

                # 1) ì •ì œ
                tokens = [
                    w for w in tokens
                    if w and w.lower() not in STOP_LOW and not re.fullmatch(r"\d+(\.\d+)?", w)
                ]
                freq = Counter(tokens)
                top_freqs = dict(freq.most_common(220))
                top20 = freq.most_common(10)

                # 2) 2ì—´ ë°°ì¹˜ (ì›Œë“œí´ë¼ìš°ë“œ : ë§‰ëŒ€ = 6 : 7)
                lc, rc = st.columns([6, 7], gap="large")

                # Left) ì›Œë“œí´ë¼ìš°ë“œ â€” â€œì ë‹¹íˆ í¼ + ì„ ëª…â€, ì»¬ëŸ¼ í­ì— ë§ì¶° ìë™ ë§ì¶¤
                with lc:
                    st.markdown("**ì›Œë“œí´ë¼ìš°ë“œ**")
                    if top_freqs:
                        font_path = find_korean_font()
                        # ë°ì€/ì–´ë‘ìš´ í…Œë§ˆ ìë™ ë°°ê²½
                        bg = "white" if ui.get("plotly_template", "plotly_white") == "plotly_white" else ui.get("card", "#0f1115")
                        # (ì™¼ìª½) ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±

                        

                        png_bytes = render_wordcloud_png(top_freqs, bg_color=VIZ_BG["wc"], alpha=0.5)
                        if png_bytes:
                            st.image(png_bytes, use_container_width=True, output_format="PNG")  # â˜… ì•ˆì „
                        else:
                            if not WC_FONT_PATH:
                                st.error("ì›Œë“œí´ë¼ìš°ë“œìš© í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë¦¬í¬ì— assets/fonts/NanumGothic.ttf ì¶”ê°€ ë˜ëŠ” packages.txtì— fonts-nanum)")
                            else:
                                st.info("í‘œì‹œí•  ë‹¨ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

                        

                        # ìƒìœ„ í‚¤ì›Œë“œ ì¹©(ë¹ ë¥¸ ìŠ¤ìº”ìš©, 12ê°œ)
                        chips = " ".join([f'<span class="ksp-chip">{k}</span>' for k, _ in freq.most_common(12)])
                        st.markdown(chips, unsafe_allow_html=True)
                    else:
                        st.info("í‘œì‹œí•  ë‹¨ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

                # Right) Top-20 ê°€ë¡œë§‰ëŒ€ â€” ë¼ë²¨ ì˜ë¦¼ ë°©ì§€ + ê°’ ì™¸ë¶€í‘œì‹œ
                with rc:
                    st.markdown("**ìƒìœ„ í‚¤ì›Œë“œ Top-10**")
                    if top20:
                        bar_df = pd.DataFrame(top20, columns=["í‚¤ì›Œë“œ", "ë¹ˆë„"])
                        fig_bar = px.bar(
                            bar_df.sort_values("ë¹ˆë„"),
                            x="ë¹ˆë„", y="í‚¤ì›Œë“œ", orientation="h", text="ë¹ˆë„"
                        )
                        fig_bar = style_fig(fig_bar, "Top-10 í‚¤ì›Œë“œ", legend="none", top_margin=64,
                        bg_color=VIZ_BG["bar_topk"], bg_alpha=0.5)
                        fig_bar.update_traces(textposition="outside", cliponaxis=False)
                        fig_bar.update_xaxes(title_text="ë¹ˆë„")
                        fig_bar.update_yaxes(title_text=None)
                        st.plotly_chart(style_fig(fig_bar, "Top-10 í‚¤ì›Œë“œ", legend="none", top_margin=64),
                                        use_container_width=True, config={"displayModeBar": False})
                    else:
                        st.info("í‘œì‹œí•  í‚¤ì›Œë“œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

            with tab_table:
                st.markdown("#### í”„ë¡œì íŠ¸ ëª©ë¡")
                cols_show = ["íŒŒì¼ëª…","ì§€ì›ê¸°ê´€","ì—°ë„","ì£¼ì œë¶„ë¥˜(ëŒ€)", "ICT ìœ í˜•","ì£¼ìš” ë‚´ìš©","ê¸°ëŒ€ íš¨ê³¼","Hashtag_str"]
                st.dataframe(sub[cols_show].drop_duplicates().reset_index(drop=True), use_container_width=True)
    else:
        st.info("ìƒë‹¨ ì§€ë„ì—ì„œ êµ­ê°€ë¥¼ í´ë¦­í•˜ë©´ ìƒì„¸ê°€ ì—´ë¦½ë‹ˆë‹¤.")

# ===================== â‘¡ ICT ìœ í˜• ë‹¨ì¼í´ë˜ìŠ¤ (ì§€ë„ë¥¼ êµ­ê°€ í•˜ì´ë¼ì´íŠ¸ë¡œë§Œ ì‚¬ìš©, ìƒì„¸ëŠ” 'í´ë˜ìŠ¤ ì „ì²´' ê¸°ì¤€) =====================
elif mode == "ICT ìœ í˜• ë‹¨ì¼í´ë˜ìŠ¤":
    st.subheader("ICT ìœ í˜• ë‹¨ì¼í´ë˜ìŠ¤ í”„ë¡œì íŠ¸ ìˆ˜")

    # 1) í´ë˜ìŠ¤ ì„ íƒ
    wb_classes = [c for c in sorted(df["ICT ìœ í˜•"].astype(str).str.strip().dropna().unique()) if c and c != "nan"]
    if not wb_classes:
        st.info("ICT ìœ í˜• ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    sel = st.selectbox("ICT ìœ í˜• ì„ íƒ", wb_classes, index=0, key="wb_class_select_main")

    # 2) ì§€ë„(ê°œìš”): ì´ Classê°€ ìˆ˜í–‰ëœ 'êµ­ê°€ í•˜ì´ë¼ì´íŠ¸'ë§Œ, í´ë¦­ì€ ì§‘ê³„ì— ì˜í–¥ X
    sub_wb_geo = dfx[dfx["ICT ìœ í˜•"].astype(str).str.strip() == sel]  # ì§€ë„ìš©(êµ­ê°€ í™•ì¥ë³¸ ì‚¬ìš©)
    agg_geo = sub_wb_geo.groupby(["iso3", "country_ko"], as_index=False).agg(n=("íŒŒì¼ëª…", "nunique"))
    value_map = {r.iso3: int(r.n) for _, r in agg_geo.iterrows()}
    gj = augment_geojson_values(world_geojson, key_on_info, value_map, "ksp_wb_cnt")

    base = make_base_map()
    ch = folium.Choropleth(
        geo_data=gj, data=agg_geo, columns=["iso3","n"],
        key_on=key_on_info[0],
        fill_color="PuBuGn", fill_opacity=0.90,
        line_opacity=0.5, line_color="#888",
        nan_fill_color="#fbfbfb", legend_name=f"{sel} ê±´ìˆ˜", highlight=True
    ); ch.add_to(base)
    ch.geojson.add_child(folium.features.GeoJsonTooltip(
        fields=["ISO3", "name" if "name" in gj["features"][0]["properties"] else "ISO3", "ksp_wb_cnt"],
        aliases=["ISO3","êµ­ê°€","ê±´ìˆ˜"], sticky=False
    ))
    ch.geojson.add_child(folium.features.GeoJsonPopup(fields=["ISO3"], aliases=["ISO3"]))
    # í´ë¦­ì€ ë³´ì¡° ì •ë³´ë¡œë§Œ ì‚¬ìš©(ì„ íƒêµ­ê°€ í‘œì‹œì—ë§Œ ì“°ê³ , ë³¸ë¬¸ ì§‘ê³„ì—ëŠ” ì˜í–¥ X)
    ret = st_folium(base, height=520, use_container_width=True)
    clicked_iso = extract_iso_from_stfolium(ret)

    # 3) ìƒì„¸ íŒ¨ë„ â€” â˜… í•µì‹¬: 'í´ë˜ìŠ¤ ì „ì²´' ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„/ì‹œê°í™” â˜…
    st.subheader("ìƒì„¸ íŒ¨ë„ â€” ICT ìœ í˜•")

    # ë³¸ë¬¸ ì§‘ê³„ìš©ì€ 'êµ­ê°€ í™•ì¥ ì—†ëŠ” ì›ë³¸ df'ì—ì„œ í•„í„° (ë™ì¼ ë³´ê³ ì„œê°€ ë‹¤êµ­ê°€ì— ì¤‘ë³µ ì§‘ê³„ë˜ëŠ” ë¬¸ì œ ë°©ì§€)
    sub_wb = df[df["ICT ìœ í˜•"].astype(str).str.strip() == sel].copy()

    # ìƒë‹¨ íƒ€ì´í‹€ + ë©”íŠ¸ë¦­
    n_docs = sub_wb["íŒŒì¼ëª…"].nunique()
    part_countries = sub_wb_geo["iso3"].nunique()  # ì°¸ì—¬êµ­ê°€ ìˆ˜(ì§€ë„ìš© í™•ì¥ dfë¡œ ê³„ì‚°)
    st.markdown(f"### {sel} â€” ì „ì²´ í”„ë¡œì íŠ¸ {n_docs:,}ê±´ Â· ì°¸ì—¬êµ­ê°€ {part_countries:,}ê°œêµ­")

    tab_overview, tab_brief, tab_cloud, tab_table = st.tabs(
        ["ê°œìš”", f"{sel} ì¢…í•©ìš”ì•½", "ì›Œë“œí´ë¼ìš°ë“œ / í‚¤ì›Œë“œ", "í…Œì´ë¸”"]
    )

    # ---- (1) ê°œìš”: ì—°ë„ ë²”ìœ„, ëŒ€ìƒê¸°ê´€ ìˆ˜, ì°¸ì—¬êµ­ê°€ ìƒìœ„ ë³´ê¸°(ì„ íƒ êµ­ê°€ ë³´ì¡° í‘œê¸°) ----
    with tab_overview:
        _suby = expand_years(sub_wb)
        sub_years = sorted(set(_suby["ì—°ë„"].dropna().astype(int).tolist()))
        cA, cB, cC = st.columns(3)
        with cA:
            st.metric("ì—°ë„ ë²”ìœ„", f"{min(sub_years) if sub_years else '-'}â€“{max(sub_years) if sub_years else '-'}")
        with cB:
            st.metric("í”„í† ì íŠ¸ ìˆ˜", f"{n_docs:,}")
        with cC:
            st.metric("ëŒ€ìƒê¸°ê´€ ìˆ˜", f"{sub_wb['ëŒ€ìƒê¸°ê´€'].nunique():,}")

        # ì°¸ì—¬êµ­ê°€ Top-10 (í”„ë¡œì íŠ¸ ìˆ˜ ê¸°ì¤€)
        st.markdown("#### ì°¸ì—¬êµ­ê°€ (í”„ë¡œì íŠ¸ ìˆ˜ Top 10)")
        top_c = (sub_wb_geo.groupby(["country_ko"], as_index=False)
                          .agg(ê±´ìˆ˜=("íŒŒì¼ëª…","nunique"))
                          .sort_values("ê±´ìˆ˜", ascending=False).head(10))
        if clicked_iso:
            # í´ë¦­í•œ êµ­ê°€ê°€ ìˆìœ¼ë©´ ì¹©ìœ¼ë¡œ ë³´ì¡° í‘œê¸°
            iso_name = sub_wb_geo.loc[sub_wb_geo["iso3"]==clicked_iso, "country_ko"]
            if len(iso_name):
                st.caption(f"ì§€ë„ë¡œ ì„ íƒëœ êµ­ê°€: **{iso_name.iloc[0]}** (ì§‘ê³„ì—ëŠ” ì˜í–¥ ì—†ìŒ)")

        st.dataframe(top_c.reset_index(drop=True), use_container_width=True)

    # ---- (2) ì¢…í•©ìš”ì•½: WB_ClassBriefs.ipynbì—ì„œ sel í‚¤ë¡œ ë¡œë“œ ----
    with tab_brief:
        if isinstance(wb_briefs_map, dict) and wb_briefs_map:
            # í‚¤ ë§¤ì¹­(ëŒ€ì†Œë¬¸ì/ê³µë°± ë¬´ì‹œ)
            key_fold = next((k for k in wb_briefs_map.keys()
                             if str(k).strip().lower() == str(sel).strip().lower()), None)
            brief_txt = wb_briefs_map.get(sel) or (wb_briefs_map.get(key_fold) if key_fold else None)
            st.write(brief_txt if brief_txt else f"'{sel}' ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤. WB_ClassBriefs.ipynbì— ì¶”ê°€í•˜ì„¸ìš”.")
        else:
            st.info("ì¢Œì¸¡ì—ì„œ WB_ClassBriefs.ipynbë¥¼ ì§€ì •í•˜ì„¸ìš”.")

    # ---- (3) ì›Œë“œí´ë¼ìš°ë“œ/í‚¤ì›Œë“œ: í´ë˜ìŠ¤ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ìƒì„± (êµ­ê°€ ë¬´ê´€) ----
    with tab_cloud:
        tokens: list[str] = []
        if "Hashtag_str" in sub_wb.columns and sub_wb["Hashtag_str"].notna().any():
            for txt in sub_wb["Hashtag_str"].dropna().astype(str):
                tokens += [z.strip() for z in re.split(r"[;,]", txt) if z.strip()]
        elif "Hashtag" in sub_wb.columns and sub_wb["Hashtag"].notna().any():
            for txt in sub_wb["Hashtag"].dropna().astype(str):
                tokens += [z.strip() for z in re.split(r"[;,]", txt) if z.strip()]

        pool_cols = [c for c in ["ìš”ì•½", "ì£¼ìš” ë‚´ìš©"] if c in sub_wb.columns]
        if pool_cols:
            for txt in sub_wb[pool_cols].fillna("").astype(str).agg(" ".join, axis=1).tolist():
                for w in re.split(r"[^0-9A-Za-zê°€-í£]+", txt):
                    w = w.strip()
                    if len(w) >= 2:
                        tokens.append(w)

        tokens = [w for w in tokens if w and w.lower() not in STOP_LOW and not re.fullmatch(r"\d+(\.\d+)?", w)]
        freq   = Counter(tokens)
        top20  = freq.most_common(10)
        top_freqs = dict(freq.most_common(220))

        lc, rc = st.columns([6, 7], gap="large")
        with lc:
            st.markdown("**ì›Œë“œí´ë¼ìš°ë“œ**")
            if top_freqs:
                png_bytes = render_wordcloud_png(top_freqs, bg_color=VIZ_BG["wc"], alpha=0.5)
                if png_bytes:
                    st.image(png_bytes, use_container_width=True, output_format="PNG")  # â˜… ì•ˆì „
                else:
                    if not WC_FONT_PATH:
                        st.error("ì›Œë“œí´ë¼ìš°ë“œìš© í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë¦¬í¬ì— assets/fonts/NanumGothic.ttf ì¶”ê°€ ë˜ëŠ” packages.txtì— fonts-nanum)")
                    else:
                        st.info("í‘œì‹œí•  ë‹¨ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                chips = " ".join([f'<span class="ksp-chip">{k}</span>' for k, _ in freq.most_common(12)])
                st.markdown(chips, unsafe_allow_html=True)
            else:
                st.info("í‘œì‹œí•  ë‹¨ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        with rc:
            st.markdown("**ìƒìœ„ í‚¤ì›Œë“œ Top-10**")
            if top20:
                bar_df = pd.DataFrame(top20, columns=["í‚¤ì›Œë“œ","ë¹ˆë„"])
                fig_bar = px.bar(bar_df.sort_values("ë¹ˆë„"), x="ë¹ˆë„", y="í‚¤ì›Œë“œ",
                                 orientation="h", text="ë¹ˆë„")
                fig_bar = style_fig(fig_bar, f"Top-10 í‚¤ì›Œë“œ ({sel})",
                                    legend="none", top_margin=64,
                                    bg_color=VIZ_BG["bar_topk"], bg_alpha=0.5)
                fig_bar.update_traces(textposition="outside", cliponaxis=False)
                fig_bar.update_xaxes(title_text="ë¹ˆë„"); fig_bar.update_yaxes(title_text=None)
                st.plotly_chart(fig_bar, use_container_width=True, config={"displayModeBar": False})
            else:
                st.info("í‘œì‹œí•  í‚¤ì›Œë“œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

    # ---- (4) í…Œì´ë¸”: í´ë˜ìŠ¤ ì „ì²´ ë³´ê³ ì„œ ëª©ë¡ ----
    with tab_table:
        st.markdown("#### í”„ë¡œì íŠ¸ ëª©ë¡ (í´ë˜ìŠ¤ ì „ì²´)")
        cols_show = ["íŒŒì¼ëª…","ì§€ì›ê¸°ê´€","ì—°ë„","ì£¼ì œë¶„ë¥˜(ëŒ€)","ICT ìœ í˜•","ëŒ€ìƒêµ­","ëŒ€ìƒê¸°ê´€","ì£¼ìš” ë‚´ìš©","ê¸°ëŒ€ íš¨ê³¼","Hashtag_str"]
        st.dataframe(sub_wb[cols_show].drop_duplicates().reset_index(drop=True), use_container_width=True)




# --------------------- ì „ì²´ ë¶„í¬ ëŒ€ì‹œë³´ë“œ ---------------------
st.markdown("---")
st.subheader("ì „ì²´ ë¶„í¬ ëŒ€ì‹œë³´ë“œ")

# (1) ì£¼ì œë¶„ë¥˜(ëŒ€) ë„ë„›
subj_counts = df["ì£¼ì œë¶„ë¥˜(ëŒ€)"].fillna("ë¯¸ë¶„ë¥˜").value_counts().reset_index()
subj_counts.columns = ["ì£¼ì œë¶„ë¥˜(ëŒ€)","count"]
fig1 = px.pie(subj_counts, names="ì£¼ì œë¶„ë¥˜(ëŒ€)", values="count", hole=0.55)
# ë„ë„›
fig1 = style_fig(fig1, "ì£¼ì œë¶„ë¥˜(ëŒ€) ë¶„í¬", legend="right", top_margin=120,
                 bg_color=VIZ_BG["donut_subj"], bg_alpha=0.5)
# (2) ICT ìœ í˜• ë„ë„›
wb_counts = (df["ICT ìœ í˜•"].astype(str).str.strip().replace({"nan":"ë¯¸ë¶„ë¥˜"})
             .fillna("ë¯¸ë¶„ë¥˜").value_counts().reset_index())
wb_counts.columns = ["ICT ìœ í˜•","count"]
fig2 = px.pie(wb_counts, names="ICT ìœ í˜•", values="count", hole=0.55)
fig2 = style_fig(fig2, "ICT ìœ í˜• ë¶„í¬", legend="right", top_margin=120,
                 bg_color=VIZ_BG["donut_wb"], bg_alpha=0.5)

c0, c00 = st.columns([1,1], gap="large")
with c0: st.plotly_chart(fig1, use_container_width=True)
with c00: st.plotly_chart(fig2, use_container_width=True)

# (3) ì£¼ì œÃ—WB 100% ëˆ„ì  ë§‰ëŒ€
cross = (df.assign(WB=df["ICT ìœ í˜•"].astype(str).str.strip().replace({"nan":"ë¯¸ë¶„ë¥˜"}).fillna("ë¯¸ë¶„ë¥˜"))
           .groupby(["ì£¼ì œë¶„ë¥˜(ëŒ€)","WB"], as_index=False).size())
pivot = cross.pivot(index="ì£¼ì œë¶„ë¥˜(ëŒ€)", columns="WB", values="size").fillna(0)
pivot_pct = pivot.div(pivot.sum(axis=1).replace(0, np.nan), axis=0).fillna(0).reset_index().melt(
    id_vars="ì£¼ì œë¶„ë¥˜(ëŒ€)", var_name="WB", value_name="pct")
fig3 = px.bar(pivot_pct, x="ì£¼ì œë¶„ë¥˜(ëŒ€)", y="pct", color="WB", barmode="stack")
fig3.update_yaxes(range=[0,1], tickformat=".0%")
fig3.update_layout(bargap=0.68, bargroupgap=0.08)   # ê°’â†‘ = ê°„ê²©â†‘ = ë§‰ëŒ€ ìŠ¬ë¦¼


# 100% ëˆ„ì  ë§‰ëŒ€
st.plotly_chart(style_fig(fig3, "ì£¼ì œë¶„ë¥˜(ëŒ€)ë³„ ICT ìœ í˜• ë¹„ì¤‘ (100%)",
                          legend="right", top_margin=120,
                          bg_color=VIZ_BG["stack_100"], bg_alpha=0.5),
                use_container_width=True)

# ---------- (4)(5) ì—°ë„ë³„ ë¹„ì¤‘ â€” ì„ íƒí˜• ì‹œê°í™” (íˆíŠ¸ë§µ ì œê±°) ----------
dfy_valid = dfy.dropna(subset=["ì—°ë„"]).copy()

def time_share(df_in, group_col):
    g = df_in.groupby(["ì—°ë„", group_col], as_index=False).size()
    totals = g.groupby("ì—°ë„")["size"].transform("sum")
    g["pct"] = g["size"] / totals
    return g

def draw_year_chart(g, group_col, title_prefix):
    if g.empty:
        fig = px.line(); return style_fig(fig, f"{title_prefix} (ì—°ë„ ì¶”ì¶œ ë¶ˆê°€)")

    if year_mode == "ìˆœìœ„ Bump":
        ranks = g.copy()
        ranks["rank"] = ranks.groupby("ì—°ë„")["pct"].rank(ascending=False, method="dense")
        fig = px.line(ranks, x="ì—°ë„", y="rank", color=group_col, markers=True)
        fig.update_traces(line=dict(width=3), marker=dict(size=8))
        fig.update_yaxes(autorange="reversed", dtick=1, title="ìˆœìœ„(1=ìµœìƒ)")
        return style_fig(fig, f"{title_prefix} â€” ìˆœìœ„ Bump", legend="top", top_margin=120)
    else:  # 100% ëˆ„ì  ë§‰ëŒ€
        # fig = px.bar(g, x="ì—°ë„", y="pct", color=group_col, barmode="stack", labels={"pct":"ë¹„ì¤‘"})
        # fig.update_yaxes(range=[0,1], tickformat=".0%")
        # return style_fig(fig, f"{title_prefix} â€” 100% ëˆ„ì  ë§‰ëŒ€", legend="top", top_margin=120)
        
        fig = px.line(g, x="ì—°ë„", y="pct", color=group_col, labels={"pct": "ë¹„ì¤‘"}, markers=True)  # ê° ì ì„ ë™ê·¸ë¼ë¯¸ë¡œ í‘œì‹œ
        fig.update_yaxes(range=[0, 1], tickformat=".0%")
        fig.update_layout(title="ë¹„ìœ¨ ì¶”ì„¸ (ë¼ì¸ í”Œë¡¯)", legend=dict(orientation="h", y=1.1))
        return style_fig(fig, f"{title_prefix} â€” ë¹„ì¤‘ Bump", legend="top", top_margin=120)

if not dfy_valid.empty:
    g_subj = time_share(dfy_valid, "ì£¼ì œë¶„ë¥˜(ëŒ€)")
    g_wb   = time_share(dfy_valid.assign(WB=dfy_valid["ICT ìœ í˜•"].astype(str).str.strip().replace({"nan":"ë¯¸ë¶„ë¥˜"}).fillna("ë¯¸ë¶„ë¥˜")), "WB")
else:
    g_subj = pd.DataFrame(columns=["ì—°ë„","ì£¼ì œë¶„ë¥˜(ëŒ€)","size","pct"])
    g_wb   = pd.DataFrame(columns=["ì—°ë„","WB","size","pct"])

fig4 = draw_year_chart(g_subj, "ì£¼ì œë¶„ë¥˜(ëŒ€)", "ì—°ë„ë³„ ì£¼ì œë¶„ë¥˜(ëŒ€) ë¹„ì¤‘")
fig5 = draw_year_chart(g_wb, "WB", "ì—°ë„ë³„ ICT ìœ í˜• ë¹„ì¤‘")
c1, c2 = st.columns([1,1], gap="large")
with c1: st.plotly_chart(fig4, use_container_width=True)
with c2: st.plotly_chart(fig5, use_container_width=True)

# =====================================================================
# ì¶”ê°€ ì‹œê°í™” â‘ : ëŒ€í‘œ í‚¤ì›Œë“œ ìƒëŒ€ íŠ¸ë Œë“œ(ìƒìŠ¹ì„¸/í•˜ë½ì„¸)  â€” Plotly
# =====================================================================
st.markdown("---")
st.subheader("AI ì¶”ì¶œ í‚¤ì›Œë“œ ìƒëŒ€ íŠ¸ë Œë“œ (ìƒìŠ¹/í•˜ë½)")

# 'ëŒ€í‘œ í‚¤ì›Œë“œ ìƒëŒ€ íŠ¸ë Œë“œ'ì— ì¶”ê°€ ì ìš©í•  ë¶ˆìš©ì–´
BASE_STOP = { 
    "ê²½ì œ","ì‚¬íšŒ","ì‚¬íšŒì •ì±…","ì •ì±…","ë°ì´í„°","ë””ì§€í„¸","ì„œë¹„ìŠ¤","ì‹œì¥","ìš´ì˜","í˜„í™©","ì „ëµ","ë°©ì•ˆ","ë„ì…",
    "ê°œì„ ","êµ¬ì¶•","ì²´ê³„","ê¸°ë°˜","ì¤‘ì¥ê¸°","ìµœì¢…ë³´ê³ ","ì¤‘ê°„ë³´ê³ ","ë¶„ì„","ì§€ì›","ì •ë¶€","ê³µê³µ","êµ­ê°€","ì°¨ì„¸ëŒ€",
    "í‰ê°€","í”„ë¡œì íŠ¸","ë¡œë“œë§µ","ë¹„ì „","í™œìš©","ê°•í™”","í™•ëŒ€","ì˜ˆì •","ì—°êµ¬","ì‚¬ë¡€","í˜„ì§€","ì •í•©ì„±","ìˆ˜ë¦½",
    "ë§ˆìŠ¤í„°í”Œëœ","ê°œí¸","ê³ ë„í™”","ê°œì •","ê°œë°œ","ì—…ê·¸ë ˆì´ë“œ","ì ìš©","ì‹œë²”","ì»¨ì„¤íŒ…","í˜‘ë ¥","ì •ë¹„","ë„ì‹œ",
    "ì¸í”„ë¼","í”Œë«í¼","í”Œë ›í¼","ì‹œìŠ¤í…œ","í¬í„¸","ì¡°ë‹¬","ë²•ì œ","ì œë„","ê°€ì´ë“œë¼ì¸","ê¸°íš","ì¶”ì§„","ì„±ê³¼",
    "í˜„ì•ˆ","ê³¼ì œ","ê¸°ìˆ ","ê³„íš","ìë£Œ","ë³´ê³ ","ìš”ì•½","ì¥ë‹¨ì ","í•œê³„","ê²½ë³´","ì•ˆì „","ë³´ì•ˆ","ì„±ì¥",
    "ì „ìì„¸ê¸ˆê³„ì‚°ì„œ","ë²•ì ","ì˜ë¬´í™”","ì˜ˆì‚°","ê°€ë­„","êµìœ¡","ê°œì¸ì •ë³´ë³´í˜¸","vat","ê±´ì„¤","vision","ì„¸ì •","ë‹¤ì¸µ","ë¯¼ê°„","ê·¼ê±°",
    "ì‚°ì—…","ì„¸ìˆ˜","ì„¸ë¬´ì¡°ì§","ì¬ì •","ì¸ì‚¬","ì¬ë¬´ë¶€","íˆ¬ì","í†µí•©","í›ˆë ¨","í™ë³´","ì¡°ì •","ë¬´ì—­","í™ìˆ˜","í´ë¼ìš°ë“œ","ë°ì´í„°ì„¼í„°",
    "ì „ìì •ë¶€","ì¶”ì •","ì†ŒìŠ¤","ì½˜í…ì¸ ", "ì¡°ì„¸", "ì˜ë£Œ", "êµí†µ", "ip", "Ip", "ì¸ì¦", "í˜ê¸°ë¬¼", "ë‚©ì„¸ì", "ì˜ì•½í’ˆ", "ìƒì‚°ì„±",
    "ì „ì", "ê°ì‚¬", "ê³µë¬´ì›ì˜", "ë“±ë¡", "ì§‘í–‰", "ì‚¬ì´ë²„", "ì¡°ì„¸í–‰ì •", "ë†’ì—¬", "ì›ê²©", "ì‚¬ìš©ì", "ì½œì„¼í„°", "ê¸°ê´€ë³„", "ì—ë„ˆì§€", "ì „ìì¡°ë‹¬", "ê¸ˆìœµ", "ë‚©ì„¸", "ì •ë³´í™”",
    # ì¶•/ë¼ë²¨ ê´€ë ¨ ë¶ˆìš©ì–´ ì¶”ê°€
    "ì—°ë„","ë…„ë„","year","years",
    # ì˜ë¬¸ ìƒíˆ¬ì–´
    "and","or","of","in","to","for","the","with","on","by","from","eu",
    "data","digital","service","services","policy","strategy","plan","roadmap","project","program",
    "system","platform","portal","model","evaluation","improvement","implementation","phase","final","interim",
    "procurement"
}

BASE_STOP_LOW = {s.lower() for s in BASE_STOP}
 

# ---- ê³ ì • íŒŒë¼ë¯¸í„° (ìŠ¬ë¼ì´ë” ì œê±°)
TOP_K_PER_FIG = 25   # ìƒìŠ¹/í•˜ë½ ê°ê° í‘œê¸° í‚¤ì›Œë“œ ìˆ˜
ROLL = 5             # Jeffreys + ë¡¤ë§ ìœˆë„(ë…„)
ALPHA = 0.7
WINDOW_YEARS = 10
RECENT_YEARS = 5
MIN_DOCS_BASE, MIN_YEARS_BASE = 4, 3
RECENT_DOCS_MIN, RECENT_YEARS_MIN = 2, 2

HASHTAG_COL = "Hashtag" if "Hashtag" in df.columns else ("Hashtag_str" if "Hashtag_str" in df.columns else None)

def clean(s): return s.astype(str).str.replace(r"\s+"," ",regex=True).str.strip()

YEAR_RE = re.compile(r"(?<!\d)(?:19|20)\d{2}(?!\d)")

def years_from_span(text: str):
    if not isinstance(text, str): return []
    t = text.replace("~","-").replace("â€“","-").replace("â€”","-")
    t = re.sub(r"[()]", " ", t)
    ys = [int(y) for y in YEAR_RE.findall(t)]
    return list(range(min(ys), max(ys)+1)) if ys else []

SYN = {"sme":"SME","pki":"PKI","ai":"AI","ict":"ICT","bigdata":"ë¹…ë°ì´í„°","big data":"ë¹…ë°ì´í„°",
       "e-gp":"ì „ìì¡°ë‹¬","egp":"ì „ìì¡°ë‹¬","e-procurement":"ì „ìì¡°ë‹¬","data center":"ë°ì´í„°ì„¼í„°","cloud":"í´ë¼ìš°ë“œ",
       "platform":"í”Œë«í¼","platfrom":"í”Œë«í¼","í”Œë ›í¼":"í”Œë«í¼"}

def norm_token(x: str) -> str:
    x = re.sub(r"[\"'â€™â€œâ€()\[\]{}<>]", "", x.strip()); xl = x.lower()
    return SYN.get(xl, x)

def split_hashtags(s, stopset):
    if not isinstance(s,str) or not s.strip(): return []
    raw = re.split(r"[,\;/]| {2,}", s)
    out=[]
    for t in raw:
        t = norm_token(t)
        core = re.sub(r"\s+", "", t.lower())
        if not core or re.fullmatch(r"[\W_]+", core) or re.fullmatch(r"\d+(\.\d+)?", core):
            continue
        if len(core) < 2: continue
        if core in stopset: continue
        out.append(t)
    return sorted(set(out), key=str.lower)

def jeffreys_rolling_ratio(num, den, k=ROLL, alpha=ALPHA):
    numr = (num + alpha).rolling(k, center=True, min_periods=1).sum()
    denr = (den + 2*alpha).rolling(k, center=True, min_periods=1).sum()
    return (numr/denr*100.0).fillna(0.0)

@st.cache_data(show_spinner=False)
def build_keyword_time(df_in: pd.DataFrame, stop_extra: set):
    df_local = df_in.copy()
    if HASHTAG_COL:
        df_local[HASHTAG_COL] = clean(df_local[HASHTAG_COL])
    years_list = df_local["ì—°ë„"].apply(years_from_span)
    all_years  = sorted({y for ys in years_list for y in (ys or [])})
    if not all_years: return [], {}, pd.Series([], dtype=int), pd.DataFrame()

    # ë™ì  ë¶ˆìš©ì–´(ëŒ€ë¶„ë¥˜/í´ë˜ìŠ¤/êµ­ê°€ ë“±)
    dyn = set()
    for col in ["ì£¼ì œë¶„ë¥˜(ëŒ€)","ICT ìœ í˜•","ëŒ€ìƒêµ­","ëŒ€ìƒê¸°ê´€","ì§€ì›ê¸°ê´€"]:
        if col in df_local.columns: dyn |= set(map(str.lower, df_local[col].astype(str).unique()))
    stopset = {w.lower() for w in stop_extra} | dyn

    tokens_by_row = [split_hashtags(s, stopset) for s in (df_local[HASHTAG_COL] if HASHTAG_COL else pd.Series([""]*len(df_local)))]

    docs_per_year = pd.Series(0, index=all_years, dtype=int)
    for ys in years_list:
        for y in (ys or []): docs_per_year[y] += 1

    kw_doc = {y: Counter() for y in all_years}
    for i, ys in enumerate(years_list):
        toks = set(tokens_by_row[i])
        if not ys or not toks: continue
        for y in ys:
            kw_doc[y].update(toks)
    return all_years, kw_doc, docs_per_year, df_local

all_years, kw_doc, docs_per_year, _ = build_keyword_time(df, STOP | BASE_STOP)


def ensure_topk(pool_tokens, need_k, docs_per_year, kw_doc, years):
    """
    í† í° ì„ ë³„: (1) ê¸°ë³¸ ì»·ì˜¤í”„ ì¶©ì¡± â†’ (2) ìµœê·¼ RECENT_YEARS ì»·ì˜¤í”„ ëŒ€ì²´ â†’ (3) ìµœê·¼ì„±/ë³€ë™ì„± ë­í¬ ë³´ì¶©
    í•­ìƒ need_k ê°œìˆ˜ë¥¼ ë°˜í™˜í•˜ë ¤ê³  ì‹œë„.
    """
    def cnt_years_for(k, yrs):
        cnt = sum(kw_doc[y][k] for y in yrs)
        yrs_hit = sum(kw_doc[y][k] > 0 for y in yrs)
        return cnt, yrs_hit

    # (1) ê¸°ë³¸ ì»·ì˜¤í”„
    base_ok = []
    for k in pool_tokens:
        c, yh = cnt_years_for(k, years)
        if c >= MIN_DOCS_BASE and yh >= MIN_YEARS_BASE:
            base_ok.append(k)

    # (2) ìµœê·¼ ì»·ì˜¤í”„ (ë¶€ì¡±í•˜ë©´ ëŒ€ì²´ í—ˆìš©)
    last = years[-min(RECENT_YEARS, len(years)):]
    recent_ok = []
    for k in pool_tokens:
        c, yh = cnt_years_for(k, last)
        if c >= RECENT_DOCS_MIN and yh >= RECENT_YEARS_MIN:
            recent_ok.append(k)

    # (3) ë­í¬ â€” ìµœê·¼ ì ì¤‘ìˆ˜, ìµœê·¼ ë“±ì¥ì—°ìˆ˜, ë³€ë™ì„±
    recent_hits  = Counter(); recent_years = Counter()
    for y in last:
        recent_hits.update(kw_doc[y])
        for k,c in kw_doc[y].items():
            if c>0: recent_years[k]+=1
    var_proxy = {k: np.var([kw_doc[y][k]>0 for y in years]) for k in set().union(*[kw_doc[y].keys() for y in years])}
    ranked = sorted(set().union(*[kw_doc[y].keys() for y in years]),
                    key=lambda k: (recent_hits[k], recent_years[k], var_proxy.get(k,0.0)),
                    reverse=True)

    # í•©ì¹˜ê¸° + need_k ì±„ìš¸ ë•Œê¹Œì§€ ë³´ì¶©
    out = list(dict.fromkeys(base_ok))
    if len(out) < need_k:
        out = list(dict.fromkeys(out + recent_ok))
    if len(out) < need_k:
        out = list(dict.fromkeys(out + ranked))
    return out[:need_k]


def build_share_lift(tokens, years, kw_doc, docs_per_year):
    share = pd.DataFrame({
        k: jeffreys_rolling_ratio(
            pd.Series({y: kw_doc[y][k] for y in years}, dtype=float),
            docs_per_year.astype(float))
        for k in tokens
    }, index=years)
    w = docs_per_year / docs_per_year.sum()
    base = (share.mul(w, axis=0)).sum(axis=0).replace(0, np.nan)
    lift = share.div(base, axis=1).replace([np.inf,-np.inf], np.nan).fillna(0.0)
    return share, lift


def cagr(series):
    s = np.asarray(series, float)
    s = np.where(s<=0, np.nan, s)
    s = pd.Series(s).dropna()
    if len(s)<2: return 0.0
    n = len(s)-1
    return ((s.iloc[-1]/s.iloc[0])**(1/n) - 1) * 100.0


# ---- Plotly ë¼ì¸ ì°¨íŠ¸ ìƒì„±

def plot_trend_plotly(keys, years_plot, lift_df, title):
    fig = go.Figure()
    for k in keys:
        ys = [lift_df.loc[y, k] for y in years_plot]
        fig.add_trace(go.Scatter(x=years_plot, y=ys, mode="lines+markers", name=k,
                                 line=dict(width=3), marker=dict(size=8), connectgaps=True))
    fig.add_hline(y=1.0, line_width=1.5, line_dash="dash", opacity=0.6)
    fig.update_xaxes(title_text="ì—°ë„")
    fig.update_yaxes(title_text="lift (ë°°)")
    return style_fig(fig, title, legend="right", top_margin=100)

# ---- Plotly: ë¼ì¸ ë ë¼ë²¨(ê²¹ì¹¨ ë°©ì§€) ìœ í‹¸
# --- REPLACE THIS FUNCTION ENTIRELY ---
def add_line_end_labels(fig, years_plot, df, keys,
                        min_gap=0.03, xpad_frac=0.16, right_margin=200):
    """
    years_plot: ì •ë ¬ëœ ì—°ë„ ë¦¬ìŠ¤íŠ¸
    df: [index=years, columns=keys] ë˜ëŠ” [index=keys, columns=years] ëª¨ë‘ ì²˜ë¦¬
    keys: ë¼ë²¨ë§í•  ì‹œë¦¬ì¦ˆ ì´ë¦„ë“¤
    """
    import numpy as _np

    if not keys:
        return fig

    # 1) ë°ì´í„° ë°©í–¥ ìë™ ë³´ì •: keysê°€ ì—´ì— ì—†ìœ¼ë©´ ì „ì¹˜
    df2 = df if (keys[0] in df.columns) else df.T

    # 2) ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” í‚¤ë§Œ ì‚¬ìš©
    keys = [k for k in keys if k in df2.columns]
    if not keys:
        return fig

    # 3) y ë²”ìœ„ ê³„ì‚°
    ymins = [_np.nanmin(df2.loc[years_plot, k].astype(float).values) for k in keys]
    ymaxs = [_np.nanmax(df2.loc[years_plot, k].astype(float).values) for k in keys]
    y_min, y_max = float(min(ymins)), float(max(ymaxs))
    yrng = (y_max - y_min) if y_max > y_min else 1.0

    # 4) ë§ˆì§€ë§‰ y ê°’ ì •ë ¬ â†’ ê°„ê²© ë²Œë ¤ì„œ ê²¹ì¹¨ ë°©ì§€
    y_last = _np.array([df2.loc[years_plot, k].iloc[-1] for k in keys], dtype=float)
    order = _np.argsort(-y_last)
    y_des = y_last[order].copy()
    top_cap, bottom_cap = y_max - 0.02*yrng, y_min + 0.02*yrng
    gap = max(min_gap, 0.6/max(1, len(keys))) * yrng

    y_pos = _np.empty_like(y_des)
    y_pos[0] = float(_np.clip(y_des[0], bottom_cap, top_cap))
    for i in range(1, len(y_des)):
        yi = float(_np.clip(y_des[i], bottom_cap, top_cap))
        if y_pos[i-1] - yi < gap:
            yi = y_pos[i-1] - gap
        if yi < bottom_cap:
            shift = bottom_cap - yi
            y_pos[:i] += shift
            yi = bottom_cap
            if _np.any(y_pos[:i] > top_cap):
                start = top_cap - gap * (i)
                y_pos[:i+1] = _np.linspace(start, top_cap, i+1)
                break
        y_pos[i] = yi

    inv = _np.empty_like(order); inv[order] = _np.arange(len(order))
    y_final = y_pos[inv]

    # 5) xì¶• íŒ¨ë”© + ì˜¤ë¥¸ìª½ ë§ˆì§„ í™•ì¥(ì˜ë¦¼ ë°©ì§€)
    x0, x1 = years_plot[0], years_plot[-1]
    xpad = (x1 - x0) * xpad_frac
    fig.update_xaxes(range=[x0, x1 + xpad])
    # margin.rë§Œ ì¦ì„¤(ê¸°ì¡´ l/t/bëŠ” ìœ ì§€)
    fig.update_layout(margin=dict(r=max(getattr(fig.layout.margin, "r", 0), right_margin)))
    x_label = x1 + xpad*0.55

    # 6) ì—°ê²°ì„  + ì£¼ì„
    for i, k in enumerate(keys):
        yk_end = float(y_last[i]); yf = float(y_final[i])
        fig.add_shape(type="line",
                      x0=x1, y0=yk_end, x1=x1 + xpad*0.45, y1=yf,
                      line=dict(color="rgba(128,128,128,0.85)", width=1))
        fig.add_annotation(x=x_label, y=yf, text=k, showarrow=False,
                           xanchor="left", yanchor="middle",
                           bgcolor="rgba(255,255,255,0.82)",
                           bordercolor="rgba(0,0,0,0)", borderpad=2,
                           font=dict(size=12, color=ui['text']))
    return fig



if all_years:
    # í’€ í›„ë³´
    all_tokens = sorted({k for y in all_years for k in kw_doc[y].keys()})
    need_k = max(TOP_K_PER_FIG*2, 16)
    pool_tokens = ensure_topk(all_tokens, need_k, docs_per_year, kw_doc, all_years)
    share_all, lift_all = build_share_lift(pool_tokens, all_years, kw_doc, docs_per_year)

    win_years = all_years[-min(WINDOW_YEARS, len(all_years)):]
    share_win  = share_all.loc[win_years]
    lift_win   = lift_all.loc[win_years]

    latest_share = share_win.iloc[-1]
    delta_share  = (share_win.iloc[-1] - share_win.iloc[0])  # p.p. ë³€í™”
    last_lift    = lift_win.iloc[-1]
    cagr_lift    = pd.Series({k: cagr(lift_win[k].values) for k in lift_win.columns})

    # ì ìˆ˜(ì •ë ¬ìš©) â€” 2-of-3 ê·œì¹™ê³¼ ì¡°í™”ë˜ë„ë¡ êµ¬ì„±
    rise_score = (last_lift - 1.0) + 0.7*(cagr_lift/100.0) + 0.5*(delta_share/100.0)
    fall_score = (1.0 - last_lift) + 0.7*((-cagr_lift)/100.0) + 0.5*((-delta_share)/100.0)

    # 2-of-3 ê·œì¹™ìœ¼ë¡œ ìƒ/í•˜ë½ í›„ë³´ ë¶„ë¦¬
    sig_up   = ((last_lift >= 1.0).astype(int) + (cagr_lift > 0).astype(int) + (delta_share > 0).astype(int))
    sig_down = ((last_lift < 1.0).astype(int)  + (cagr_lift < 0).astype(int) + (delta_share < 0).astype(int))

    rise_order = [k for k in rise_score.sort_values(ascending=False).index if sig_up[k]   >= 2]
    fall_order = [k for k in fall_score.sort_values(ascending=False).index if sig_down[k] >= 2]

    used=set(); rise_sel=[]; fall_sel=[]
    for k in rise_order:
        if k not in used: rise_sel.append(k); used.add(k)
    for k in fall_order:
        if k not in used: fall_sel.append(k); used.add(k)

    # ë¶€ì¡± ì‹œ ìµœê·¼ì„± ê¸°ì¤€ ë³´ì¶© (ì¤‘ë³µ ê¸ˆì§€)
    def backfill(sel, need, base_rank, predicate):
        if len(sel) >= need: return sel
        recent = win_years[-min(RECENT_YEARS, len(win_years)):]
        hits_recent  = Counter(); years_recent = Counter()
        for y in recent:
            hits_recent.update(kw_doc[y])
            for k,c in kw_doc[y].items():
                if c>0: years_recent[k]+=1
        var_proxy = {k: np.var([kw_doc[y][k]>0 for y in win_years]) for k in base_rank}
        rank = sorted(base_rank, key=lambda k: (hits_recent[k], years_recent[k], var_proxy.get(k,0.0)), reverse=True)
        for k in rank:
            if len(sel) >= need: break
            if k in used: continue
            if predicate(k): sel.append(k); used.add(k)
        if len(sel) < need:
            for k in base_rank:
                if len(sel) >= need: break
                if k in used: continue
                sel.append(k); used.add(k)
        return sel

    rise_sel = backfill(rise_sel, TOP_K_PER_FIG, list(rise_score.sort_values(ascending=False).index),
                        lambda k: (last_lift[k] >= 1.0) or (cagr_lift[k] > 0) or (delta_share[k] > 0))
    fall_sel = backfill(fall_sel, TOP_K_PER_FIG, list(fall_score.sort_values(ascending=False).index),
                        lambda k: (last_lift[k] < 1.0) or (cagr_lift[k] < 0) or (delta_share[k] < 0))

    rise_sel = rise_sel[:TOP_K_PER_FIG]
    fall_sel = fall_sel[:TOP_K_PER_FIG]

    years_plot = win_years[-min(RECENT_YEARS*2, len(win_years)):]  # ìµœê·¼ 10ë…„ ë‚´ì—ì„œ 10~?ë…„ ìŠ¬ë¼ì´ìŠ¤

    fig_up   = plot_trend_plotly(rise_sel, years_plot, lift_all, f"ìƒìŠ¹ì„¸ â€” ìµœê·¼ {len(years_plot)}ë…„")
    fig_up   = style_fig(fig_up, bg_color=VIZ_BG["trend_up"], bg_alpha=0.5)
    fig_up   = add_line_end_labels(fig_up, years_plot, lift_all, rise_sel)
    fig_down = plot_trend_plotly(fall_sel, years_plot, lift_all, f"í•˜ë½ì„¸ â€” ìµœê·¼ {len(years_plot)}ë…„")
    fig_down = style_fig(fig_down, bg_color=VIZ_BG["trend_down"], bg_alpha=0.5)
    fig_down = add_line_end_labels(fig_down, years_plot, lift_all, fall_sel)

    u, v = st.columns([1,1], gap="large")
    # íŠ¸ë Œë“œ ì°¨íŠ¸ëŠ” ë²”ë¡€ë¥¼ ìƒë‹¨ìœ¼ë¡œ ì´ë™ (ë¸”ë¡ ë‚´ë¶€ì— ìœ ì§€)
    with u:
        fig_up.update_layout(legend=dict(orientation="h", y=1.10, yanchor="bottom", x=0, xanchor="left"))
        fig_up = add_line_end_labels(fig_up, years_plot, lift_all, rise_sel)
        fig_up = force_legend_top_padding(fig_up, base_top=130)  # â˜… ì¶”ê°€(ë³´ìˆ˜ì )
        st.plotly_chart(fig_up, use_container_width=True, config={"displayModeBar": False})
    with v:
        fig_down.update_layout(legend=dict(orientation="h", y=1.10, yanchor="bottom", x=0, xanchor="left"))
        fig_down = add_line_end_labels(fig_down, years_plot, lift_all, fall_sel)
        fig_down = force_legend_top_padding(fig_down, base_top=130)  # â˜… ì¶”ê°€
        st.plotly_chart(fig_down, use_container_width=True, config={"displayModeBar": False})
else:
    st.info("ì—°ë„ì—ì„œ ì—°ë„ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ í‚¤ì›Œë“œ ìƒëŒ€ íŠ¸ë Œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

# =====================================================================
# ì¶”ê°€ ì‹œê°í™” â‘¡: ëŒ€í‘œ 'ì£¼ì œ(í‚¤ì›Œë“œ)' ìƒëŒ€ íŠ¸ë Œë“œ(ìƒìŠ¹/í•˜ë½) â€” Plotly
# =====================================================================
st.markdown("---")
st.subheader("ë¶„ì„ ê¸°ë°˜ í‚¤ì›Œë“œ ìƒëŒ€ íŠ¸ë Œë“œ (ìƒìŠ¹/í•˜ë½)")

THEMES = OrderedDict([
    (r"(ì „ì\s*ì¡°ë‹¬|e[\s\-]*procure(?:ment)?|e[\s\-]*gp\b|joneps|koneps|prozorro)", "ì „ìì¡°ë‹¬Â·e-Procurement"),
    (r"(ì „ì\s*ë¬´ì—­|ë””ì§€í„¸\s*ë¬´ì—­|e[\s\-]*trade|electronic\s*trade|ì „ì\s*ìƒê±°ë˜|e[\s\-]*commerce|trade\s*facilitation)", "ì „ìë¬´ì—­Â·e-Invoice"),
    (r"(ì „ì\s*ì„¸ê¸ˆ\s*ê³„ì‚°ì„œ|ì „ìì„¸ê¸ˆê³„ì‚°ì„œ|e[\s\-]*invoice|e\s*invoice|ì „ì\s*ì¸ë³´ì´ìŠ¤)", "ì „ìë¬´ì—­Â·e-Invoice"),
    (r"(ifmis|í†µí•©\s*ì¬ì •ê´€ë¦¬|ì¬ì •ê´€ë¦¬\s*ì •ë³´\s*ì‹œìŠ¤í…œ|government[-\s]*wide\s*fm|span\b)", "ì¬ì •ê´€ë¦¬(IFMIS)"),
    (r"(ì „ì\s*ì„œëª…|ë””ì§€í„¸\s*ì¸ì¦|pki|ê³µì¸ì¸ì¦|electronic\s*signature|digital\s*certificat(?:e|ion)|"
     r"certification\s*authority|(?:^|\W)ca(?:$|\W))", "ì „ìì„œëª…/PKI"),
    (r"(ì§€ì‹\s*ì¬ì‚°|ì§€ì‹ì¬ì‚°ê¶Œ|ip\b|inapi|íŠ¹í—ˆ|ì¶œì›\s*ì‹¬ì‚¬|ì¶œì›ì‹¬ì‚¬|patent|trademark|ìƒí‘œ)", "ì§€ì‹ì¬ì‚°Â·ì¶œì›ì‹¬ì‚¬"),
    (r"(ë°ì´í„°\s*ì„¼í„°|ë°ì´í„°ì„¼í„°|í´ë¼ìš°ë“œ|cloud|gov\s*cloud|government\s*cloud|ë°ì´í„°\s*ê±°ë²„ë„ŒìŠ¤|"
     r"data\s*governance|ë°ì´í„°\s*í”Œë«í¼)", "ë°ì´í„°ê±°ë²„ë„ŒìŠ¤Â·ì •ë¶€ í´ë¼ìš°ë“œ"),
    (r"(?:(?:neis|ë‚˜ì´ìŠ¤|êµìœ¡\s*í–‰ì •\s*ì •ë³´\s*ì‹œìŠ¤í…œ|í•™êµ\s*í–‰ì •\s*ì •ë³´\s*ì‹œìŠ¤í…œ|"
     r"(?:^|\W)emis(?:$|\W)|education\s*management\s*information\s*system)"
     r"|(?:e[\s\-]*health|telemedicine|ehr\b|his\b|hmis\b|"
     r"(?:ë³´ê±´|ê±´ê°•|health)\s*(?:ict|ì •ë³´|ì‹œìŠ¤í…œ|í”Œë«í¼))"
     r"|(?:(?:livestock|ì¶•ì‚°|ê°€ì¶•|ë„ì¶•|meat|nmis|traceab\w*|ì´ë ¥\s*ì¶”ì )\s*"
     r"(?:ict|ì •ë³´|ì‹œìŠ¤í…œ|í”Œë«í¼|ì¶”ì |ê´€ë¦¬)))", "NEISêµìœ¡ë³´ê±´Â·ì¶•ì‚° ICT"),
    (r"(ê´€ê´‘\s*ë¹…ë°ì´í„°|tourism\s*data|ëª¨ë°”ì¼\s*ë°ì´í„°\s*ê´€ê´‘|tourism\s*analytics|ê´€ê´‘\s*ë¶„ì„)", "ê´€ê´‘ ë¹…ë°ì´í„°"),
    (r"(êµìœ¡\s*ict|í¬ìš©ì \s*êµìœ¡|ìŠ¤ë§ˆíŠ¸\s*êµì‹¤|edtech|ë””ì§€í„¸\s*êµì¬|ìŠ¤ë§ˆíŠ¸\s*êµìœ¡)", "êµìœ¡ ICT"),
    (r"(ë‚´ë¶€\s*ê°ì‚¬|ë‚´ë¶€\s*í†µì œ|it\s*í†µì œ|internal\s*audit|internal\s*control|bpkp|ê°ì‚¬\s*ì²´ê³„|ê±°ë²„ë„ŒìŠ¤\s*ê°œì„ )", "í–‰ì •ê°œí˜Â·ë‚´ë¶€í†µì œ"),
    (r"(ìŠ¤ë§ˆíŠ¸\s*ì‹œí‹°|smart\s*city|hydro(?:met|meteorolog)|hydro[-\s]*met|"
     r"aws\b|automatic\s*weather\s*station|rain\s*gauge|ìš°ëŸ‰(?:ê³„)?|ê°•ìš°|"
     r"ìˆ˜ë¬¸|ìˆ˜ë¬¸\s*ê´€ì¸¡|ìˆ˜ìœ„(?:ê³„|ê´€ì¸¡)?|ê´€ì¸¡\s*ë„¤íŠ¸ì›Œí¬|iot\s*ì„¼ì„œ|telemetry|scada)", "ìŠ¤ë§ˆíŠ¸ì‹œí‹°Â·ìˆ˜ë¬¸ê´€ì¸¡"),
])

def normalize_text(row):
    parts = [str(row.get(c, "")) for c in ["íŒŒì¼ëª…","ì£¼ìš” ë¶„ì•¼","ìš”ì•½","ì£¼ìš” ë‚´ìš©","ê¸°ëŒ€ íš¨ê³¼"] if c in row.index]
    t = " ".join(parts).lower()
    t = re.sub(r"[â€œâ€\"'`]", "", t); t = re.sub(r"[Â·âˆ™â€¢â€§ï½¥ãƒ»]", " ", t); t = re.sub(r"\s+", " ", t).strip()
    return t

def detect_themes(text: str):
    hits = set()
    for pat, label in THEMES.items():
        if re.search(pat, text, flags=re.I): hits.add(label)
    return list(hits)

if all_years:
    # í…Œë§ˆÃ—ì—°ë„ count
    theme_year_cnt = defaultdict(int)
    for _, row in df.iterrows():
        themes = detect_themes(normalize_text(row))
        ys = years_from_span(row.get("ì—°ë„", ""))
        if not themes or not ys: continue
        for y in ys:
            for th in themes:
                theme_year_cnt[(th, y)] += 1
    rows = [(th, y, c) for (th, y), c in theme_year_cnt.items()]
    if rows:
        cnt_df = pd.DataFrame(rows, columns=["theme","year","count"])
        pivot_cnt = cnt_df.pivot_table(index="theme", columns="year", values="count", aggfunc="sum").fillna(0.0)
        pivot_cnt = pivot_cnt.reindex(columns=all_years, fill_value=0.0)
        valid = [th for th in pivot_cnt.index if pivot_cnt.loc[th].sum() >= 4 and (pivot_cnt.loc[th] > 0).sum() >= 3]
        pivot_cnt = pivot_cnt.loc[valid]

        den_year  = pd.Series(docs_per_year, dtype=float) + 2*ALPHA
        share     = pivot_cnt.add(ALPHA, axis=0).div(den_year, axis=1) * 100.0
        share     = share.rolling(5, axis=1, center=True, min_periods=1).mean()

        w = (docs_per_year / docs_per_year.sum()).reindex(all_years)
        base = (share.mul(w.values, axis=1)).sum(axis=1)
        lift = share.div(base, axis=0).replace([np.inf, -np.inf], np.nan).fillna(0.0)

        N_plot = min(10, len(all_years))
        years_plot = all_years[-N_plot:]

        # ìµœê·¼ ê¸°ìš¸ê¸°
        N_slope = min(5, len(all_years))
        yrs_slope = np.array(all_years[-N_slope:], dtype=float)
        recent_slope = {th: np.polyfit(yrs_slope, lift.loc[th, all_years[-N_slope:]].values.astype(float), 1)[0]
                        for th in lift.index}
        var_lift = lift.var(axis=1, ddof=1).fillna(0.0)
        up_sorted   = sorted([th for th in lift.index if recent_slope[th] > 0],
                             key=lambda k: abs(recent_slope[k])*(1+var_lift[k]), reverse=True)[:8]
        down_sorted = sorted([th for th in lift.index if recent_slope[th] < 0],
                             key=lambda k: abs(recent_slope[k])*(1+var_lift[k]), reverse=True)[:8]

        def plot_theme_plotly(keys, title):
            fig = go.Figure()
            for k in keys:
                fig.add_trace(go.Scatter(x=years_plot, y=lift.loc[k, years_plot].values,
                                         mode="lines+markers", name=k,
                                         line=dict(width=3), marker=dict(size=8)))
            fig.add_hline(y=1.0, line_width=1.5, line_dash="dash", opacity=0.6)
            fig.update_xaxes(title_text="ì—°ë„")
            fig.update_yaxes(title_text="lift (ë°°)")
            return style_fig(fig, title + f" â€” ìµœê·¼ {N_slope}ë…„ ê¸°ì¤€")

        uu, vv = st.columns([1,1], gap="large")
        fig_tu = plot_theme_plotly(up_sorted, "ìƒìŠ¹ì„¸")
        fig_tu = style_fig(fig_tu, bg_color=VIZ_BG["theme_up"], bg_alpha=0.5)
        fig_tu.update_layout(legend=dict(orientation="h", y=1.10, yanchor="bottom", x=0, xanchor="left"))
        fig_tu = add_line_end_labels(fig_tu, years_plot, lift, up_sorted)
        fig_tu = force_legend_top_padding(fig_tu, base_top=130)  # â˜… ì¶”ê°€
        with uu:
            st.plotly_chart(fig_tu, use_container_width=True, config={"displayModeBar": False})

        fig_td = plot_theme_plotly(down_sorted, "í•˜ë½ì„¸")
        fig_td = style_fig(fig_td, bg_color=VIZ_BG["theme_down"], bg_alpha=0.5)
        fig_td.update_layout(legend=dict(orientation="h", y=1.10, yanchor="bottom", x=0, xanchor="left"))
        fig_td = add_line_end_labels(fig_td, years_plot, lift, down_sorted)
        fig_td = force_legend_top_padding(fig_td, base_top=130)  # â˜… ì¶”ê°€
        with vv:
            st.plotly_chart(fig_td, use_container_width=True, config={"displayModeBar": False})
    else:
        st.info("í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì œë¥¼ ê°ì§€í•˜ì§€ ëª»í•´ 'ì£¼ì œ(í‚¤ì›Œë“œ)' íŠ¸ë Œë“œë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
else:
    st.info("ì—°ë„ì—ì„œ ì—°ë„ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ 'ì£¼ì œ(í‚¤ì›Œë“œ)' íŠ¸ë Œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

# --------------------- ì„¤ì¹˜ / ì‹¤í–‰ ---------------------
with st.expander("ì„¤ì¹˜ / ì‹¤í–‰"):
    st.code("pip install streamlit folium streamlit-folium pandas wordcloud plotly matplotlib", language="bash")
    st.code("streamlit run S_KSP_clickpro_v4_plotly_patch_FIXED.py", language="bash")














































































